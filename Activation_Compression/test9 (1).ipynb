{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c81cfe9",
   "metadata": {},
   "source": [
    "/storage/ice1/shared/d-pace_community/makerspace-datasets/IMAGE/\n",
    "Imagenet2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e96b8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/yyu496/.conda/envs/lib/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "# os.environ[\"TORCH_LOGS\"] = \"output_code\"\n",
    "# os.environ[\"TORCH_LOGS\"] = \"inductor\"\n",
    "# os.environ[\"TORCHINDUCTOR_TRACE\"] = \"1\"\n",
    "# os.environ[\"TORCHINDUCTOR_VERBOSE\"] = \"1\"\n",
    "# os.environ[\"TORCHINDUCTOR_DEBUG\"] = \"1\"\n",
    "# os.environ[\"TORCHINDUCTOR_DUMP\"] = \"1\"\n",
    "# os.environ[\"TORCHINDUCTOR_COMPILE_THREADS\"] = \"1\"\n",
    "# os.environ[\"TORCH_COMPILE_DEBUG\"] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = (\n",
    "    # \"backend:cudaMallocAsync,\"\n",
    "    \"expandable_segments:True,\"\n",
    "    # \"garbage_collection_threshold:0.6\"\n",
    ")\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/hice1/yyu496/kaggle/CW')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch._functorch.aot_autograd import aot_module, make_boxed_func\n",
    "from torch.autograd import grad\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.optim.lr_scheduler import (\n",
    "    LinearLR,\n",
    "    CosineAnnealingLR,\n",
    "    SequentialLR\n",
    ")\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "import torch\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import torch._dynamo as dynamo\n",
    "import torch.nn.utils.parametrize as P\n",
    "from torch.nn.utils.parametrizations import spectral_norm\n",
    "\n",
    "import ACT6.cpp_extension as cpp_extension\n",
    "\n",
    "from ACT6.controller import Controller\n",
    "import ACT6.cuda_graph_utils as cuda_utils\n",
    "from ACT6.layers import RMSNorm, DOConv2d, DOConv1d, DOLinear\n",
    "from ACT6.fusion.fused_layers import DOBatchNormReLU2d\n",
    "from freq_utils import radial_spectrum_2d, wasserstein_1d, kl_div_spectrum\n",
    "from VT_test import VisionTransformerV2\n",
    "\n",
    "import timm\n",
    "from timm.layers import trunc_normal_\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "import actnn\n",
    "# available choices are [\"L0\", \"L1\", \"L2\", \"L3\", \"L4\", \"L5\"]\n",
    "actnn.set_optimization_level(\"L3\")\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchmetrics import Accuracy \n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aad128bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "warmup_epochs = 25\n",
    "num_epochs = 512\n",
    "# ============= Data ==================\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "def safe_collate(batch):\n",
    "    xs, ys = zip(*batch)\n",
    "    xs = torch.stack([x.contiguous().clone() for x in xs], 0)\n",
    "    ys = torch.tensor(ys)\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "transform_train = v2.Compose([\n",
    "    # --- Convert to tensor FIRST (kills PIL early) ---\n",
    "    v2.ToImage(),                          # handles PIL → Tensor safely\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "\n",
    "    # --- Geometry ---\n",
    "    v2.Resize((224, 224), antialias=True),\n",
    "    # v2.RandomResizedCrop(224, scale=(0.5, 1.0), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "    # --- Color ---\n",
    "    v2.RandomApply([\n",
    "        v2.ColorJitter(\n",
    "            brightness=0.2,\n",
    "            contrast=0.2,\n",
    "            saturation=0.2,\n",
    "            hue=0.1\n",
    "        )\n",
    "    ], p=0.5),\n",
    "\n",
    "    # --- RandAugment (vectorized, NO PIL) ---\n",
    "    v2.RandAugment(num_ops=4, magnitude=9),\n",
    "\n",
    "    # --- Normalize ---\n",
    "    v2.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "\n",
    "    # --- Random Erasing (tensor-based, fast) ---\n",
    "    v2.RandomErasing(p=0.5, scale=(0.02, 0.33),\n",
    "                     ratio=(0.3, 3.3), value=0),\n",
    "])\n",
    "\n",
    "check_transform = v2.Compose([\n",
    "    v2.ToImage(),                          \n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "\n",
    "#/home/hice1/yyu496/scratch/data/cifar10_resized/train\n",
    "# /storage/ice1/shared/d-pace_community/makerspace-datasets/IMAGE/Imagenet2012/train\n",
    "#/storage/ice1/shared/d-pace_community/makerspace-datasets/IMAGE/CIFAR-10-images\n",
    "check_set = ImageFolder(\n",
    "    root=\"/home/hice1/yyu496/scratch/data/cifar10_resized/train\",\n",
    "    transform=check_transform\n",
    ")\n",
    "# check_set = datasets.CIFAR10(root=\"./data\", train=True,\n",
    "#         download=True, transform=check_transform)\n",
    "\n",
    "check_loder = DataLoader(\n",
    "    check_set, batch_size=batch_size, shuffle=True,\n",
    "    num_workers=15, pin_memory=True, drop_last=True,\n",
    "    persistent_workers=True, prefetch_factor=3,\n",
    "    collate_fn=safe_collate\n",
    ")\n",
    "\n",
    "# download and prepare\n",
    "# train_dataset = datasets.CIFAR10(root=\"./data\", train=True,\n",
    "#                                  download=True, transform=transform_train)\n",
    "train_dataset = ImageFolder(\n",
    "    root=\"/home/hice1/yyu496/scratch/data/cifar10_resized/train\",\n",
    "    transform=transform_train\n",
    ")\n",
    "\n",
    "# DataLoader with GPU-friendly settings\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                          num_workers=15, pin_memory=True, drop_last=True,\n",
    "                          persistent_workers=True, prefetch_factor=3)\n",
    "\n",
    "\n",
    "\n",
    "val_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "\n",
    "    v2.Resize((224, 224), antialias=True),\n",
    "\n",
    "    v2.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# /storage/ice1/shared/d-pace_community/makerspace-datasets/IMAGE/Imagenet2012/val\n",
    "# /storage/ice1/shared/d-pace_community/makerspace-datasets/IMAGE/CIFAR-10-images/test\n",
    "#/home/hice1/yyu496/scratch/data/cifar10_resized/test\n",
    "# === CIFAR-10 validation dataset (train=False) ===\n",
    "# val_dataset = datasets.CIFAR10(\n",
    "#     root=\"./data\",\n",
    "#     train=False,\n",
    "#     download=True,\n",
    "#     transform=val_transform\n",
    "# )\n",
    "val_dataset = ImageFolder(\n",
    "    root=\"/home/hice1/yyu496/scratch/data/cifar10_resized/test\",\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "# === Validation DataLoader ===\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=512,\n",
    "    shuffle=False,   \n",
    "    num_workers=15,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True, \n",
    "    prefetch_factor=3\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def mixup_cutmix(x, y, alpha=1.0, p_cutmix=0.5):\n",
    "    if alpha <= 0:\n",
    "        return x, y\n",
    "\n",
    "    lam = torch.distributions.Beta(alpha, alpha).sample().to(x.device)\n",
    "\n",
    "    b = x.size(0)\n",
    "    perm = torch.randperm(b, device=x.device)\n",
    "    x2, y2 = x[perm], y[perm]\n",
    "\n",
    "    H, W = x.shape[-2:]\n",
    "\n",
    "    if torch.rand(()) < p_cutmix:\n",
    "        # ---- CutMix ----\n",
    "        cx = torch.randint(W, (1,), device=x.device)\n",
    "        cy = torch.randint(H, (1,), device=x.device)\n",
    "\n",
    "        w = (W * torch.sqrt(1 - lam)).long()\n",
    "        h = (H * torch.sqrt(1 - lam)).long()\n",
    "\n",
    "        x1 = torch.clamp(cx - w // 2, 0, W)\n",
    "        x2b = torch.clamp(cx + w // 2, 0, W)\n",
    "        y1 = torch.clamp(cy - h // 2, 0, H)\n",
    "        y2b = torch.clamp(cy + h // 2, 0, H)\n",
    "\n",
    "        x[:, :, y1:y2b, x1:x2b] = x2[:, :, y1:y2b, x1:x2b]\n",
    "        lam = 1 - ((x2b-x1)*(y2b-y1) / (W*H))\n",
    "    else:\n",
    "        # ---- Mixup ----\n",
    "        x = lam * x + (1 - lam) * x2\n",
    "\n",
    "    return x, (y, y2, lam)\n",
    "\n",
    "def replace_activation_func(m):\n",
    "    for name, child in m.named_children():\n",
    "        if isinstance(child, nn.ReLU):\n",
    "            setattr(m, name, nn.SiLU(inplace=True))\n",
    "            # setattr(m, name, nn.ReLU6(inplace=True))\n",
    "            # setattr(m, name, nn.LeakyReLU(inplace=True))\n",
    "            # setattr(m, name, nn.GELU())\n",
    "        else:\n",
    "            replace_activation_func(child)\n",
    "\n",
    "\n",
    "def replace_norm(m):\n",
    "    for name, child in m.named_children():\n",
    "        if isinstance(child, nn.LayerNorm):\n",
    "            setattr(m, name, RMSNorm(dims=child.normalized_shape[-1]))\n",
    "        else:\n",
    "            replace_activation_func(child)\n",
    "\n",
    "\n",
    "def disable_act_inplace(m):\n",
    "    for module in m.modules():\n",
    "        if isinstance(module, (nn.ReLU, nn.SiLU, nn.GELU, nn.ReLU6)) and hasattr(module, 'inplace'):\n",
    "            module.inplace = False\n",
    "\n",
    "def set_seed(SEED):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "def init_fan_out(m):\n",
    "    if isinstance(m, (nn.Conv2d,)):\n",
    "        nn.init.kaiming_normal_(\n",
    "            m.weight,\n",
    "            mode='fan_out',\n",
    "            nonlinearity='relu'\n",
    "        )\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(\n",
    "            m.weight,\n",
    "            mode='fan_in',\n",
    "            nonlinearity='relu'\n",
    "        )\n",
    "\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "def apply_trunc_init_for_act(module, std=0.02):\n",
    "    \"\"\"\n",
    "    Truncated normal init tuned for ACT stability.\n",
    "    Applies only to ACT-sensitive layers.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Convs (ACT-fragile entry + transition layers) ----\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        # stem conv or downsample conv benefit most\n",
    "        if module.kernel_size[0] > 1 or module.in_channels < module.out_channels:\n",
    "            trunc_normal_(module.weight, std=std)\n",
    "        else:\n",
    "            nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "    # ---- Linear layers (ViT / MLP / projections) ----\n",
    "    elif isinstance(module, nn.Linear):\n",
    "        trunc_normal_(module.weight, std=std)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "    # ---- BatchNorm / LayerNorm ----\n",
    "    elif isinstance(module, (nn.BatchNorm2d, nn.GroupNorm, nn.LayerNorm)):\n",
    "        nn.init.ones_(module.weight)\n",
    "        nn.init.zeros_(module.bias)\n",
    "\n",
    "\n",
    "\n",
    "def init_orthogonal_conv(m, gain=1.0):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.orthogonal_(m.weight, gain=gain)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        # nn.init.orthogonal_(m.weight, gain=gain)\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "def init_orthogonal_conv_v2(n, m, gain=1.0):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.orthogonal_(m.weight, gain=gain)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    if n == 'layer2.0.conv1' and isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "        print('True')\n",
    "\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        # nn.init.orthogonal_(m.weight, gain=gain)\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a1f5183",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_cache = None\n",
    "def forward_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        global act_cache\n",
    "        act_cache = output\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a19dbf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIVISION = {\n",
    "    'pool_kernel_size' : 3\n",
    "}\n",
    "# DIVISION = None\n",
    "\n",
    "config = {\n",
    "    \"default_bits\": 2,\n",
    "    'auto_precision': None,\n",
    "    'DIVISION' : DIVISION,\n",
    "    \"group_size\": 256,\n",
    "    'batch_size' : batch_size,\n",
    "    'fp8' : False,\n",
    "    'depth_point_conv' : False,\n",
    "    'rms_norm' : False\n",
    "}\n",
    "\n",
    "num_classes = 10\n",
    "# vit_base_patch16_224\n",
    "# efficientnet_b0\n",
    "# model = timm.create_model('vit_base_patch16_224', pretrained=False, num_classes=10)\n",
    "# replace_activation_func(model)\n",
    "# replace_norm(model)\n",
    "\n",
    "# set_seed(2)\n",
    "model = VisionTransformerV2(\n",
    "    image_size=224,\n",
    "    patch_size=16,\n",
    "    in_channels=3,\n",
    "    num_classes=num_classes,\n",
    "    embed_dim=384,\n",
    "    depth=8,\n",
    "    num_head=6,\n",
    "    mlp_ratio=4.0,\n",
    "    attn_p=0.0,\n",
    "    mlp_p=0.0,\n",
    "    pos_p=0.0\n",
    ")\n",
    "# model = models.resnet50(weights=None)\n",
    "# model = models.resnet18(weights=None)\n",
    "# model = models.efficientnet_b0(weights=None)\n",
    "# model = models.mobilenet_v2(weights=None)\n",
    "# model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "# model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "# replace_activation_func(model)\n",
    "disable_act_inplace(model)\n",
    "# model.apply(lambda m: apply_trunc_init_for_act(m, std=0.02))\n",
    "# model.apply(init_fan_out)\n",
    "# model.apply(lambda m: init_orthogonal_conv(m, gain=1.0))\n",
    "# init(model)\n",
    "# model.load_state_dict(torch.load(\"/home/hice1/yyu496/scratch/Model_Checkpoint/ResNet18_bs_8192\"))\n",
    "\n",
    "\n",
    "# model = actnn.QModule(model)\n",
    "# model.cuda()\n",
    "# model.compile(fullgraph=True)\n",
    "\n",
    "\n",
    "# set_seed(2)\n",
    "# torch.set_rng_state(cached_rng_state['CPU'])\n",
    "# torch.cuda.set_rng_state(cached_rng_state['GPU'])\n",
    "# model.load_state_dict(baseline)\n",
    "# disable_act_inplace(model)\n",
    "# # init(model)\n",
    "# for n, m in model.named_modules():\n",
    "#     if isinstance(m, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n",
    "#             cahced_init(m.weight)\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "compute_stream = torch.cuda.Stream()\n",
    "controller = Controller(model, config, train_loader, criterion, test=False)\n",
    "controller.iterate(criterion)\n",
    "controller.warp_model(graph_mode=True, quantizer=True)\n",
    "# controller.warmup(train_loader, criterion, compute_stream)\n",
    "\n",
    "# for name, module in controller.traced_model.named_modules():\n",
    "#     if isinstance(module, (DOConv2d, DOConv1d, DOLinear)):\n",
    "#         if 'features.8.0' in name:\n",
    "#             m = module.register_forward_hook(forward_hook(name))\n",
    "#             print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "962045d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD_LARS(Optimizer):\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 lr=0.1,\n",
    "                 momentum=0.9,\n",
    "                 weight_decay=1e-4,\n",
    "                 eta=0.1,\n",
    "                 eps=1e-9,\n",
    "                 nesterov=False,\n",
    "                 trust_coef=True):\n",
    "        defaults = dict(lr=lr,\n",
    "                        momentum=momentum,\n",
    "                        weight_decay=weight_decay,\n",
    "                        eta=eta,\n",
    "                        eps=eps,\n",
    "                        nesterov=nesterov,\n",
    "                        trust_coef=trust_coef)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            momentum = group['momentum']\n",
    "            weight_decay = group['weight_decay']\n",
    "            eta = group['eta']\n",
    "            eps = group['eps']\n",
    "            nesterov = group['nesterov']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                grad = p.grad\n",
    "\n",
    "                # ---- decoupled weight decay ----\n",
    "                if weight_decay != 0 and p.ndim > 1:\n",
    "                    grad = grad.add(p, alpha=weight_decay)\n",
    "\n",
    "                # ---- LARS trust ratio (only apply to weight tensors) ----\n",
    "                if p.ndim > 1:      # don't LARS biases / norms\n",
    "                    w_norm = p.norm()\n",
    "                    g_norm = grad.norm()\n",
    "                    trust_ratio = 0\n",
    "\n",
    "                    if not torch.isfinite(grad).all():\n",
    "                        print(\"grad has NaN/Inf\")\n",
    "                    if not torch.isfinite(w_norm):\n",
    "                        print(\"w_norm NaN/Inf:\", w_norm.item())\n",
    "                    if not torch.isfinite(g_norm):\n",
    "                        print(\"g_norm NaN/Inf:\", g_norm.item())\n",
    "\n",
    "                    if w_norm > 0 and g_norm > 0:\n",
    "                        trust_ratio = eta * (w_norm / (g_norm + eps))\n",
    "                    else:\n",
    "                        trust_ratio = 1.0\n",
    "\n",
    "                    if not torch.isfinite(trust_ratio):\n",
    "                        print(\"trust_ratio NaN/Inf:\", trust_ratio.item())\n",
    "                    print(\"trust_ratio\", trust_ratio.item(), \"dw_norm\", g_norm.item(), \"w_norm\", w_norm.item())\n",
    "\n",
    "                    \n",
    "                else:\n",
    "                    trust_ratio = 1.0\n",
    "\n",
    "                scaled_lr = lr * trust_ratio\n",
    "\n",
    "                # ---- Momentum buffer ----\n",
    "                param_state = self.state[p]\n",
    "                if 'momentum_buffer' not in param_state:\n",
    "                    buf = param_state['momentum_buffer'] = torch.zeros_like(p)\n",
    "                else:\n",
    "                    buf = param_state['momentum_buffer']\n",
    "\n",
    "                buf.mul_(momentum).add_(grad)  # standard momentum update\n",
    "\n",
    "                if nesterov:\n",
    "                    update = grad.add(buf, alpha=momentum)\n",
    "                else:\n",
    "                    update = buf\n",
    "\n",
    "                # ---- Update weights ----\n",
    "                p.add_(update, alpha=-scaled_lr)\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb276b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = Accuracy(task='multiclass', num_classes=10, average='macro')\n",
    "\n",
    "# opt = torch.optim.SGD([torch.tensor([0.], device='cuda', requires_grad=True)], lr=1e-2 * 5)\n",
    "\n",
    "opt = torch.optim.AdamW(controller.traced_model.parameters(), lr=(1e-4 * 8), fused=True, capturable=True)\n",
    "# opt = SGD_LARS(\n",
    "#     controller.traced_model.parameters(),\n",
    "#     lr=1e-3 * 3,                     # LARS needs high LR\n",
    "#     momentum=0.9,\n",
    "#     weight_decay=1e-7,          # decoupled weight decay\n",
    "#     eta=0.1,                  # LARS coefficient (0.001–0.02)\n",
    "#     nesterov=True\n",
    "# )\n",
    "# opt = torch.optim.AdamW(model.parameters(), lr=(1e-4), fused=True, capturable=True)\n",
    "# opt = SGD_LARS(\n",
    "#     model.parameters(),\n",
    "#     lr=1.0,                     # LARS needs high LR\n",
    "#     momentum=0.9,\n",
    "#     weight_decay=1e-7,          # decoupled weight decay\n",
    "#     eta=0.001,                  # LARS coefficient (0.001–0.02)\n",
    "#     nesterov=False\n",
    "# )\n",
    "\n",
    "\n",
    "total_timer_start = torch.cuda.Event(enable_timing=True)\n",
    "total_timer_end = torch.cuda.Event(enable_timing=True)\n",
    "total_time = 0.0\n",
    "\n",
    "e_timer_start = torch.cuda.Event(enable_timing=True)\n",
    "e_timer_end = torch.cuda.Event(enable_timing=True)\n",
    "partile_time = 0.0\n",
    "\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "#     opt,\n",
    "#     1e-3,\n",
    "#     total_steps= num_epochs * len(train_loader),\n",
    "#     pct_start=0.1,\n",
    "#     div_factor=10,\n",
    "#     final_div_factor=1000\n",
    "# )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "#     opt,\n",
    "#     T_0=10,\n",
    "#     T_mult=2,\n",
    "#     eta_min=1e-5\n",
    "# )\n",
    "\n",
    "warmup_sched = LinearLR(\n",
    "    opt,\n",
    "    start_factor=0.5,   # lr = lr * start_factor\n",
    "    end_factor=1.0,    # lr = lr * end_factor = lr\n",
    "    total_iters=warmup_epochs\n",
    ")\n",
    "\n",
    "# 2. Cosine decay\n",
    "cosine_sched = CosineAnnealingLR(\n",
    "    opt,\n",
    "    T_max=num_epochs - warmup_epochs,\n",
    "    eta_min=1e-5\n",
    ")\n",
    "\n",
    "# 3. Combine them\n",
    "scheduler = SequentialLR(\n",
    "    opt,\n",
    "    schedulers=[warmup_sched, cosine_sched],\n",
    "    milestones=[warmup_epochs]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f1d2f4",
   "metadata": {},
   "source": [
    "cuda_graph_generator = cuda_utils.Graph(controller.traced_model,\n",
    "                                        criterion,\n",
    "                                        opt,\n",
    "                                        train_loader,\n",
    "                                        compute_stream,\n",
    "                                        mode='qdrop',\n",
    "                                        num_of_graph=1,\n",
    "                                        device='cuda')\n",
    "\n",
    "\n",
    "# cuda_graph_generator = cuda_utils.Graph(model,\n",
    "#                                         criterion,\n",
    "#                                         opt,\n",
    "#                                         train_loader,\n",
    "#                                         compute_stream,\n",
    "#                                         mode='qdrop',\n",
    "#                                         num_of_graph=1,\n",
    "#                                         device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530033ff",
   "metadata": {},
   "source": [
    "graphs, compute_stream, static_x, static_y, logits, loss = cuda_graph_generator.capture_cuda_graph_qdrop()\n",
    "\n",
    "total_graphs = cuda_graph_generator.num_of_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "833f0627",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def lowpass_param_2d(t: torch.Tensor, k: int = 3):\n",
    "    \"\"\"\n",
    "    Low-pass filter for parameter tensors that have 2D structure in the last two dims\n",
    "    (e.g. conv weights [out_c, in_c, kH, kW] or activation-like grads [B, C, H, W]).\n",
    "\n",
    "    For non-4D tensors or very small spatial dims, returns t unchanged.\n",
    "    \"\"\"\n",
    "    if t.ndim != 4:\n",
    "        return t\n",
    "\n",
    "    *prefix, H, W = t.shape\n",
    "    if H < k or W < k:\n",
    "        return t  # nothing to do, kernel larger than spatial size\n",
    "\n",
    "    # Merge prefix dims into batch*channels, so shape -> [N, 1, H, W]\n",
    "    t_flat = t.view(-1, 1, H, W)\n",
    "    t_l = F.avg_pool2d(t_flat, kernel_size=k, stride=1, padding=k // 2)\n",
    "    t_l = F.interpolate(t_l, size=(H, W), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    return t_l.view_as(t)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def inject_grad_noise_large_batch(\n",
    "    model,\n",
    "    step,\n",
    "    batch_size,\n",
    "    total_samples,\n",
    "    base_std=1,\n",
    "    gamma=0.55,\n",
    "    lp_kernel: int = 3\n",
    "):\n",
    "    # Large-batch SGD noise recovery term\n",
    "    gns_scale = (1.0 / batch_size - 1.0 / total_samples) ** 0.5\n",
    "    decay = 1.0 / ((1 + step) ** gamma)\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.grad is None:\n",
    "            continue\n",
    "\n",
    "        # Skip bias / norm (critical for ViT & BN stability)\n",
    "        if p.ndim == 1:\n",
    "            continue\n",
    "\n",
    "\n",
    "        grad_std = p.grad.std().clamp(min=1e-6)\n",
    "\n",
    "        noise = torch.randn_like(p.grad) * base_std * gns_scale * grad_std * decay\n",
    "\n",
    "        if lp_kernel is not None and lp_kernel > 1:\n",
    "            noise = lowpass_param_2d(noise, k=lp_kernel)\n",
    "\n",
    "        p.grad.add_(noise)\n",
    "\n",
    "\n",
    "def update_lr(model, scheduler):\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, 'ema_grad_meta'):\n",
    "            m.ema_grad_meta['lr'].copy_(lr)\n",
    "\n",
    "\n",
    "def checkpoint_save_helper(model, acc, max_acc, opt):\n",
    "    if acc >= max_acc:\n",
    "        max_acc = acc\n",
    "        torch.save(model.state_dict(), \"/home/hice1/yyu496/scratch/Model_Checkpoint/ResNet18_bs_8192_2.pth\")\n",
    "        torch.save(opt.state_dict(), '/home/hice1/yyu496/scratch/Model_Checkpoint/ResNet18_bs_8192_opt_2.pth')\n",
    "        return max_acc\n",
    "    return max_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441888b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train Loss: 2.0998802185058594\n",
      "Learning Rate: 0.0004\n",
      "Train Accuracy: 0.19668948650360107\n",
      "Peak Mem Reserved: 9412018176\n",
      "Peak Mem Allocated: 8862983168\n",
      "Current train time: 53.52324494934082 s\n",
      "Throughout: 934.1735548232261 samples per second\n",
      "Val Loss: 1.8671875\n",
      "Val Accuracy: 0.328900009393692\n",
      "\n",
      "\n",
      "Epoch: 1\n",
      "Train Loss: 1.9191701412200928\n",
      "Learning Rate: 0.00041600000000000003\n",
      "Train Accuracy: 0.29376572370529175\n",
      "Peak Mem Reserved: 9684647936\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.300074081420895 s\n",
      "Throughout: 1340.4799114033106 samples per second\n",
      "Val Loss: 1.6015625\n",
      "Val Accuracy: 0.42499998211860657\n",
      "\n",
      "\n",
      "Epoch: 2\n",
      "Train Loss: 1.7935917377471924\n",
      "Learning Rate: 0.0004320000000000001\n",
      "Train Accuracy: 0.3921717703342438\n",
      "Peak Mem Reserved: 9684647936\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.652419174194335 s\n",
      "Throughout: 1327.9359227538896 samples per second\n",
      "Val Loss: 1.0859375\n",
      "Val Accuracy: 0.5252000689506531\n",
      "\n",
      "\n",
      "Epoch: 3\n",
      "Train Loss: 1.6243280172348022\n",
      "Learning Rate: 0.00044800000000000005\n",
      "Train Accuracy: 0.46312278509140015\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.058839874267576 s\n",
      "Throughout: 1349.2057541369052 samples per second\n",
      "Val Loss: 0.87890625\n",
      "Val Accuracy: 0.5956000089645386\n",
      "\n",
      "\n",
      "Epoch: 4\n",
      "Train Loss: 1.6268692016601562\n",
      "Learning Rate: 0.0004640000000000001\n",
      "Train Accuracy: 0.5069968700408936\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.23493637084961 s\n",
      "Throughout: 1307.7045431706301 samples per second\n",
      "Val Loss: 0.5390625\n",
      "Val Accuracy: 0.597599983215332\n",
      "\n",
      "\n",
      "Epoch: 5\n",
      "Train Loss: 1.5290515422821045\n",
      "Learning Rate: 0.0004800000000000001\n",
      "Train Accuracy: 0.5445412993431091\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.252435882568356 s\n",
      "Throughout: 1342.1941093359924 samples per second\n",
      "Val Loss: 0.41796875\n",
      "Val Accuracy: 0.6433999538421631\n",
      "\n",
      "\n",
      "Epoch: 6\n",
      "Train Loss: 1.4944946765899658\n",
      "Learning Rate: 0.0004960000000000001\n",
      "Train Accuracy: 0.573107123374939\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 39.568325317382815 s\n",
      "Throughout: 1263.6370025504827 samples per second\n",
      "Val Loss: 0.57421875\n",
      "Val Accuracy: 0.6376999616622925\n",
      "\n",
      "\n",
      "Epoch: 7\n",
      "Train Loss: 1.411497950553894\n",
      "Learning Rate: 0.0005120000000000001\n",
      "Train Accuracy: 0.5911350846290588\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.40104988098145 s\n",
      "Throughout: 1373.586755422778 samples per second\n",
      "Val Loss: 0.515625\n",
      "Val Accuracy: 0.6619000434875488\n",
      "\n",
      "\n",
      "Epoch: 8\n",
      "Train Loss: 1.3755042552947998\n",
      "Learning Rate: 0.000528\n",
      "Train Accuracy: 0.612882673740387\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.45488652038574 s\n",
      "Throughout: 1300.224874503113 samples per second\n",
      "Val Loss: 0.49609375\n",
      "Val Accuracy: 0.7096999883651733\n",
      "\n",
      "\n",
      "Epoch: 9\n",
      "Train Loss: 1.3489956855773926\n",
      "Learning Rate: 0.000544\n",
      "Train Accuracy: 0.6300317645072937\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.57386985778808 s\n",
      "Throughout: 1367.0962409615765 samples per second\n",
      "Val Loss: 0.4140625\n",
      "Val Accuracy: 0.715499997138977\n",
      "\n",
      "\n",
      "Epoch: 10\n",
      "Train Loss: 1.3124973773956299\n",
      "Learning Rate: 0.00056\n",
      "Train Accuracy: 0.6444108486175537\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.613313903808596 s\n",
      "Throughout: 1294.890154327528 samples per second\n",
      "Val Loss: 0.3046875\n",
      "Val Accuracy: 0.7193999290466309\n",
      "\n",
      "\n",
      "Epoch: 11\n",
      "Train Loss: 1.3004270792007446\n",
      "Learning Rate: 0.0005759999999999999\n",
      "Train Accuracy: 0.6572107672691345\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.0804803314209 s\n",
      "Throughout: 1385.7908636670006 samples per second\n",
      "Val Loss: 0.439453125\n",
      "Val Accuracy: 0.7366000413894653\n",
      "\n",
      "\n",
      "Epoch: 12\n",
      "Train Loss: 1.270513892173767\n",
      "Learning Rate: 0.0005919999999999999\n",
      "Train Accuracy: 0.6719975471496582\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 39.19169421386719 s\n",
      "Throughout: 1275.7805193914917 samples per second\n",
      "Val Loss: 0.392578125\n",
      "Val Accuracy: 0.7395000457763672\n",
      "\n",
      "\n",
      "Epoch: 13\n",
      "Train Loss: 1.256469488143921\n",
      "Learning Rate: 0.0006079999999999998\n",
      "Train Accuracy: 0.6775758266448975\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.78476397705078 s\n",
      "Throughout: 1359.2584155547095 samples per second\n",
      "Val Loss: 0.43359375\n",
      "Val Accuracy: 0.7675000429153442\n",
      "\n",
      "\n",
      "Epoch: 14\n",
      "Train Loss: 1.2177462577819824\n",
      "Learning Rate: 0.0006239999999999999\n",
      "Train Accuracy: 0.6930475234985352\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.289386505126956 s\n",
      "Throughout: 1305.8448976011928 samples per second\n",
      "Val Loss: 0.470703125\n",
      "Val Accuracy: 0.7720000147819519\n",
      "\n",
      "\n",
      "Epoch: 15\n",
      "Train Loss: 1.219857931137085\n",
      "Learning Rate: 0.0006399999999999998\n",
      "Train Accuracy: 0.6999480724334717\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.55071130371094 s\n",
      "Throughout: 1331.5326997563095 samples per second\n",
      "Val Loss: 0.390625\n",
      "Val Accuracy: 0.7656999826431274\n",
      "\n",
      "\n",
      "Epoch: 16\n",
      "Train Loss: 1.2236270904541016\n",
      "Learning Rate: 0.0006559999999999998\n",
      "Train Accuracy: 0.7083432078361511\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.86377886962891 s\n",
      "Throughout: 1356.3449416520257 samples per second\n",
      "Val Loss: 0.609375\n",
      "Val Accuracy: 0.7932000160217285\n",
      "\n",
      "\n",
      "Epoch: 17\n",
      "Train Loss: 1.2131386995315552\n",
      "Learning Rate: 0.0006719999999999997\n",
      "Train Accuracy: 0.7184562087059021\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.75609782409668 s\n",
      "Throughout: 1360.318503865251 samples per second\n",
      "Val Loss: 0.4375\n",
      "Val Accuracy: 0.7972999811172485\n",
      "\n",
      "\n",
      "Epoch: 18\n",
      "Train Loss: 1.1886345148086548\n",
      "Learning Rate: 0.0006879999999999997\n",
      "Train Accuracy: 0.7225788235664368\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 39.254075256347654 s\n",
      "Throughout: 1273.7530988432763 samples per second\n",
      "Val Loss: 0.396484375\n",
      "Val Accuracy: 0.7968999743461609\n",
      "\n",
      "\n",
      "Epoch: 19\n",
      "Train Loss: 1.176755666732788\n",
      "Learning Rate: 0.0007039999999999998\n",
      "Train Accuracy: 0.7338032126426697\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.336722702026364 s\n",
      "Throughout: 1304.2325080478815 samples per second\n",
      "Val Loss: 0.462890625\n",
      "Val Accuracy: 0.794700026512146\n",
      "\n",
      "\n",
      "Epoch: 20\n",
      "Train Loss: 1.1400079727172852\n",
      "Learning Rate: 0.0007199999999999997\n",
      "Train Accuracy: 0.7380870580673218\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.58088165283203 s\n",
      "Throughout: 1330.4637305184692 samples per second\n",
      "Val Loss: 0.408203125\n",
      "Val Accuracy: 0.8065999746322632\n",
      "\n",
      "\n",
      "Epoch: 21\n",
      "Train Loss: 1.1324626207351685\n",
      "Learning Rate: 0.0007359999999999997\n",
      "Train Accuracy: 0.7432479858398438\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.918626678466794 s\n",
      "Throughout: 1354.3299006071388 samples per second\n",
      "Val Loss: 0.251953125\n",
      "Val Accuracy: 0.8216000199317932\n",
      "\n",
      "\n",
      "Epoch: 22\n",
      "Train Loss: 1.1658324003219604\n",
      "Learning Rate: 0.0007519999999999997\n",
      "Train Accuracy: 0.7486401796340942\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.300985717773436 s\n",
      "Throughout: 1340.4471500648747 samples per second\n",
      "Val Loss: 0.462890625\n",
      "Val Accuracy: 0.8095999956130981\n",
      "\n",
      "\n",
      "Epoch: 23\n",
      "Train Loss: 1.0837634801864624\n",
      "Learning Rate: 0.0007679999999999997\n",
      "Train Accuracy: 0.7516932487487793\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 34.882607620239256 s\n",
      "Throughout: 1433.3790794639297 samples per second\n",
      "Val Loss: 0.39453125\n",
      "Val Accuracy: 0.8162000179290771\n",
      "\n",
      "\n",
      "Epoch: 24\n",
      "Train Loss: 1.0972716808319092\n",
      "Learning Rate: 0.0007839999999999996\n",
      "Train Accuracy: 0.7546702027320862\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.86858244323731 s\n",
      "Throughout: 1286.3859924147923 samples per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/yyu496/.conda/envs/lib/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:209: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.376953125\n",
      "Val Accuracy: 0.8284000158309937\n",
      "\n",
      "\n",
      "Epoch: 25\n",
      "Train Loss: 1.1225162744522095\n",
      "Learning Rate: 0.0008\n",
      "Train Accuracy: 0.7581474781036377\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 39.49858935546875 s\n",
      "Throughout: 1265.8679921458329 samples per second\n",
      "Val Loss: 0.365234375\n",
      "Val Accuracy: 0.8249000310897827\n",
      "\n",
      "\n",
      "Epoch: 26\n",
      "Train Loss: 1.0700345039367676\n",
      "Learning Rate: 0.0007999917812188376\n",
      "Train Accuracy: 0.7645065784454346\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.9607936706543 s\n",
      "Throughout: 1352.784803419912 samples per second\n",
      "Val Loss: 0.310546875\n",
      "Val Accuracy: 0.8080000281333923\n",
      "\n",
      "\n",
      "Epoch: 27\n",
      "Train Loss: 1.048949956893921\n",
      "Learning Rate: 0.0007999671252173672\n",
      "Train Accuracy: 0.770566463470459\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.90036756896973 s\n",
      "Throughout: 1319.2484191350334 samples per second\n",
      "Val Loss: 0.232421875\n",
      "Val Accuracy: 0.8294999599456787\n",
      "\n",
      "\n",
      "Epoch: 28\n",
      "Train Loss: 1.1146903038024902\n",
      "Learning Rate: 0.0007999260330216256\n",
      "Train Accuracy: 0.7776342630386353\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.13661848449707 s\n",
      "Throughout: 1346.379989359366 samples per second\n",
      "Val Loss: 0.265625\n",
      "Val Accuracy: 0.8294999003410339\n",
      "\n",
      "\n",
      "Epoch: 29\n",
      "Train Loss: 1.0252506732940674\n",
      "Learning Rate: 0.0007998685063416271\n",
      "Train Accuracy: 0.7813456654548645\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.733240356445314 s\n",
      "Throughout: 1325.0916043169711 samples per second\n",
      "Val Loss: 0.275390625\n",
      "Val Accuracy: 0.839900016784668\n",
      "\n",
      "\n",
      "Epoch: 30\n",
      "Train Loss: 1.0180344581604004\n",
      "Learning Rate: 0.0007997945475712914\n",
      "Train Accuracy: 0.7840784788131714\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.01070622253418 s\n",
      "Throughout: 1350.9604409968597 samples per second\n",
      "Val Loss: 0.302734375\n",
      "Val Accuracy: 0.8443999886512756\n",
      "\n",
      "\n",
      "Epoch: 31\n",
      "Train Loss: 1.0511187314987183\n",
      "Learning Rate: 0.000799704159788345\n",
      "Train Accuracy: 0.7859991192817688\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.21478585815429 s\n",
      "Throughout: 1380.6515437048142 samples per second\n",
      "Val Loss: 0.283203125\n",
      "Val Accuracy: 0.8416000008583069\n",
      "\n",
      "\n",
      "Epoch: 32\n",
      "Train Loss: 1.0607423782348633\n",
      "Learning Rate: 0.0007995973467541924\n",
      "Train Accuracy: 0.7911166548728943\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.72109602355957 s\n",
      "Throughout: 1361.6151317466379 samples per second\n",
      "Val Loss: 0.33203125\n",
      "Val Accuracy: 0.8416000008583069\n",
      "\n",
      "\n",
      "Epoch: 33\n",
      "Train Loss: 1.0288841724395752\n",
      "Learning Rate: 0.0007994741129137599\n",
      "Train Accuracy: 0.79606693983078\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.5157554473877 s\n",
      "Throughout: 1332.7733749123158 samples per second\n",
      "Val Loss: 0.275390625\n",
      "Val Accuracy: 0.8554999828338623\n",
      "\n",
      "\n",
      "Epoch: 34\n",
      "Train Loss: 1.0131161212921143\n",
      "Learning Rate: 0.0007993344633953107\n",
      "Train Accuracy: 0.7999604940414429\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 35.817043106079105 s\n",
      "Throughout: 1395.9834666394802 samples per second\n",
      "Val Loss: 0.27734375\n",
      "Val Accuracy: 0.8460999727249146\n",
      "\n",
      "\n",
      "Epoch: 35\n",
      "Train Loss: 0.9859637022018433\n",
      "Learning Rate: 0.0007991784040102313\n",
      "Train Accuracy: 0.8030738830566406\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.30710475158691 s\n",
      "Throughout: 1340.2272927081851 samples per second\n",
      "Val Loss: 0.361328125\n",
      "Val Accuracy: 0.8537999987602234\n",
      "\n",
      "\n",
      "Epoch: 36\n",
      "Train Loss: 0.9949881434440613\n",
      "Learning Rate: 0.0007990059412527897\n",
      "Train Accuracy: 0.8026708960533142\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.808594497680666 s\n",
      "Throughout: 1322.4506402391446 samples per second\n",
      "Val Loss: 0.26171875\n",
      "Val Accuracy: 0.857200026512146\n",
      "\n",
      "\n",
      "Epoch: 37\n",
      "Train Loss: 1.01796293258667\n",
      "Learning Rate: 0.0007988170822998651\n",
      "Train Accuracy: 0.8099660873413086\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.280705383300784 s\n",
      "Throughout: 1306.1410310848537 samples per second\n",
      "Val Loss: 0.63671875\n",
      "Val Accuracy: 0.8514000177383423\n",
      "\n",
      "\n",
      "Epoch: 38\n",
      "Train Loss: 0.9912440180778503\n",
      "Learning Rate: 0.0007986118350106496\n",
      "Train Accuracy: 0.8114429712295532\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 39.23242601013184 s\n",
      "Throughout: 1274.4559815670696 samples per second\n",
      "Val Loss: 0.259765625\n",
      "Val Accuracy: 0.8466999530792236\n",
      "\n",
      "\n",
      "Epoch: 39\n",
      "Train Loss: 0.9648060202598572\n",
      "Learning Rate: 0.0007983902079263206\n",
      "Train Accuracy: 0.8161531686782837\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 39.300097061157224 s\n",
      "Throughout: 1272.2614888760204 samples per second\n",
      "Val Loss: 0.322265625\n",
      "Val Accuracy: 0.8641000390052795\n",
      "\n",
      "\n",
      "Epoch: 40\n",
      "Train Loss: 0.9803500175476074\n",
      "Learning Rate: 0.0007981522102696857\n",
      "Train Accuracy: 0.8159813284873962\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 39.02578675842285 s\n",
      "Throughout: 1281.2041512323542 samples per second\n",
      "Val Loss: 0.26953125\n",
      "Val Accuracy: 0.859499990940094\n",
      "\n",
      "\n",
      "Epoch: 41\n",
      "Train Loss: 0.9226459860801697\n",
      "Learning Rate: 0.000797897851944799\n",
      "Train Accuracy: 0.8165014386177063\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.314722763061525 s\n",
      "Throughout: 1339.9536777342976 samples per second\n",
      "Val Loss: 0.263671875\n",
      "Val Accuracy: 0.8449999690055847\n",
      "\n",
      "\n",
      "Epoch: 42\n",
      "Train Loss: 0.9041581153869629\n",
      "Learning Rate: 0.0007976271435365484\n",
      "Train Accuracy: 0.8222898244857788\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.25981483459473 s\n",
      "Throughout: 1341.9283005554917 samples per second\n",
      "Val Loss: 0.259765625\n",
      "Val Accuracy: 0.8416999578475952\n",
      "\n",
      "\n",
      "Epoch: 43\n",
      "Train Loss: 0.9101638197898865\n",
      "Learning Rate: 0.000797340096310216\n",
      "Train Accuracy: 0.8254979848861694\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.61252253723144 s\n",
      "Throughout: 1294.9166931994248 samples per second\n",
      "Val Loss: 0.314453125\n",
      "Val Accuracy: 0.8590999841690063\n",
      "\n",
      "\n",
      "Epoch: 44\n",
      "Train Loss: 0.9897509813308716\n",
      "Learning Rate: 0.0007970367222110083\n",
      "Train Accuracy: 0.8299590349197388\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 35.56092102050781 s\n",
      "Throughout: 1406.0378236875597 samples per second\n",
      "Val Loss: 0.291015625\n",
      "Val Accuracy: 0.8637999892234802\n",
      "\n",
      "\n",
      "Epoch: 45\n",
      "Train Loss: 0.9564162492752075\n",
      "Learning Rate: 0.0007967170338635601\n",
      "Train Accuracy: 0.8268590569496155\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 34.570356781005856 s\n",
      "Throughout: 1446.3258310215567 samples per second\n",
      "Val Loss: 0.369140625\n",
      "Val Accuracy: 0.865399956703186\n",
      "\n",
      "\n",
      "Epoch: 46\n",
      "Train Loss: 0.9435696601867676\n",
      "Learning Rate: 0.0007963810445714084\n",
      "Train Accuracy: 0.8314399719238281\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.490395706176756 s\n",
      "Throughout: 1299.0253563949782 samples per second\n",
      "Val Loss: 0.40234375\n",
      "Val Accuracy: 0.8702999949455261\n",
      "\n",
      "\n",
      "Epoch: 47\n",
      "Train Loss: 0.895775556564331\n",
      "Learning Rate: 0.0007960287683164392\n",
      "Train Accuracy: 0.8350375294685364\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.99434954833984 s\n",
      "Throughout: 1315.9851555396542 samples per second\n",
      "Val Loss: 0.263671875\n",
      "Val Accuracy: 0.8637999296188354\n",
      "\n",
      "\n",
      "Epoch: 48\n",
      "Train Loss: 0.938621461391449\n",
      "Learning Rate: 0.0007956602197583052\n",
      "Train Accuracy: 0.8390268683433533\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.589857666015625 s\n",
      "Throughout: 1330.1460315239283 samples per second\n",
      "Val Loss: 0.353515625\n",
      "Val Accuracy: 0.8689000010490417\n",
      "\n",
      "\n",
      "Epoch: 49\n",
      "Train Loss: 0.8973193764686584\n",
      "Learning Rate: 0.0007952754142338164\n",
      "Train Accuracy: 0.8411617875099182\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 35.65637762451172 s\n",
      "Throughout: 1402.2736837302245 samples per second\n",
      "Val Loss: 0.3671875\n",
      "Val Accuracy: 0.8775999546051025\n",
      "\n",
      "\n",
      "Epoch: 50\n",
      "Train Loss: 0.8596863746643066\n",
      "Learning Rate: 0.0007948743677563015\n",
      "Train Accuracy: 0.8416029810905457\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.088000610351564 s\n",
      "Throughout: 1312.7494013537425 samples per second\n",
      "Val Loss: 0.255859375\n",
      "Val Accuracy: 0.8689999580383301\n",
      "\n",
      "\n",
      "Epoch: 51\n",
      "Train Loss: 0.9226495027542114\n",
      "Learning Rate: 0.0007944570970149412\n",
      "Train Accuracy: 0.8434250354766846\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 34.470797744750975 s\n",
      "Throughout: 1450.5031293513864 samples per second\n",
      "Val Loss: 0.255859375\n",
      "Val Accuracy: 0.8664000034332275\n",
      "\n",
      "\n",
      "Epoch: 52\n",
      "Train Loss: 0.8906970024108887\n",
      "Learning Rate: 0.0007940236193740742\n",
      "Train Accuracy: 0.8475192785263062\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.00017721557617 s\n",
      "Throughout: 1351.3448789361803 samples per second\n",
      "Val Loss: 0.25\n",
      "Val Accuracy: 0.8698000311851501\n",
      "\n",
      "\n",
      "Epoch: 53\n",
      "Train Loss: 0.8866336345672607\n",
      "Learning Rate: 0.0007935739528724748\n",
      "Train Accuracy: 0.847506046295166\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 39.23045167541504 s\n",
      "Throughout: 1274.5201205861727 samples per second\n",
      "Val Loss: 0.2490234375\n",
      "Val Accuracy: 0.8725999593734741\n",
      "\n",
      "\n",
      "Epoch: 54\n",
      "Train Loss: 0.9302303791046143\n",
      "Learning Rate: 0.0007931081162226015\n",
      "Train Accuracy: 0.8497940897941589\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 34.58122833251953 s\n",
      "Throughout: 1445.871139082152 samples per second\n",
      "Val Loss: 0.234375\n",
      "Val Accuracy: 0.8592000007629395\n",
      "\n",
      "\n",
      "Epoch: 55\n",
      "Train Loss: 0.9137418866157532\n",
      "Learning Rate: 0.0007926261288098187\n",
      "Train Accuracy: 0.8499006628990173\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.05847497558594 s\n",
      "Throughout: 1313.7678278510741 samples per second\n",
      "Val Loss: 0.236328125\n",
      "Val Accuracy: 0.8751000165939331\n",
      "\n",
      "\n",
      "Epoch: 56\n",
      "Train Loss: 0.8953019976615906\n",
      "Learning Rate: 0.0007921280106915901\n",
      "Train Accuracy: 0.8515134453773499\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.30676405334473 s\n",
      "Throughout: 1377.1538528340368 samples per second\n",
      "Val Loss: 0.44140625\n",
      "Val Accuracy: 0.8794999718666077\n",
      "\n",
      "\n",
      "Epoch: 57\n",
      "Train Loss: 0.8842615485191345\n",
      "Learning Rate: 0.0007916137825966439\n",
      "Train Accuracy: 0.8564972877502441\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.906152252197266 s\n",
      "Throughout: 1319.0470947127508 samples per second\n",
      "Val Loss: 0.19921875\n",
      "Val Accuracy: 0.8837000131607056\n",
      "\n",
      "\n",
      "Epoch: 58\n",
      "Train Loss: 0.865385890007019\n",
      "Learning Rate: 0.00079108346592411\n",
      "Train Accuracy: 0.8569416403770447\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 39.11260697937012 s\n",
      "Throughout: 1278.3601979375196 samples per second\n",
      "Val Loss: 0.26953125\n",
      "Val Accuracy: 0.874000072479248\n",
      "\n",
      "\n",
      "Epoch: 59\n",
      "Train Loss: 0.8634220361709595\n",
      "Learning Rate: 0.0007905370827426297\n",
      "Train Accuracy: 0.8597851395606995\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 39.01693214416504 s\n",
      "Throughout: 1281.4949113695877 samples per second\n",
      "Val Loss: 0.2734375\n",
      "Val Accuracy: 0.8768999576568604\n",
      "\n",
      "\n",
      "Epoch: 60\n",
      "Train Loss: 0.8317668437957764\n",
      "Learning Rate: 0.0007899746557894378\n",
      "Train Accuracy: 0.8609140515327454\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.38357368469238 s\n",
      "Throughout: 1374.246533155605 samples per second\n",
      "Val Loss: 0.2734375\n",
      "Val Accuracy: 0.8648999333381653\n",
      "\n",
      "\n",
      "Epoch: 61\n",
      "Train Loss: 0.8484199643135071\n",
      "Learning Rate: 0.0007893962084694153\n",
      "Train Accuracy: 0.8598459362983704\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 35.248446563720705 s\n",
      "Throughout: 1418.502228448906 samples per second\n",
      "Val Loss: 0.30078125\n",
      "Val Accuracy: 0.8778000473976135\n",
      "\n",
      "\n",
      "Epoch: 62\n",
      "Train Loss: 0.8812039494514465\n",
      "Learning Rate: 0.0007888017648541162\n",
      "Train Accuracy: 0.8654074668884277\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 35.753320327758786 s\n",
      "Throughout: 1398.4715137402254 samples per second\n",
      "Val Loss: 0.263671875\n",
      "Val Accuracy: 0.8762999773025513\n",
      "\n",
      "\n",
      "Epoch: 63\n",
      "Train Loss: 0.8956079483032227\n",
      "Learning Rate: 0.0007881913496807662\n",
      "Train Accuracy: 0.8639525175094604\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 33.707718200683594 s\n",
      "Throughout: 1483.339800763672 samples per second\n",
      "Val Loss: 0.29296875\n",
      "Val Accuracy: 0.8762000203132629\n",
      "\n",
      "\n",
      "Epoch: 64\n",
      "Train Loss: 0.8888795375823975\n",
      "Learning Rate: 0.0007875649883512317\n",
      "Train Accuracy: 0.8686791658401489\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.338910766601565 s\n",
      "Throughout: 1304.1580733575988 samples per second\n",
      "Val Loss: 0.2353515625\n",
      "Val Accuracy: 0.8712000250816345\n",
      "\n",
      "\n",
      "Epoch: 65\n",
      "Train Loss: 0.8195157051086426\n",
      "Learning Rate: 0.0007869227069309648\n",
      "Train Accuracy: 0.8714219927787781\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.0617177734375 s\n",
      "Throughout: 1386.5118770584252 samples per second\n",
      "Val Loss: 0.224609375\n",
      "Val Accuracy: 0.8696998953819275\n",
      "\n",
      "\n",
      "Epoch: 66\n",
      "Train Loss: 0.8717115521430969\n",
      "Learning Rate: 0.0007862645321479169\n",
      "Train Accuracy: 0.8691978454589844\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 35.48319580078125 s\n",
      "Throughout: 1409.1177209832695 samples per second\n",
      "Val Loss: 0.33203125\n",
      "Val Accuracy: 0.8743999004364014\n",
      "\n",
      "\n",
      "Epoch: 67\n",
      "Train Loss: 0.8969910144805908\n",
      "Learning Rate: 0.0007855904913914273\n",
      "Train Accuracy: 0.87265545129776\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.43623356628418 s\n",
      "Throughout: 1300.8558685588648 samples per second\n",
      "Val Loss: 0.19140625\n",
      "Val Accuracy: 0.8800999522209167\n",
      "\n",
      "\n",
      "Epoch: 68\n",
      "Train Loss: 0.7975877523422241\n",
      "Learning Rate: 0.0007849006127110831\n",
      "Train Accuracy: 0.8751184344291687\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.4162253112793 s\n",
      "Throughout: 1373.0143520534887 samples per second\n",
      "Val Loss: 0.2265625\n",
      "Val Accuracy: 0.8870999813079834\n",
      "\n",
      "\n",
      "Epoch: 69\n",
      "Train Loss: 0.8544893860816956\n",
      "Learning Rate: 0.0007841949248155521\n",
      "Train Accuracy: 0.8737661838531494\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.34330696105957 s\n",
      "Throughout: 1304.00854706608 samples per second\n",
      "Val Loss: 0.345703125\n",
      "Val Accuracy: 0.8870999813079834\n",
      "\n",
      "\n",
      "Epoch: 70\n",
      "Train Loss: 0.8451308012008667\n",
      "Learning Rate: 0.0007834734570713886\n",
      "Train Accuracy: 0.8765224814414978\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.53734170532226 s\n",
      "Throughout: 1368.4629933741644 samples per second\n",
      "Val Loss: 0.2734375\n",
      "Val Accuracy: 0.8667999505996704\n",
      "\n",
      "\n",
      "Epoch: 71\n",
      "Train Loss: 0.8616852760314941\n",
      "Learning Rate: 0.0007827362395018098\n",
      "Train Accuracy: 0.877181887626648\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.60420896911621 s\n",
      "Throughout: 1365.9631339714545 samples per second\n",
      "Val Loss: 0.318359375\n",
      "Val Accuracy: 0.8723999857902527\n",
      "\n",
      "\n",
      "Epoch: 72\n",
      "Train Loss: 0.8435765504837036\n",
      "Learning Rate: 0.0007819833027854482\n",
      "Train Accuracy: 0.8769673109054565\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.681000869750974 s\n",
      "Throughout: 1363.1034817600234 samples per second\n",
      "Val Loss: 0.359375\n",
      "Val Accuracy: 0.8859000205993652\n",
      "\n",
      "\n",
      "Epoch: 73\n",
      "Train Loss: 0.846112072467804\n",
      "Learning Rate: 0.0007812146782550739\n",
      "Train Accuracy: 0.8804975152015686\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.62531866455078 s\n",
      "Throughout: 1365.175835272511 samples per second\n",
      "Val Loss: 0.330078125\n",
      "Val Accuracy: 0.8817999958992004\n",
      "\n",
      "\n",
      "Epoch: 74\n",
      "Train Loss: 0.8362456560134888\n",
      "Learning Rate: 0.0007804303978962911\n",
      "Train Accuracy: 0.8812913298606873\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 39.57293211364746 s\n",
      "Throughout: 1263.489899015003 samples per second\n",
      "Val Loss: 0.251953125\n",
      "Val Accuracy: 0.8863999843597412\n",
      "\n",
      "\n",
      "Epoch: 75\n",
      "Train Loss: 0.8519914746284485\n",
      "Learning Rate: 0.0007796304943462068\n",
      "Train Accuracy: 0.8813906311988831\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.30406977844238 s\n",
      "Throughout: 1340.3363305119715 samples per second\n",
      "Val Loss: 0.208984375\n",
      "Val Accuracy: 0.8862999677658081\n",
      "\n",
      "\n",
      "Epoch: 76\n",
      "Train Loss: 0.854251503944397\n",
      "Learning Rate: 0.0007788150008920726\n",
      "Train Accuracy: 0.8846167325973511\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.61858781433106 s\n",
      "Throughout: 1365.426767780269 samples per second\n",
      "Val Loss: 0.18359375\n",
      "Val Accuracy: 0.8868000507354736\n",
      "\n",
      "\n",
      "Epoch: 77\n",
      "Train Loss: 0.8394064903259277\n",
      "Learning Rate: 0.0007779839514699003\n",
      "Train Accuracy: 0.8857414126396179\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.92168551635742 s\n",
      "Throughout: 1318.5067941780337 samples per second\n",
      "Val Loss: 0.2734375\n",
      "Val Accuracy: 0.8831000328063965\n",
      "\n",
      "\n",
      "Epoch: 78\n",
      "Train Loss: 0.8422290086746216\n",
      "Learning Rate: 0.000777137380663048\n",
      "Train Accuracy: 0.8844553828239441\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.97505854797363 s\n",
      "Throughout: 1282.8717098258105 samples per second\n",
      "Val Loss: 0.20703125\n",
      "Val Accuracy: 0.8879000544548035\n",
      "\n",
      "\n",
      "Epoch: 79\n",
      "Train Loss: 0.7946612238883972\n",
      "Learning Rate: 0.000776275323700783\n",
      "Train Accuracy: 0.884887158870697\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.62552165222168 s\n",
      "Throughout: 1328.8852301413247 samples per second\n",
      "Val Loss: 0.2275390625\n",
      "Val Accuracy: 0.883400022983551\n",
      "\n",
      "\n",
      "Epoch: 80\n",
      "Train Loss: 0.8030046820640564\n",
      "Learning Rate: 0.000775397816456814\n",
      "Train Accuracy: 0.8902122378349304\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.91292362976074 s\n",
      "Throughout: 1354.5391446503552 samples per second\n",
      "Val Loss: 0.30859375\n",
      "Val Accuracy: 0.8833000063896179\n",
      "\n",
      "\n",
      "Epoch: 81\n",
      "Train Loss: 0.8166373372077942\n",
      "Learning Rate: 0.0007745048954477993\n",
      "Train Accuracy: 0.8884450793266296\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.801490371704105 s\n",
      "Throughout: 1288.6102961772413 samples per second\n",
      "Val Loss: 0.390625\n",
      "Val Accuracy: 0.8812999129295349\n",
      "\n",
      "\n",
      "Epoch: 82\n",
      "Train Loss: 0.7790380716323853\n",
      "Learning Rate: 0.0007735965978318268\n",
      "Train Accuracy: 0.8928244709968567\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 35.28679945373535 s\n",
      "Throughout: 1416.9604717354766 samples per second\n",
      "Val Loss: 0.234375\n",
      "Val Accuracy: 0.8805000185966492\n",
      "\n",
      "\n",
      "Epoch: 83\n",
      "Train Loss: 0.8263899087905884\n",
      "Learning Rate: 0.000772672961406868\n",
      "Train Accuracy: 0.8922504782676697\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.20900146484375 s\n",
      "Throughout: 1380.872102992022 samples per second\n",
      "Val Loss: 0.173828125\n",
      "Val Accuracy: 0.8915000557899475\n",
      "\n",
      "\n",
      "Epoch: 84\n",
      "Train Loss: 0.7794183492660522\n",
      "Learning Rate: 0.0007717340246092046\n",
      "Train Accuracy: 0.8948953151702881\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.24885528564453 s\n",
      "Throughout: 1307.22866414164 samples per second\n",
      "Val Loss: 0.22265625\n",
      "Val Accuracy: 0.8914000391960144\n",
      "\n",
      "\n",
      "Epoch: 85\n",
      "Train Loss: 0.7932382225990295\n",
      "Learning Rate: 0.0007707798265118292\n",
      "Train Accuracy: 0.8945350050926208\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.12634446716309 s\n",
      "Throughout: 1311.4291626637137 samples per second\n",
      "Val Loss: 0.28515625\n",
      "Val Accuracy: 0.8827999830245972\n",
      "\n",
      "\n",
      "Epoch: 86\n",
      "Train Loss: 0.7701342701911926\n",
      "Learning Rate: 0.0007698104068228197\n",
      "Train Accuracy: 0.8943476676940918\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 35.09264703369141 s\n",
      "Throughout: 1424.799900446281 samples per second\n",
      "Val Loss: 0.2080078125\n",
      "Val Accuracy: 0.8820000290870667\n",
      "\n",
      "\n",
      "Epoch: 87\n",
      "Train Loss: 0.7594724893569946\n",
      "Learning Rate: 0.0007688258058836863\n",
      "Train Accuracy: 0.8960643410682678\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.435623947143554 s\n",
      "Throughout: 1335.626195802064 samples per second\n",
      "Val Loss: 0.212890625\n",
      "Val Accuracy: 0.8755999803543091\n",
      "\n",
      "\n",
      "Epoch: 88\n",
      "Train Loss: 0.7937626242637634\n",
      "Learning Rate: 0.0007678260646676928\n",
      "Train Accuracy: 0.9011824727058411\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.9434486541748 s\n",
      "Throughout: 1353.4199383508214 samples per second\n",
      "Val Loss: 0.2119140625\n",
      "Val Accuracy: 0.8839999437332153\n",
      "\n",
      "\n",
      "Epoch: 89\n",
      "Train Loss: 0.7861042022705078\n",
      "Learning Rate: 0.0007668112247781524\n",
      "Train Accuracy: 0.9012497663497925\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.77017356872559 s\n",
      "Throughout: 1323.7958758389434 samples per second\n",
      "Val Loss: 0.279296875\n",
      "Val Accuracy: 0.8845000267028809\n",
      "\n",
      "\n",
      "Epoch: 90\n",
      "Train Loss: 0.7793689966201782\n",
      "Learning Rate: 0.0007657813284466951\n",
      "Train Accuracy: 0.9016730189323425\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.75314080810547 s\n",
      "Throughout: 1290.2180044602262 samples per second\n",
      "Val Loss: 0.271484375\n",
      "Val Accuracy: 0.880899965763092\n",
      "\n",
      "\n",
      "Epoch: 91\n",
      "Train Loss: 0.7874645590782166\n",
      "Learning Rate: 0.0007647364185315111\n",
      "Train Accuracy: 0.9013643264770508\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 35.164343185424805 s\n",
      "Throughout: 1421.8948932543803 samples per second\n",
      "Val Loss: 0.361328125\n",
      "Val Accuracy: 0.8788000345230103\n",
      "\n",
      "\n",
      "Epoch: 92\n",
      "Train Loss: 0.8086826205253601\n",
      "Learning Rate: 0.0007636765385155675\n",
      "Train Accuracy: 0.9031783938407898\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.067265090942385 s\n",
      "Throughout: 1348.8990859543562 samples per second\n",
      "Val Loss: 0.271484375\n",
      "Val Accuracy: 0.883699893951416\n",
      "\n",
      "\n",
      "Epoch: 93\n",
      "Train Loss: 0.7777671813964844\n",
      "Learning Rate: 0.0007626017325047974\n",
      "Train Accuracy: 0.9025465250015259\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 35.611170516967775 s\n",
      "Throughout: 1404.05382002752 samples per second\n",
      "Val Loss: 0.2470703125\n",
      "Val Accuracy: 0.8772000074386597\n",
      "\n",
      "\n",
      "Epoch: 94\n",
      "Train Loss: 0.7432868480682373\n",
      "Learning Rate: 0.0007615120452262664\n",
      "Train Accuracy: 0.9039883613586426\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.745863891601566 s\n",
      "Throughout: 1324.6484474057825 samples per second\n",
      "Val Loss: 0.1826171875\n",
      "Val Accuracy: 0.8923999667167664\n",
      "\n",
      "\n",
      "Epoch: 95\n",
      "Train Loss: 0.785865843296051\n",
      "Learning Rate: 0.0007604075220263103\n",
      "Train Accuracy: 0.9056428670883179\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.241953659057614 s\n",
      "Throughout: 1307.4645831583316 samples per second\n",
      "Val Loss: 0.20703125\n",
      "Val Accuracy: 0.880500078201294\n",
      "\n",
      "\n",
      "Epoch: 96\n",
      "Train Loss: 0.7787148356437683\n",
      "Learning Rate: 0.0007592882088686474\n",
      "Train Accuracy: 0.9057843089103699\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 33.24101127624512 s\n",
      "Throughout: 1504.1660310657062 samples per second\n",
      "Val Loss: 0.208984375\n",
      "Val Accuracy: 0.8946000337600708\n",
      "\n",
      "\n",
      "Epoch: 97\n",
      "Train Loss: 0.7337082624435425\n",
      "Learning Rate: 0.0007581541523324674\n",
      "Train Accuracy: 0.9072664976119995\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 39.4964600982666 s\n",
      "Throughout: 1265.936235186666 samples per second\n",
      "Val Loss: 0.2119140625\n",
      "Val Accuracy: 0.8822999596595764\n",
      "\n",
      "\n",
      "Epoch: 98\n",
      "Train Loss: 0.7706238031387329\n",
      "Learning Rate: 0.0007570053996104918\n",
      "Train Accuracy: 0.9068436026573181\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.98356855773926 s\n",
      "Throughout: 1351.951743703134 samples per second\n",
      "Val Loss: 0.306640625\n",
      "Val Accuracy: 0.8839999437332153\n",
      "\n",
      "\n",
      "Epoch: 99\n",
      "Train Loss: 0.7721220850944519\n",
      "Learning Rate: 0.0007558419985070103\n",
      "Train Accuracy: 0.9073031544685364\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.92546301269531 s\n",
      "Throughout: 1318.375466721733 samples per second\n",
      "Val Loss: 0.275390625\n",
      "Val Accuracy: 0.8851000070571899\n",
      "\n",
      "\n",
      "Epoch: 100\n",
      "Train Loss: 0.6940293908119202\n",
      "Learning Rate: 0.0007546639974358916\n",
      "Train Accuracy: 0.9113777279853821\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.075913879394534 s\n",
      "Throughout: 1313.1661175192016 samples per second\n",
      "Val Loss: 0.263671875\n",
      "Val Accuracy: 0.8916999697685242\n",
      "\n",
      "\n",
      "Epoch: 101\n",
      "Train Loss: 0.7473995089530945\n",
      "Learning Rate: 0.0007534714454185688\n",
      "Train Accuracy: 0.909819483757019\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.666364791870116 s\n",
      "Throughout: 1363.6475904774256 samples per second\n",
      "Val Loss: 0.263671875\n",
      "Val Accuracy: 0.8901999592781067\n",
      "\n",
      "\n",
      "Epoch: 102\n",
      "Train Loss: 0.7927662134170532\n",
      "Learning Rate: 0.0007522643920819991\n",
      "Train Accuracy: 0.9131650924682617\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.532357284545895 s\n",
      "Throughout: 1297.6107231325143 samples per second\n",
      "Val Loss: 0.203125\n",
      "Val Accuracy: 0.891200065612793\n",
      "\n",
      "\n",
      "Epoch: 103\n",
      "Train Loss: 0.7381899356842041\n",
      "Learning Rate: 0.0007510428876565988\n",
      "Train Accuracy: 0.9115583896636963\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.67237481689453 s\n",
      "Throughout: 1363.4241101005978 samples per second\n",
      "Val Loss: 0.302734375\n",
      "Val Accuracy: 0.8901999592781067\n",
      "\n",
      "\n",
      "Epoch: 104\n",
      "Train Loss: 0.7490326166152954\n",
      "Learning Rate: 0.000749806982974153\n",
      "Train Accuracy: 0.9121097922325134\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.48248463439941 s\n",
      "Throughout: 1333.9563929044523 samples per second\n",
      "Val Loss: 0.2392578125\n",
      "Val Accuracy: 0.8945000171661377\n",
      "\n",
      "\n",
      "Epoch: 105\n",
      "Train Loss: 0.7845861911773682\n",
      "Learning Rate: 0.0007485567294657003\n",
      "Train Accuracy: 0.9098963737487793\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.70399853515625 s\n",
      "Throughout: 1291.8561877937025 samples per second\n",
      "Val Loss: 0.140625\n",
      "Val Accuracy: 0.8848999738693237\n",
      "\n",
      "\n",
      "Epoch: 106\n",
      "Train Loss: 0.7285172939300537\n",
      "Learning Rate: 0.0007472921791593926\n",
      "Train Accuracy: 0.9141592383384705\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.96165663146973 s\n",
      "Throughout: 1352.753219330251 samples per second\n",
      "Val Loss: 0.271484375\n",
      "Val Accuracy: 0.8907000422477722\n",
      "\n",
      "\n",
      "Epoch: 107\n",
      "Train Loss: 0.764580488204956\n",
      "Learning Rate: 0.0007460133846783298\n",
      "Train Accuracy: 0.9143854975700378\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.53824237060547 s\n",
      "Throughout: 1331.9749898347075 samples per second\n",
      "Val Loss: 0.220703125\n",
      "Val Accuracy: 0.8910999298095703\n",
      "\n",
      "\n",
      "Epoch: 108\n",
      "Train Loss: 0.7279037237167358\n",
      "Learning Rate: 0.0007447203992383699\n",
      "Train Accuracy: 0.9169943928718567\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.70973522949219 s\n",
      "Throughout: 1291.6647376576727 samples per second\n",
      "Val Loss: 0.197265625\n",
      "Val Accuracy: 0.8864001035690308\n",
      "\n",
      "\n",
      "Epoch: 109\n",
      "Train Loss: 0.766545295715332\n",
      "Learning Rate: 0.0007434132766459151\n",
      "Train Accuracy: 0.9151620864868164\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.90415495300293 s\n",
      "Throughout: 1354.8609923103372 samples per second\n",
      "Val Loss: 0.1875\n",
      "Val Accuracy: 0.8931000232696533\n",
      "\n",
      "\n",
      "Epoch: 110\n",
      "Train Loss: 0.7490721344947815\n",
      "Learning Rate: 0.0007420920712956717\n",
      "Train Accuracy: 0.9184242486953735\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.3546060333252 s\n",
      "Throughout: 1375.3415441819525 samples per second\n",
      "Val Loss: 0.275390625\n",
      "Val Accuracy: 0.8924999833106995\n",
      "\n",
      "\n",
      "Epoch: 111\n",
      "Train Loss: 0.7654865980148315\n",
      "Learning Rate: 0.0007407568381683873\n",
      "Train Accuracy: 0.9169870018959045\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.49827583312988 s\n",
      "Throughout: 1333.3946398630092 samples per second\n",
      "Val Loss: 0.28125\n",
      "Val Accuracy: 0.8933999538421631\n",
      "\n",
      "\n",
      "Epoch: 112\n",
      "Train Loss: 0.7908150553703308\n",
      "Learning Rate: 0.0007394076328285626\n",
      "Train Accuracy: 0.9171260595321655\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 35.99748106384278 s\n",
      "Throughout: 1388.9860768680805 samples per second\n",
      "Val Loss: 0.3203125\n",
      "Val Accuracy: 0.8959000110626221\n",
      "\n",
      "\n",
      "Epoch: 113\n",
      "Train Loss: 0.7510578036308289\n",
      "Learning Rate: 0.0007380445114221391\n",
      "Train Accuracy: 0.9217719435691833\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 33.06401205444336 s\n",
      "Throughout: 1512.2181759935777 samples per second\n",
      "Val Loss: 0.23828125\n",
      "Val Accuracy: 0.8934999704360962\n",
      "\n",
      "\n",
      "Epoch: 114\n",
      "Train Loss: 0.7271384596824646\n",
      "Learning Rate: 0.0007366675306741623\n",
      "Train Accuracy: 0.9171172380447388\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.057637771606444 s\n",
      "Throughout: 1313.796728532199 samples per second\n",
      "Val Loss: 0.279296875\n",
      "Val Accuracy: 0.8821999430656433\n",
      "\n",
      "\n",
      "Epoch: 115\n",
      "Train Loss: 0.7462425231933594\n",
      "Learning Rate: 0.0007352767478864219\n",
      "Train Accuracy: 0.9215188026428223\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.591120086669925 s\n",
      "Throughout: 1295.6348478019665 samples per second\n",
      "Val Loss: 0.375\n",
      "Val Accuracy: 0.8851000666618347\n",
      "\n",
      "\n",
      "Epoch: 116\n",
      "Train Loss: 0.7181169986724854\n",
      "Learning Rate: 0.0007338722209350668\n",
      "Train Accuracy: 0.9224960803985596\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.57450389099121 s\n",
      "Throughout: 1367.0725418182822 samples per second\n",
      "Val Loss: 0.1826171875\n",
      "Val Accuracy: 0.8810999393463135\n",
      "\n",
      "\n",
      "Epoch: 117\n",
      "Train Loss: 0.7255483269691467\n",
      "Learning Rate: 0.0007324540082681961\n",
      "Train Accuracy: 0.9226351976394653\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.0527559967041 s\n",
      "Throughout: 1349.42728698636 samples per second\n",
      "Val Loss: 0.2080078125\n",
      "Val Accuracy: 0.8734999895095825\n",
      "\n",
      "\n",
      "Epoch: 118\n",
      "Train Loss: 0.7334712147712708\n",
      "Learning Rate: 0.0007310221689034283\n",
      "Train Accuracy: 0.9226100444793701\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.377468032836916 s\n",
      "Throughout: 1374.4771888704956 samples per second\n",
      "Val Loss: 0.28125\n",
      "Val Accuracy: 0.8871999979019165\n",
      "\n",
      "\n",
      "Epoch: 119\n",
      "Train Loss: 0.6985984444618225\n",
      "Learning Rate: 0.0007295767624254434\n",
      "Train Accuracy: 0.9230926036834717\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.04079678344726 s\n",
      "Throughout: 1314.3783576519763 samples per second\n",
      "Val Loss: 0.1572265625\n",
      "Val Accuracy: 0.89410001039505\n",
      "\n",
      "\n",
      "Epoch: 120\n",
      "Train Loss: 0.7451931834220886\n",
      "Learning Rate: 0.0007281178489835052\n",
      "Train Accuracy: 0.9254803657531738\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.5023521270752 s\n",
      "Throughout: 1298.6219604188689 samples per second\n",
      "Val Loss: 0.267578125\n",
      "Val Accuracy: 0.8985999822616577\n",
      "\n",
      "\n",
      "Epoch: 121\n",
      "Train Loss: 0.7278549671173096\n",
      "Learning Rate: 0.0007266454892889569\n",
      "Train Accuracy: 0.9249774217605591\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.53505989074707 s\n",
      "Throughout: 1297.5197168956745 samples per second\n",
      "Val Loss: 0.2890625\n",
      "Val Accuracy: 0.8925999402999878\n",
      "\n",
      "\n",
      "Epoch: 122\n",
      "Train Loss: 0.7312551140785217\n",
      "Learning Rate: 0.0007251597446126954\n",
      "Train Accuracy: 0.9234479665756226\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.72618754577637 s\n",
      "Throughout: 1361.4263647071682 samples per second\n",
      "Val Loss: 0.251953125\n",
      "Val Accuracy: 0.8974000215530396\n",
      "\n",
      "\n",
      "Epoch: 123\n",
      "Train Loss: 0.711747407913208\n",
      "Learning Rate: 0.0007236606767826211\n",
      "Train Accuracy: 0.924384355545044\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.934841262817386 s\n",
      "Throughout: 1284.1968370306365 samples per second\n",
      "Val Loss: 0.27734375\n",
      "Val Accuracy: 0.8891000151634216\n",
      "\n",
      "\n",
      "Epoch: 124\n",
      "Train Loss: 0.7087526321411133\n",
      "Learning Rate: 0.0007221483481810655\n",
      "Train Accuracy: 0.9269288778305054\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.13644741821289 s\n",
      "Throughout: 1346.3861913586925 samples per second\n",
      "Val Loss: 0.1884765625\n",
      "Val Accuracy: 0.8938999772071838\n",
      "\n",
      "\n",
      "Epoch: 125\n",
      "Train Loss: 0.7908589839935303\n",
      "Learning Rate: 0.0007206228217421949\n",
      "Train Accuracy: 0.9284923672676086\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.4977884979248 s\n",
      "Throughout: 1298.7759024832094 samples per second\n",
      "Val Loss: 0.2109375\n",
      "Val Accuracy: 0.8962000012397766\n",
      "\n",
      "\n",
      "Epoch: 126\n",
      "Train Loss: 0.703976035118103\n",
      "Learning Rate: 0.000719084160949391\n",
      "Train Accuracy: 0.9284275770187378\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.62119831848145 s\n",
      "Throughout: 1329.0379422985434 samples per second\n",
      "Val Loss: 0.302734375\n",
      "Val Accuracy: 0.8881000876426697\n",
      "\n",
      "\n",
      "Epoch: 127\n",
      "Train Loss: 0.7618981003761292\n",
      "Learning Rate: 0.0007175324298326104\n",
      "Train Accuracy: 0.9295996427536011\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.41952647399902 s\n",
      "Throughout: 1372.8898983817508 samples per second\n",
      "Val Loss: 0.236328125\n",
      "Val Accuracy: 0.8891999125480652\n",
      "\n",
      "\n",
      "Epoch: 128\n",
      "Train Loss: 0.6746429204940796\n",
      "Learning Rate: 0.0007159676929657184\n",
      "Train Accuracy: 0.9321989417076111\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.878888427734374 s\n",
      "Throughout: 1355.7892369228252 samples per second\n",
      "Val Loss: 0.244140625\n",
      "Val Accuracy: 0.8916999697685242\n",
      "\n",
      "\n",
      "Epoch: 129\n",
      "Train Loss: 0.7644778490066528\n",
      "Learning Rate: 0.0007143900154638033\n",
      "Train Accuracy: 0.9288113713264465\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.85261291503906 s\n",
      "Throughout: 1320.9127758822353 samples per second\n",
      "Val Loss: 0.25390625\n",
      "Val Accuracy: 0.8905999660491943\n",
      "\n",
      "\n",
      "Epoch: 130\n",
      "Train Loss: 0.7173641324043274\n",
      "Learning Rate: 0.0007127994629804655\n",
      "Train Accuracy: 0.9327860474586487\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.316992568969724 s\n",
      "Throughout: 1304.9040816551853 samples per second\n",
      "Val Loss: 0.27734375\n",
      "Val Accuracy: 0.8750999569892883\n",
      "\n",
      "\n",
      "Epoch: 131\n",
      "Train Loss: 0.6872476935386658\n",
      "Learning Rate: 0.0007111961017050864\n",
      "Train Accuracy: 0.930669903755188\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.135933837890626 s\n",
      "Throughout: 1346.4048115301164 samples per second\n",
      "Val Loss: 0.275390625\n",
      "Val Accuracy: 0.8908000588417053\n",
      "\n",
      "\n",
      "Epoch: 132\n",
      "Train Loss: 0.7210462689399719\n",
      "Learning Rate: 0.0007095799983600733\n",
      "Train Accuracy: 0.9313076734542847\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.70037953186035 s\n",
      "Throughout: 1291.97699363225 samples per second\n",
      "Val Loss: 0.259765625\n",
      "Val Accuracy: 0.8884000182151794\n",
      "\n",
      "\n",
      "Epoch: 133\n",
      "Train Loss: 0.7099606990814209\n",
      "Learning Rate: 0.000707951220198083\n",
      "Train Accuracy: 0.9297399520874023\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 34.44019799804688 s\n",
      "Throughout: 1451.7918858316532 samples per second\n",
      "Val Loss: 0.1845703125\n",
      "Val Accuracy: 0.8822000026702881\n",
      "\n",
      "\n",
      "Epoch: 134\n",
      "Train Loss: 0.7204914689064026\n",
      "Learning Rate: 0.0007063098349992233\n",
      "Train Accuracy: 0.9314895272254944\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 39.071412460327146 s\n",
      "Throughout: 1279.7080231171492 samples per second\n",
      "Val Loss: 0.220703125\n",
      "Val Accuracy: 0.8937999606132507\n",
      "\n",
      "\n",
      "Epoch: 135\n",
      "Train Loss: 0.6746950745582581\n",
      "Learning Rate: 0.000704655911068232\n",
      "Train Accuracy: 0.9333893060684204\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.11979277038574 s\n",
      "Throughout: 1311.6545596450271 samples per second\n",
      "Val Loss: 0.2333984375\n",
      "Val Accuracy: 0.8810999393463135\n",
      "\n",
      "\n",
      "Epoch: 136\n",
      "Train Loss: 0.7168024778366089\n",
      "Learning Rate: 0.0007029895172316353\n",
      "Train Accuracy: 0.9346147775650024\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.82903704833984 s\n",
      "Throughout: 1321.73599703602 samples per second\n",
      "Val Loss: 0.2333984375\n",
      "Val Accuracy: 0.8918999433517456\n",
      "\n",
      "\n",
      "Epoch: 137\n",
      "Train Loss: 0.7136873006820679\n",
      "Learning Rate: 0.0007013107228348822\n",
      "Train Accuracy: 0.9349867701530457\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.007561309814456 s\n",
      "Throughout: 1351.0752459860123 samples per second\n",
      "Val Loss: 0.236328125\n",
      "Val Accuracy: 0.8869001269340515\n",
      "\n",
      "\n",
      "Epoch: 138\n",
      "Train Loss: 0.6841669678688049\n",
      "Learning Rate: 0.0006996195977394606\n",
      "Train Accuracy: 0.9349534511566162\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.5935897064209 s\n",
      "Throughout: 1366.3595291179302 samples per second\n",
      "Val Loss: 0.2412109375\n",
      "Val Accuracy: 0.8891000151634216\n",
      "\n",
      "\n",
      "Epoch: 139\n",
      "Train Loss: 0.7051724195480347\n",
      "Learning Rate: 0.0006979162123199882\n",
      "Train Accuracy: 0.9344634413719177\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 35.201709213256834 s\n",
      "Throughout: 1420.385575515469 samples per second\n",
      "Val Loss: 0.220703125\n",
      "Val Accuracy: 0.8971999883651733\n",
      "\n",
      "\n",
      "Epoch: 140\n",
      "Train Loss: 0.6869478821754456\n",
      "Learning Rate: 0.0006962006374612854\n",
      "Train Accuracy: 0.9359169602394104\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.60211012268066 s\n",
      "Throughout: 1329.7126101931508 samples per second\n",
      "Val Loss: 0.162109375\n",
      "Val Accuracy: 0.8905999660491943\n",
      "\n",
      "\n",
      "Epoch: 141\n",
      "Train Loss: 0.6922652125358582\n",
      "Learning Rate: 0.0006944729445554249\n",
      "Train Accuracy: 0.9353651404380798\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.34231610107422 s\n",
      "Throughout: 1304.0422458621163 samples per second\n",
      "Val Loss: 0.21484375\n",
      "Val Accuracy: 0.8850000500679016\n",
      "\n",
      "\n",
      "Epoch: 142\n",
      "Train Loss: 0.7047203183174133\n",
      "Learning Rate: 0.0006927332054987605\n",
      "Train Accuracy: 0.9363756775856018\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.18023666381836 s\n",
      "Throughout: 1309.5780531759426 samples per second\n",
      "Val Loss: 0.173828125\n",
      "Val Accuracy: 0.8886999487876892\n",
      "\n",
      "\n",
      "Epoch: 143\n",
      "Train Loss: 0.7104240655899048\n",
      "Learning Rate: 0.0006909814926889357\n",
      "Train Accuracy: 0.9364120960235596\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.65599627685547 s\n",
      "Throughout: 1364.0333118314372 samples per second\n",
      "Val Loss: 0.2734375\n",
      "Val Accuracy: 0.8937000632286072\n",
      "\n",
      "\n",
      "Epoch: 144\n",
      "Train Loss: 0.728380024433136\n",
      "Learning Rate: 0.0006892178790218709\n",
      "Train Accuracy: 0.937908411026001\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.938417755126956 s\n",
      "Throughout: 1284.078883596049 samples per second\n",
      "Val Loss: 0.408203125\n",
      "Val Accuracy: 0.8890999555587769\n",
      "\n",
      "\n",
      "Epoch: 145\n",
      "Train Loss: 0.7323904633522034\n",
      "Learning Rate: 0.0006874424378887292\n",
      "Train Accuracy: 0.9381569623947144\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.383615859985355 s\n",
      "Throughout: 1337.4843189933097 samples per second\n",
      "Val Loss: 0.169921875\n",
      "Val Accuracy: 0.8892999887466431\n",
      "\n",
      "\n",
      "Epoch: 146\n",
      "Train Loss: 0.6899076700210571\n",
      "Learning Rate: 0.0006856552431728634\n",
      "Train Accuracy: 0.939725935459137\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.09058520507813 s\n",
      "Throughout: 1348.050987158715 samples per second\n",
      "Val Loss: 0.2021484375\n",
      "Val Accuracy: 0.8970000147819519\n",
      "\n",
      "\n",
      "Epoch: 147\n",
      "Train Loss: 0.6989685893058777\n",
      "Learning Rate: 0.0006838563692467409\n",
      "Train Accuracy: 0.9393242597579956\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.803065704345705 s\n",
      "Throughout: 1322.6440519677794 samples per second\n",
      "Val Loss: 0.30859375\n",
      "Val Accuracy: 0.8950000405311584\n",
      "\n",
      "\n",
      "Epoch: 148\n",
      "Train Loss: 0.6751746535301208\n",
      "Learning Rate: 0.000682045890968848\n",
      "Train Accuracy: 0.9379780888557434\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.44233549499512 s\n",
      "Throughout: 1300.6493844919905 samples per second\n",
      "Val Loss: 0.23828125\n",
      "Val Accuracy: 0.893500030040741\n",
      "\n",
      "\n",
      "Epoch: 149\n",
      "Train Loss: 0.6972268223762512\n",
      "Learning Rate: 0.0006802238836805763\n",
      "Train Accuracy: 0.9370310306549072\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.389921508789065 s\n",
      "Throughout: 1374.0068108672278 samples per second\n",
      "Val Loss: 0.2119140625\n",
      "Val Accuracy: 0.8992000222206116\n",
      "\n",
      "\n",
      "Epoch: 150\n",
      "Train Loss: 0.6883149147033691\n",
      "Learning Rate: 0.000678390423203086\n",
      "Train Accuracy: 0.9391797780990601\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.86780535888672 s\n",
      "Throughout: 1320.382829850638 samples per second\n",
      "Val Loss: 0.234375\n",
      "Val Accuracy: 0.887999951839447\n",
      "\n",
      "\n",
      "Epoch: 151\n",
      "Train Loss: 0.6920186877250671\n",
      "Learning Rate: 0.0006765455858341517\n",
      "Train Accuracy: 0.9431958198547363\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 33.98336734008789 s\n",
      "Throughout: 1471.307993102213 samples per second\n",
      "Val Loss: 0.224609375\n",
      "Val Accuracy: 0.8982999920845032\n",
      "\n",
      "\n",
      "Epoch: 152\n",
      "Train Loss: 0.6749480962753296\n",
      "Learning Rate: 0.0006746894483449864\n",
      "Train Accuracy: 0.939833402633667\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.120689514160155 s\n",
      "Throughout: 1346.9577385120197 samples per second\n",
      "Val Loss: 0.2412109375\n",
      "Val Accuracy: 0.904699981212616\n",
      "\n",
      "\n",
      "Epoch: 153\n",
      "Train Loss: 0.6892544031143188\n",
      "Learning Rate: 0.0006728220879770475\n",
      "Train Accuracy: 0.9399895668029785\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.481270385742185 s\n",
      "Throughout: 1370.5663062529006 samples per second\n",
      "Val Loss: 0.2734375\n",
      "Val Accuracy: 0.8878000974655151\n",
      "\n",
      "\n",
      "Epoch: 154\n",
      "Train Loss: 0.6701141595840454\n",
      "Learning Rate: 0.0006709435824388222\n",
      "Train Accuracy: 0.9429362416267395\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 34.681637283325195 s\n",
      "Throughout: 1441.685108218343 samples per second\n",
      "Val Loss: 0.2197265625\n",
      "Val Accuracy: 0.8902999758720398\n",
      "\n",
      "\n",
      "Epoch: 155\n",
      "Train Loss: 0.7079753875732422\n",
      "Learning Rate: 0.0006690540099025938\n",
      "Train Accuracy: 0.9408290386199951\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.31935556030273 s\n",
      "Throughout: 1339.787336874217 samples per second\n",
      "Val Loss: 0.2041015625\n",
      "Val Accuracy: 0.8934999704360962\n",
      "\n",
      "\n",
      "Epoch: 156\n",
      "Train Loss: 0.6748449802398682\n",
      "Learning Rate: 0.0006671534490011883\n",
      "Train Accuracy: 0.9406198859214783\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.17457217407227 s\n",
      "Throughout: 1309.7723734009369 samples per second\n",
      "Val Loss: 0.1689453125\n",
      "Val Accuracy: 0.8989999294281006\n",
      "\n",
      "\n",
      "Epoch: 157\n",
      "Train Loss: 0.6871477365493774\n",
      "Learning Rate: 0.0006652419788247026\n",
      "Train Accuracy: 0.9438721537590027\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 35.420919311523434 s\n",
      "Throughout: 1411.5952090417252 samples per second\n",
      "Val Loss: 0.2119140625\n",
      "Val Accuracy: 0.8906999826431274\n",
      "\n",
      "\n",
      "Epoch: 158\n",
      "Train Loss: 0.7287496328353882\n",
      "Learning Rate: 0.0006633196789172132\n",
      "Train Accuracy: 0.943020761013031\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.85145001220703 s\n",
      "Throughout: 1356.798714390818 samples per second\n",
      "Val Loss: 0.248046875\n",
      "Val Accuracy: 0.892300009727478\n",
      "\n",
      "\n",
      "Epoch: 159\n",
      "Train Loss: 0.6666579246520996\n",
      "Learning Rate: 0.0006613866292734657\n",
      "Train Accuracy: 0.9441387057304382\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 35.9228317565918 s\n",
      "Throughout: 1391.8724542317034 samples per second\n",
      "Val Loss: 0.2294921875\n",
      "Val Accuracy: 0.8917999267578125\n",
      "\n",
      "\n",
      "Epoch: 160\n",
      "Train Loss: 0.6610417366027832\n",
      "Learning Rate: 0.0006594429103355461\n",
      "Train Accuracy: 0.9448278546333313\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.632685668945314 s\n",
      "Throughout: 1328.6322544144186 samples per second\n",
      "Val Loss: 0.2412109375\n",
      "Val Accuracy: 0.8980998992919922\n",
      "\n",
      "\n",
      "Epoch: 161\n",
      "Train Loss: 0.7139798402786255\n",
      "Learning Rate: 0.0006574886029895332\n",
      "Train Accuracy: 0.9442933797836304\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.21048602294922 s\n",
      "Throughout: 1308.5413247549377 samples per second\n",
      "Val Loss: 0.25390625\n",
      "Val Accuracy: 0.8886000514030457\n",
      "\n",
      "\n",
      "Epoch: 162\n",
      "Train Loss: 0.655720055103302\n",
      "Learning Rate: 0.0006555237885621332\n",
      "Train Accuracy: 0.9458962678909302\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.781206665039065 s\n",
      "Throughout: 1359.3898768831186 samples per second\n",
      "Val Loss: 0.1982421875\n",
      "Val Accuracy: 0.896399974822998\n",
      "\n",
      "\n",
      "Epoch: 163\n",
      "Train Loss: 0.6678506135940552\n",
      "Learning Rate: 0.0006535485488172942\n",
      "Train Accuracy: 0.9433404803276062\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 35.67647834777832 s\n",
      "Throughout: 1401.4836193358094 samples per second\n",
      "Val Loss: 0.208984375\n",
      "Val Accuracy: 0.9041999578475952\n",
      "\n",
      "\n",
      "Epoch: 164\n",
      "Train Loss: 0.7030079960823059\n",
      "Learning Rate: 0.0006515629659528046\n",
      "Train Accuracy: 0.9457420706748962\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.74106707763672 s\n",
      "Throughout: 1324.8168075678827 samples per second\n",
      "Val Loss: 0.26953125\n",
      "Val Accuracy: 0.8970000743865967\n",
      "\n",
      "\n",
      "Epoch: 165\n",
      "Train Loss: 0.6711104512214661\n",
      "Learning Rate: 0.0006495671225968724\n",
      "Train Accuracy: 0.9446154832839966\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.04727153015137 s\n",
      "Throughout: 1349.6270557821483 samples per second\n",
      "Val Loss: 0.263671875\n",
      "Val Accuracy: 0.8962000608444214\n",
      "\n",
      "\n",
      "Epoch: 166\n",
      "Train Loss: 0.6915842294692993\n",
      "Learning Rate: 0.000647561101804686\n",
      "Train Accuracy: 0.9449083209037781\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.3352448425293 s\n",
      "Throughout: 1339.217144842292 samples per second\n",
      "Val Loss: 0.24609375\n",
      "Val Accuracy: 0.8929999470710754\n",
      "\n",
      "\n",
      "Epoch: 167\n",
      "Train Loss: 0.6819370985031128\n",
      "Learning Rate: 0.0006455449870549592\n",
      "Train Accuracy: 0.9459964036941528\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.58145899963379 s\n",
      "Throughout: 1295.9592844965919 samples per second\n",
      "Val Loss: 0.205078125\n",
      "Val Accuracy: 0.9000999927520752\n",
      "\n",
      "\n",
      "Epoch: 168\n",
      "Train Loss: 0.6611015796661377\n",
      "Learning Rate: 0.0006435188622464559\n",
      "Train Accuracy: 0.9493414759635925\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.17023567199707 s\n",
      "Throughout: 1382.3520657541612 samples per second\n",
      "Val Loss: 0.1767578125\n",
      "Val Accuracy: 0.8893000483512878\n",
      "\n",
      "\n",
      "Epoch: 169\n",
      "Train Loss: 0.6585207581520081\n",
      "Learning Rate: 0.0006414828116944999\n",
      "Train Accuracy: 0.94876629114151\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.611283950805664 s\n",
      "Throughout: 1365.6991671525277 samples per second\n",
      "Val Loss: 0.1669921875\n",
      "Val Accuracy: 0.8882999420166016\n",
      "\n",
      "\n",
      "Epoch: 170\n",
      "Train Loss: 0.6783352494239807\n",
      "Learning Rate: 0.0006394369201274654\n",
      "Train Accuracy: 0.9478051662445068\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.74496577453613 s\n",
      "Throughout: 1324.6799665594472 samples per second\n",
      "Val Loss: 0.228515625\n",
      "Val Accuracy: 0.8978999853134155\n",
      "\n",
      "\n",
      "Epoch: 171\n",
      "Train Loss: 0.679861843585968\n",
      "Learning Rate: 0.0006373812726832515\n",
      "Train Accuracy: 0.9463661909103394\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.5565362701416 s\n",
      "Throughout: 1296.7969853329455 samples per second\n",
      "Val Loss: 0.212890625\n",
      "Val Accuracy: 0.8946999907493591\n",
      "\n",
      "\n",
      "Epoch: 172\n",
      "Train Loss: 0.6658258438110352\n",
      "Learning Rate: 0.0006353159549057391\n",
      "Train Accuracy: 0.9474496245384216\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.88448239135742 s\n",
      "Throughout: 1319.8015874543528 samples per second\n",
      "Val Loss: 0.2578125\n",
      "Val Accuracy: 0.895300030708313\n",
      "\n",
      "\n",
      "Epoch: 173\n",
      "Train Loss: 0.6751013994216919\n",
      "Learning Rate: 0.0006332410527412313\n",
      "Train Accuracy: 0.9505912065505981\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.429798736572266 s\n",
      "Throughout: 1335.8340596992182 samples per second\n",
      "Val Loss: 0.216796875\n",
      "Val Accuracy: 0.8960000276565552\n",
      "\n",
      "\n",
      "Epoch: 174\n",
      "Train Loss: 0.6456418633460999\n",
      "Learning Rate: 0.0006311566525348765\n",
      "Train Accuracy: 0.948976457118988\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 35.24514588928223 s\n",
      "Throughout: 1418.6350698353785 samples per second\n",
      "Val Loss: 0.2138671875\n",
      "Val Accuracy: 0.8978999257087708\n",
      "\n",
      "\n",
      "Epoch: 175\n",
      "Train Loss: 0.669235110282898\n",
      "Learning Rate: 0.0006290628410270756\n",
      "Train Accuracy: 0.9507443904876709\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.428178848266604 s\n",
      "Throughout: 1335.8918744804391 samples per second\n",
      "Val Loss: 0.2275390625\n",
      "Val Accuracy: 0.9012000560760498\n",
      "\n",
      "\n",
      "Epoch: 176\n",
      "Train Loss: 0.6628643274307251\n",
      "Learning Rate: 0.0006269597053498718\n",
      "Train Accuracy: 0.9514085650444031\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.12624949645996 s\n",
      "Throughout: 1346.756019747364 samples per second\n",
      "Val Loss: 0.1865234375\n",
      "Val Accuracy: 0.8973999619483948\n",
      "\n",
      "\n",
      "Epoch: 177\n",
      "Train Loss: 0.6802065372467041\n",
      "Learning Rate: 0.0006248473330233254\n",
      "Train Accuracy: 0.9517518877983093\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.163772354125975 s\n",
      "Throughout: 1382.5991246263177 samples per second\n",
      "Val Loss: 0.287109375\n",
      "Val Accuracy: 0.8959000706672668\n",
      "\n",
      "\n",
      "Epoch: 178\n",
      "Train Loss: 0.6360598206520081\n",
      "Learning Rate: 0.0006227258119518714\n",
      "Train Accuracy: 0.9516330361366272\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.450908325195314 s\n",
      "Throughout: 1371.7079298525844 samples per second\n",
      "Val Loss: 0.2578125\n",
      "Val Accuracy: 0.8955000638961792\n",
      "\n",
      "\n",
      "Epoch: 179\n",
      "Train Loss: 0.6530574560165405\n",
      "Learning Rate: 0.0006205952304206611\n",
      "Train Accuracy: 0.9491394758224487\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.44525625610351 s\n",
      "Throughout: 1335.2826231987685 samples per second\n",
      "Val Loss: 0.2216796875\n",
      "Val Accuracy: 0.8914999961853027\n",
      "\n",
      "\n",
      "Epoch: 180\n",
      "Train Loss: 0.693427324295044\n",
      "Learning Rate: 0.0006184556770918888\n",
      "Train Accuracy: 0.9510534405708313\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 36.216133377075195 s\n",
      "Throughout: 1380.6001728403726 samples per second\n",
      "Val Loss: 0.224609375\n",
      "Val Accuracy: 0.8958000540733337\n",
      "\n",
      "\n",
      "Epoch: 181\n",
      "Train Loss: 0.6439973711967468\n",
      "Learning Rate: 0.0006163072410011017\n",
      "Train Accuracy: 0.9528147578239441\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.29164576721191 s\n",
      "Throughout: 1340.7828743230664 samples per second\n",
      "Val Loss: 0.310546875\n",
      "Val Accuracy: 0.8938999772071838\n",
      "\n",
      "\n",
      "Epoch: 182\n",
      "Train Loss: 0.6627888679504395\n",
      "Learning Rate: 0.0006141500115534953\n",
      "Train Accuracy: 0.9514552354812622\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.80498014831543 s\n",
      "Throughout: 1288.4944099673908 samples per second\n",
      "Val Loss: 0.240234375\n",
      "Val Accuracy: 0.8952999711036682\n",
      "\n",
      "\n",
      "Epoch: 183\n",
      "Train Loss: 0.6823939085006714\n",
      "Learning Rate: 0.0006119840785201922\n",
      "Train Accuracy: 0.9501042366027832\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.50475634765625 s\n",
      "Throughout: 1298.54087501695 samples per second\n",
      "Val Loss: 0.2373046875\n",
      "Val Accuracy: 0.8898000121116638\n",
      "\n",
      "\n",
      "Epoch: 184\n",
      "Train Loss: 0.6265498399734497\n",
      "Learning Rate: 0.000609809532034507\n",
      "Train Accuracy: 0.9519007205963135\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.08804188537598 s\n",
      "Throughout: 1312.7479787612199 samples per second\n",
      "Val Loss: 0.28515625\n",
      "Val Accuracy: 0.8896999359130859\n",
      "\n",
      "\n",
      "Epoch: 185\n",
      "Train Loss: 0.650484025478363\n",
      "Learning Rate: 0.0006076264625881951\n",
      "Train Accuracy: 0.9539865255355835\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 35.84833938598633 s\n",
      "Throughout: 1394.7647466076428 samples per second\n",
      "Val Loss: 0.279296875\n",
      "Val Accuracy: 0.8959000110626221\n",
      "\n",
      "\n",
      "Epoch: 186\n",
      "Train Loss: 0.6688709855079651\n",
      "Learning Rate: 0.0006054349610276869\n",
      "Train Accuracy: 0.9524349570274353\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 37.749381393432614 s\n",
      "Throughout: 1324.5250161555937 samples per second\n",
      "Val Loss: 0.287109375\n",
      "Val Accuracy: 0.8859999775886536\n",
      "\n",
      "\n",
      "Epoch: 187\n",
      "Train Loss: 0.628564178943634\n",
      "Learning Rate: 0.0006032351185503078\n",
      "Train Accuracy: 0.9535054564476013\n",
      "Peak Mem Reserved: 9810477056\n",
      "Peak Mem Allocated: 9109489152\n",
      "Current train time: 38.213699584960935 s\n",
      "Throughout: 1308.4312836247236 samples per second\n",
      "Val Loss: 0.2158203125\n",
      "Val Accuracy: 0.9001999497413635\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "G_GNS = []\n",
    "G_SNR = []\n",
    "\n",
    "\n",
    "def gradient_noise_scale(grad):\n",
    "    tr_sigma = grad.var(0, unbiased=False).sum()\n",
    "    return tr_sigma / (grad.mean(0).pow(2).sum() + 1e-12)\n",
    "\n",
    "def signal_to_noise_ratio(grad):\n",
    "    mu = grad.mean(0)\n",
    "    tr_sigma = grad.var(0, unbiased=False).sum()\n",
    "    return mu.norm() / (tr_sigma.sqrt() + 1e-12)\n",
    "\n",
    "max_acc = 0.0\n",
    "torch.cuda.synchronize()\n",
    "total_timer_start.record()\n",
    "for i in range(num_epochs):\n",
    "    train_logits = []\n",
    "    train_y = []\n",
    "    \n",
    "\n",
    "    val_logits = []\n",
    "    val_y = []\n",
    "    partile_time = 0\n",
    "    controller.traced_model.train()\n",
    "    # model.train()\n",
    "    # aot_model.train()\n",
    "    G = []\n",
    "    torch.cuda.synchronize()\n",
    "    # k = i % total_graphs\n",
    "    for step, (x, y) in enumerate(train_loader):\n",
    "        \n",
    "        # torch.cuda.current_stream().wait_stream(warmup)  \n",
    "        x, y = x.to('cuda', non_blocking=True), y.to('cuda', non_blocking=True)\n",
    "        \n",
    "        e_timer_start.record()\n",
    "        # compute_stream.wait_stream(torch.cuda.current_stream())\n",
    "        # with torch.cuda.stream(compute_stream):\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16, enabled=True):\n",
    "                # static_x.copy_(x)\n",
    "                # static_y.copy_(y)\n",
    "                # opt.zero_grad(set_to_none=False)\n",
    "\n",
    "                # total_steps = i * len(train_loader) + step\n",
    "                # k = step % total_graphs\n",
    "                # add_smoothout_noise(controller.traced_model)\n",
    "                # assign_theta(controller.traced_model)\n",
    "                # graphs[0].replay()\n",
    "                # remove_smoothout_noise(controller.traced_model)\n",
    "                # opt.step()\n",
    "                # scheduler.step()   \n",
    "                \n",
    "                # train_logits.append(logits.detach().cpu())\n",
    "                # train_y.append(static_y.detach().cpu())  \n",
    "\n",
    "                # for m in controller.traced_model.modules():\n",
    "                #     if hasattr(m, \"qdrop\"):\n",
    "                #         temp = torch.rand(1).item()\n",
    "                #         m.qdrop.copy_(temp)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            raw_logits = controller.traced_model(x)\n",
    "            # raw_logits = model(x)\n",
    "\n",
    "            if isinstance(raw_logits, tuple):\n",
    "                logits = raw_logits[0]\n",
    "                freq_maps = raw_logits[1]\n",
    "            else:\n",
    "                logits = raw_logits\n",
    "                freq_maps = None\n",
    "\n",
    "            CE_loss = criterion(logits, y)\n",
    "\n",
    "            model_spectrum1 = radial_spectrum_2d(freq_maps, num_bins=16)\n",
    "            target_spectrum = radial_spectrum_2d(x, num_bins=16)\n",
    "            freq_loss1 = wasserstein_1d(model_spectrum1, target_spectrum)\n",
    "            # model_spectrum = radial_spectrum_2d(act)\n",
    "            # target_spectrum = radial_spectrum_2d(x).detach()\n",
    "            # freq_loss = wasserstein_1d(model_spectrum, target_spectrum)\n",
    "            # freq_loss = kl_div_spectrum(target_spectrum, model_spectrum)\n",
    "            # freq_loss = temp_kl(target_spectrum, model_spectrum)\n",
    "            # freq_loss = vit_spectral_slope_loss(act)\n",
    "            # freq_loss = local_freq_kl_loss(act, x)\n",
    "            \n",
    "\n",
    "            loss = CE_loss + freq_loss1 * 1.0\n",
    "            # loss = CE_loss\n",
    "\n",
    "            # inject_grad_noise_large_batch(controller.traced_model, step, batch_size, len(train_loader))\n",
    "            # inject_grad_noise_large_batch(model, step, batch_size, len(train_loader))\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            # scheduler.step()     \n",
    "            \n",
    "            # if step % 20 == 0:\n",
    "            #     temp = torch.nn.utils.parameters_to_vector(p.reshape(-1) for p in controller.traced_model.parameters() if p.grad is not None)\n",
    "            #     # temp = torch.nn.utils.parameters_to_vector(p.reshape(-1) for p in model.parameters() if p.grad is not None)\n",
    "            #     G.append(temp)     \n",
    "                \n",
    "            e_timer_end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            partile_time += e_timer_start.elapsed_time(e_timer_end)\n",
    "\n",
    "            train_logits.append(logits.detach().cpu())\n",
    "            train_y.append(y.detach().cpu())\n",
    "\n",
    "                # print(\"train logits mean:\", logits.mean().item())\n",
    "                # print(\"train logits std:\", logits.std().item())\n",
    "\n",
    "    # partile_time = e_timer_start.elapsed_time(e_timer_end)\n",
    "    # total_time += e_timer_start.elapsed_time(e_timer_end)\n",
    "\n",
    "    train_logits = torch.cat(train_logits)\n",
    "    train_y = torch.cat(train_y)\n",
    "    computed_acc = acc(train_logits, train_y)\n",
    "    throughtout = len(train_dataset) / (partile_time / 1000)\n",
    "\n",
    "    # temp_G = torch.cat(G)\n",
    "    # GNS = gradient_noise_scale(temp_G)\n",
    "    # SNR = signal_to_noise_ratio(temp_G)\n",
    "\n",
    "    print(f'Epoch: {i}')\n",
    "    print(f\"Train Loss: {loss}\")\n",
    "    print(f\"Learning Rate: {scheduler.get_last_lr()[0]}\")\n",
    "    print(f\"Train Accuracy: {computed_acc}\")\n",
    "    print(f'Peak Mem Reserved: {torch.cuda.max_memory_reserved()}')\n",
    "    print(f'Peak Mem Allocated: {torch.cuda.max_memory_allocated()}')\n",
    "    print(f'Current train time: {partile_time / 1000} s')\n",
    "    print(f\"Throughout: {throughtout} samples per second\")\n",
    "    # for name, m in controller.traced_model.named_modules():\n",
    "    #     if isinstance(m, DOBatchNormReLU2d):\n",
    "    #         if not torch.isfinite(m.running_mean).all() or not torch.isfinite(m.running_var).all():\n",
    "    #             print(\"BAD BN:\", name)\n",
    "    #             print(\"mean finite:\", torch.isfinite(m.running_mean).all().item(),\n",
    "    #                 \"var finite:\", torch.isfinite(m.running_var).all().item(),\n",
    "    #                 \"var min:\", m.running_var.min().item())\n",
    "    #             break\n",
    "    # print(f'GNS: {GNS}')\n",
    "    # print(f'SNR: {SNR}')\n",
    "    # G_GNS.append(GNS)\n",
    "    # G_SNR.append(SNR)\n",
    "    scheduler.step()\n",
    "    # update_lr(controller.traced_model, scheduler)\n",
    "\n",
    "    train_logits = []\n",
    "    train_y = []\n",
    "\n",
    "    controller.traced_model.eval()\n",
    "    # model.eval()\n",
    "    # aot_model.eval()\n",
    "    with torch.compiler.set_stance(\"force_eager\"):\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in val_loader:\n",
    "                x_val, y_val = x_val.to('cuda'), y_val.to('cuda')\n",
    "\n",
    "                with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16, enabled=True):\n",
    "                    raw_y_preds  = controller.traced_model(x_val)\n",
    "                    # raw_y_preds = model(x_val)\n",
    "\n",
    "                    if isinstance(raw_y_preds, tuple):\n",
    "                        y_preds = raw_y_preds[0]\n",
    "                    else:\n",
    "                        y_preds = raw_y_preds\n",
    "                val_loss = F.cross_entropy(y_preds, y_val)\n",
    "\n",
    "                val_logits.append(y_preds.detach().cpu())\n",
    "                val_y.append(y_val.detach().cpu())\n",
    "\n",
    "    val_logits = torch.cat(val_logits)\n",
    "    val_y = torch.cat(val_y)\n",
    "\n",
    "    valid_computed_acc = acc(val_logits, val_y)\n",
    "    # max_acc = checkpoint_save_helper(controller.traced_model, valid_computed_acc, max_acc, opt)\n",
    "    print(f\"Val Loss: {val_loss}\")\n",
    "    print(f\"Val Accuracy: {valid_computed_acc}\\n\\n\")\n",
    "\n",
    "\n",
    "total_timer_end.record()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "full_time = total_timer_start.elapsed_time(total_timer_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ae0b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(check_loder))\n",
    "x, y = x.to('cuda'), y.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc91bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "# torch.set_rng_state(cached_rng_state['CPU'])\n",
    "# torch.cuda.set_rng_state(cached_rng_state['GPU'])\n",
    "num_classes = 10\n",
    "# model = models.resnet18(weights=None)\n",
    "# model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "# model = models.efficientnet_b0(weights=None)\n",
    "# model = models.efficientnet_v2_s(weights=None)\n",
    "model = models.mobilenet_v2(weights=None)\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "# model.apply(lambda m: apply_trunc_init_for_act(m, std=0.02))\n",
    "# model.apply(init_fan_out)\n",
    "# model.apply(lambda m: init_orthogonal_conv(m, gain=math.sqrt(2)))  # ReLU-friendly\n",
    "# model.apply(lambda m: init_orthogonal_conv(m, gain=1.0))           # smoother spectrum\n",
    "# model = timm.create_model('vit_base_patch16_224', pretrained=False, num_classes=10)\n",
    "\n",
    "disable_act_inplace(model)\n",
    "# model.load_state_dict(baseline)\n",
    "# init(model)\n",
    "# for n, m in model.named_modules():\n",
    "#     if isinstance(m, (nn.Conv1d, nn.Conv2d)):\n",
    "#             cahced_init(m.weight)\n",
    "\n",
    "# for n, m in model.named_modules():\n",
    "#     init_orthogonal_conv(n, m, gain=1.0)\n",
    "\n",
    "# for m in model.modules():\n",
    "#     if isinstance(m, nn.Conv2d) and m.kernel_size == (1, 1):\n",
    "#         spectral_norm(m)\n",
    "\n",
    "model = model.to('cuda')\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ec11ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/home/hice1/yyu496/scratch/Model_Checkpoint/ResNet18_bs_8192_acc_89\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57026e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_cache.clear()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7bac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "activation_cache = {}\n",
    "low_frequency_energy_ratio_grad = {}\n",
    "low_frequency_energy_ratio_act = {}\n",
    "\n",
    "\n",
    "def forward_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        activation_cache[name] = output.detach().cpu()\n",
    "    return hook\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_lfer_act(x, k=4):\n",
    "    if x.ndim == 4:\n",
    "        H, W = x.shape[-2:]\n",
    "        k = min(k, H, W)\n",
    "        x_lp = F.interpolate(F.avg_pool2d(x, kernel_size=k, stride=k, padding=0), size=x.shape[-2:], mode='nearest')\n",
    "        return (x_lp.square().sum() / (x.square().sum() + 1e-7)).item()\n",
    "    else:\n",
    "        feature_dim = x.shape[-1]\n",
    "        k = min(k, feature_dim)\n",
    "        if x.ndim != 2:\n",
    "            pooled = F.avg_pool1d(x, kernel_size=k, stride=k, padding=0)\n",
    "            x_lp = F.interpolate(pooled, size=feature_dim, mode='nearest')\n",
    "        else:\n",
    "            x_lp = x.mean(dim=-1, keepdim=True).expand_as(x)\n",
    "        return (x_lp.square().sum() / (x.square().sum() + 1e-7)).item().detach().cpu()\n",
    "\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, (nn.Conv1d, nn.Conv2d)):\n",
    "        module.register_forward_hook(forward_hook(name))\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036b6a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['conv1', 'bn1', 'relu', 'maxpool', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.0', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2', 'layer1.1.bn2', 'layer1.1', 'layer1', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.downsample.0', 'layer2.0.downsample.1', 'layer2.0.downsample', 'layer2.0', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2', 'layer2.1', 'layer2', 'layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.downsample.0', 'layer3.0.downsample.1', 'layer3.0.downsample', 'layer3.0', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer3.1', 'layer3', 'layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.downsample.0', 'layer4.0.downsample.1', 'layer4.0.downsample', 'layer4.0', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'layer4.1', 'layer4', 'avgpool', 'fc', ''])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_cache.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659fba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def radial_spectrum_2d_per_sample(x, num_bins=64, eps=1e-8, out_dtype=None,\n",
    "                                  *, vit_grid=None, num_prefix_tokens=None, max_prefix_tokens=16,\n",
    "                                  log_power=True):\n",
    "    if out_dtype is None:\n",
    "        out_dtype = x.dtype\n",
    "\n",
    "    # --- your ViT token -> [B,C,H,W] conversion stays the same ---\n",
    "    if x.dim() == 3:\n",
    "        import math\n",
    "        B, N, D = x.shape\n",
    "        if vit_grid is not None:\n",
    "            H, W = vit_grid\n",
    "            needed = H * W\n",
    "            if num_prefix_tokens is None:\n",
    "                num_prefix_tokens = N - needed\n",
    "            if N - num_prefix_tokens != needed:\n",
    "                raise ValueError(\"grid mismatch\")\n",
    "        else:\n",
    "            if num_prefix_tokens is None:\n",
    "                found = None\n",
    "                for k in range(0, max_prefix_tokens + 1):\n",
    "                    m = N - k\n",
    "                    if m <= 0: break\n",
    "                    s = int(math.isqrt(m))\n",
    "                    if s * s == m:\n",
    "                        found = k\n",
    "                        H = W = s\n",
    "                        break\n",
    "                if found is None:\n",
    "                    raise ValueError(\"cannot infer grid\")\n",
    "                num_prefix_tokens = found\n",
    "            else:\n",
    "                m = N - num_prefix_tokens\n",
    "                s = int(math.isqrt(m))\n",
    "                if s * s != m:\n",
    "                    raise ValueError(\"not square\")\n",
    "                H = W = s\n",
    "\n",
    "        if num_prefix_tokens > 0:\n",
    "            x = x[:, num_prefix_tokens:, :].contiguous()\n",
    "        x = x.view(B, H, W, D).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "    elif x.dim() != 4:\n",
    "        raise ValueError(f\"Expected dim 3 or 4, got {x.dim()}\")\n",
    "\n",
    "    # x: [B,C,H,W]\n",
    "    x = x.mean(dim=1)  # [B,H,W]\n",
    "\n",
    "    x_fft = x.to(torch.float32)\n",
    "    F = torch.fft.fft2(x_fft)\n",
    "    F = torch.fft.fftshift(F, dim=(-2, -1))\n",
    "    P = (F.real * F.real + F.imag * F.imag)  # [B,H,W]\n",
    "    if log_power:\n",
    "        P = torch.log1p(P)\n",
    "\n",
    "    B, H, W = P.shape\n",
    "    yy, xx = torch.meshgrid(torch.arange(H, device=P.device),\n",
    "                            torch.arange(W, device=P.device), indexing=\"ij\")\n",
    "    cy, cx = (H - 1) / 2.0, (W - 1) / 2.0\n",
    "    r = torch.sqrt((yy - cy) ** 2 + (xx - cx) ** 2)\n",
    "    r = (r / (r.max() + 1e-12) * (num_bins - 1)).to(torch.long)  # [H,W]\n",
    "    r_flat = r.flatten()  # [HW]\n",
    "\n",
    "    vals = P.reshape(B, -1)  # [B,HW]\n",
    "\n",
    "    # per-sample scatter_add\n",
    "    spec = torch.zeros((B, num_bins), device=P.device, dtype=torch.float32)\n",
    "    spec.scatter_add_(1, r_flat[None, :].expand(B, -1), vals)\n",
    "\n",
    "    spec = spec / (spec.sum(dim=1, keepdim=True) + eps)\n",
    "    return spec.to(out_dtype)\n",
    "\n",
    "\n",
    "def vit_spectral_slope_loss(tokens, num_bins=64, alpha=2.0, num_prefix_tokens=1):\n",
    "    spec = radial_spectrum_2d_per_sample(\n",
    "        tokens, num_bins=num_bins,\n",
    "        num_prefix_tokens=num_prefix_tokens,\n",
    "        log_power=True\n",
    "    )  # [B,K]\n",
    "\n",
    "    K = spec.shape[1]\n",
    "    k = torch.arange(K, device=spec.device, dtype=spec.dtype)\n",
    "    target = 1.0 / (k + 1.0) ** alpha\n",
    "    target = target / target.sum()\n",
    "\n",
    "    # compare each sample spectrum to 1/f^alpha\n",
    "    return torch.mean(torch.sum(torch.abs(spec - target[None, :]), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471eab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def _infer_vit_grid(N, num_prefix_tokens=None, max_prefix_tokens=16):\n",
    "    if num_prefix_tokens is not None:\n",
    "        M = N - num_prefix_tokens\n",
    "        s = int(math.isqrt(M))\n",
    "        if s * s != M:\n",
    "            raise ValueError(f\"N - num_prefix_tokens = {M} not square\")\n",
    "        return s, s, num_prefix_tokens\n",
    "\n",
    "    # auto infer\n",
    "    for k in range(0, max_prefix_tokens + 1):\n",
    "        M = N - k\n",
    "        if M <= 0:\n",
    "            break\n",
    "        s = int(math.isqrt(M))\n",
    "        if s * s == M:\n",
    "            return s, s, k\n",
    "    raise ValueError(f\"Cannot infer vit grid from N={N}\")\n",
    "\n",
    "\n",
    "def _radial_bins(H, W, num_bins, device):\n",
    "    yy, xx = torch.meshgrid(\n",
    "        torch.arange(H, device=device),\n",
    "        torch.arange(W, device=device),\n",
    "        indexing=\"ij\"\n",
    "    )\n",
    "    cy, cx = (H - 1) / 2.0, (W - 1) / 2.0\n",
    "    r = torch.sqrt((yy - cy) ** 2 + (xx - cx) ** 2)\n",
    "    r = (r / (r.max() + 1e-12) * (num_bins - 1)).to(torch.long)  # [H,W]\n",
    "    return r\n",
    "\n",
    "\n",
    "def local_radial_spectrum_2d(\n",
    "    x,                      # [B,1,H,W] or [B,H,W]\n",
    "    window=8,\n",
    "    num_bins=32,\n",
    "    eps=1e-8,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns local spectra per window:\n",
    "      spectra: [B, nwin, num_bins] each normalized to sum=1\n",
    "    \"\"\"\n",
    "    if x.dim() == 4:\n",
    "        x = x.squeeze(1)\n",
    "    assert x.dim() == 3\n",
    "    B, H, W = x.shape\n",
    "\n",
    "    # crop to divisible by window\n",
    "    Hc = (H // window) * window\n",
    "    Wc = (W // window) * window\n",
    "    x = x[:, :Hc, :Wc]\n",
    "\n",
    "    # unfold into windows: [B, nH, nW, window, window]\n",
    "    nH = Hc // window\n",
    "    nW = Wc // window\n",
    "    xw = x.view(B, nH, window, nW, window).permute(0, 1, 3, 2, 4).contiguous()\n",
    "    xw = xw.view(B, nH * nW, window, window)  # [B, nwin, w, w]\n",
    "\n",
    "    # FFT per window (fp32)\n",
    "    X = xw.to(torch.float32)\n",
    "    Fw = torch.fft.fft2(X)                      # complex64\n",
    "    Fw = torch.fft.fftshift(Fw, dim=(-2, -1))\n",
    "    Pw = (Fw.real * Fw.real + Fw.imag * Fw.imag)  # [B,nwin,w,w]\n",
    "\n",
    "    # build radial bins for (window, window)\n",
    "    r = _radial_bins(window, window, num_bins, device=Pw.device)  # [w,w]\n",
    "    r_flat = r.flatten()  # [w*w]\n",
    "\n",
    "    # scatter-add radial power\n",
    "    spectra = torch.zeros((B, nH * nW, num_bins), device=Pw.device, dtype=torch.float32)\n",
    "    vals = Pw.reshape(B, nH * nW, -1)  # [B,nwin,w*w]\n",
    "\n",
    "    # vectorized scatter: flatten over B*nwin\n",
    "    BN = B * (nH * nW)\n",
    "    spectra2 = torch.zeros((BN, num_bins), device=Pw.device, dtype=torch.float32)\n",
    "    idx = r_flat[None, :].expand(BN, -1)  # [BN,w*w]\n",
    "    spectra2.scatter_add_(1, idx, vals.reshape(BN, -1))\n",
    "\n",
    "    spectra = spectra2.view(B, nH * nW, num_bins)\n",
    "    spectra = spectra / (spectra.sum(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "    return spectra  # [B,nwin,K]\n",
    "\n",
    "\n",
    "def local_freq_kl_loss(\n",
    "    vit_tokens,             # [B,N,D] tokens (output of some block or final tokens)\n",
    "    images,                 # [B,3,H_img,W_img] input image tensor\n",
    "    *,\n",
    "    vit_grid=None,          # (H,W) patch grid override\n",
    "    num_prefix_tokens=None, # if known\n",
    "    max_prefix_tokens=16,\n",
    "    window=7,\n",
    "    num_bins=32,\n",
    "    low_freq_bins=16,       # only match first few bins\n",
    "    eps=1e-8,\n",
    "    detach_target=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Local spectrum KL loss:\n",
    "      KL( target || model )\n",
    "\n",
    "    Returns scalar loss.\n",
    "    \"\"\"\n",
    "    assert vit_tokens.dim() == 3\n",
    "    B, N, D = vit_tokens.shape\n",
    "\n",
    "    # infer token grid\n",
    "    if vit_grid is not None:\n",
    "        Ht, Wt = vit_grid\n",
    "        needed = Ht * Wt\n",
    "        if num_prefix_tokens is None:\n",
    "            num_prefix_tokens = N - needed\n",
    "        assert N - num_prefix_tokens == needed\n",
    "    else:\n",
    "        Ht, Wt, num_prefix_tokens = _infer_vit_grid(N, num_prefix_tokens, max_prefix_tokens)\n",
    "\n",
    "    # patch tokens only\n",
    "    x = vit_tokens[:, num_prefix_tokens:, :]  # [B, Ht*Wt, D]\n",
    "    x = x.view(B, Ht, Wt, D).permute(0, 3, 1, 2).contiguous()  # [B,D,Ht,Wt]\n",
    "\n",
    "    # make it 1-channel \"activation image\"\n",
    "    x_map = x.mean(dim=1, keepdim=True)  # [B,1,Ht,Wt]\n",
    "\n",
    "    # downsample input image to patch grid resolution\n",
    "    img = images\n",
    "    img_gray = img.mean(dim=1, keepdim=True)  # [B,1,H_img,W_img]\n",
    "    img_ds = F.interpolate(img_gray, size=(Ht, Wt), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    if detach_target:\n",
    "        img_ds = img_ds.detach()\n",
    "\n",
    "    # compute local spectra\n",
    "    spec_model = local_radial_spectrum_2d(x_map, window=window, num_bins=num_bins, eps=eps)  # [B,nwin,K]\n",
    "    spec_tgt   = local_radial_spectrum_2d(img_ds, window=window, num_bins=num_bins, eps=eps) # [B,nwin,K]\n",
    "\n",
    "    # use only low-freq bins\n",
    "    K = min(low_freq_bins, num_bins)\n",
    "    pm = spec_model[..., :K].clamp_min(eps)\n",
    "    pt = spec_tgt[..., :K].clamp_min(eps)\n",
    "\n",
    "    # renormalize after truncation\n",
    "    pm = pm / pm.sum(dim=-1, keepdim=True)\n",
    "    pt = pt / pt.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    # KL(target || model)\n",
    "    loss = (pt * (pt.log() - pm.log())).sum(dim=-1)  # [B,nwin]\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2c4bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "def radial_spectrum_2d(\n",
    "    x,\n",
    "    num_bins=64,\n",
    "    eps=1e-8,\n",
    "    out_dtype=None,\n",
    "    *,\n",
    "    vit_grid=None,              # (H, W) explicit patch grid\n",
    "    num_prefix_tokens=None,      # number of prefix tokens total\n",
    "    max_prefix_tokens=16,\n",
    "    use_cls_token=True,         # NEW: if True, build spectrum from CLS token only\n",
    "):\n",
    "    \"\"\"\n",
    "    Supports:\n",
    "      - CNN feature maps: x [B, C, H, W]\n",
    "      - ViT tokens:       x [B, N, D]  (tokens, embed dim)\n",
    "\n",
    "    Returns:\n",
    "      spectrum [num_bins], normalized to sum=1 (aggregated across batch).\n",
    "    \"\"\"\n",
    "\n",
    "    if out_dtype is None:\n",
    "        out_dtype = x.dtype\n",
    "\n",
    "    if x.dim() == 3:\n",
    "        # ViT tokens: [B, N, D]\n",
    "        B, N, D = x.shape\n",
    "\n",
    "        # Determine grid size and how many prefix tokens exist.\n",
    "        if vit_grid is not None:\n",
    "            H, W = vit_grid\n",
    "            needed = H * W\n",
    "            if num_prefix_tokens is None:\n",
    "                num_prefix_tokens = N - needed\n",
    "            if N - num_prefix_tokens != needed:\n",
    "                raise ValueError(\n",
    "                    f\"vit_grid={vit_grid} implies {needed} patch tokens, but got N={N} \"\n",
    "                    f\"with num_prefix_tokens={num_prefix_tokens} => {N - num_prefix_tokens} tokens.\"\n",
    "                )\n",
    "        else:\n",
    "            # Auto-infer num_prefix_tokens by finding N-k that is a perfect square\n",
    "            if num_prefix_tokens is None:\n",
    "                found = None\n",
    "                for k in range(0, max_prefix_tokens + 1):\n",
    "                    m = N - k\n",
    "                    if m <= 0:\n",
    "                        break\n",
    "                    s = int(math.isqrt(m))\n",
    "                    if s * s == m:\n",
    "                        found = k\n",
    "                        H = W = s\n",
    "                        break\n",
    "                if found is None:\n",
    "                    raise ValueError(\n",
    "                        f\"Cannot infer square token grid from N={N}. \"\n",
    "                        f\"Pass vit_grid=(H,W) or num_prefix_tokens explicitly.\"\n",
    "                    )\n",
    "                num_prefix_tokens = found\n",
    "            else:\n",
    "                m = N - num_prefix_tokens\n",
    "                s = int(math.isqrt(m))\n",
    "                if s * s != m:\n",
    "                    raise ValueError(\n",
    "                        f\"N - num_prefix_tokens = {m} is not a perfect square. \"\n",
    "                        f\"Pass vit_grid=(H,W) instead.\"\n",
    "                    )\n",
    "                H = W = s\n",
    "\n",
    "        # =========================\n",
    "        # NEW: CLS-token spectrum\n",
    "        # =========================\n",
    "        if use_cls_token:\n",
    "            # CLS assumed to be token 0\n",
    "            cls = x[:, 0, :]  # [B, D]\n",
    "\n",
    "            # Turn CLS embedding into a 2D \"image\" so FFT makes sense.\n",
    "            # We reshape D -> H*W (requires D == H*W).\n",
    "            # If not equal, we crop or pad.\n",
    "            HW = H * W\n",
    "            if D < HW:\n",
    "                pad = HW - D\n",
    "                cls = torch.nn.functional.pad(cls, (0, pad), mode=\"constant\", value=0.0)\n",
    "            elif D > HW:\n",
    "                cls = cls[:, :HW]\n",
    "\n",
    "            # [B, HW] -> [B, H, W] -> [B, 1, H, W]\n",
    "            x = cls.view(B, H, W).unsqueeze(1).contiguous()\n",
    "\n",
    "        else:\n",
    "            # Default: patch-token spectrum (original behavior)\n",
    "            if num_prefix_tokens > 0:\n",
    "                x = x[:, num_prefix_tokens:, :].contiguous()  # [B, H*W, D]\n",
    "            else:\n",
    "                x = x.contiguous()\n",
    "\n",
    "            # [B, H*W, D] -> [B, H, W, D] -> [B, D, H, W]\n",
    "            x = x.view(B, H, W, D).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "    elif x.dim() != 4:\n",
    "        raise ValueError(f\"Expected x dim 3 or 4, got {x.dim()}\")\n",
    "\n",
    "    # CNN path (or converted ViT path): x [B, C, H, W]\n",
    "    x = x.mean(dim=1)                 # [B, H, W]\n",
    "\n",
    "    # FFT in fp32 for safety/stability\n",
    "    x_fft = x.to(torch.float32)\n",
    "    F = torch.fft.fft2(x_fft)         # complex64\n",
    "    F = torch.fft.fftshift(F)  # shift only spatial dims\n",
    "\n",
    "    P = (F.real * F.real + F.imag * F.imag)  # [B, H, W] float32\n",
    "\n",
    "    B, H, W = P.shape\n",
    "    yy, xx = torch.meshgrid(\n",
    "        torch.arange(H, device=P.device),\n",
    "        torch.arange(W, device=P.device),\n",
    "        indexing=\"ij\"\n",
    "    )\n",
    "    cy, cx = (H - 1) / 2.0, (W - 1) / 2.0\n",
    "    r = torch.sqrt((yy - cy) ** 2 + (xx - cx) ** 2)\n",
    "    r = (r / (r.max() + 1e-12) * (num_bins - 1)).to(torch.long)  # [H, W]\n",
    "\n",
    "    spectrum = torch.zeros(num_bins, device=P.device, dtype=torch.float32)\n",
    "    idx = r.flatten().expand(B, -1)     # [B, HW]\n",
    "    vals = P.reshape(B, -1)            # [B, HW]\n",
    "    spectrum.scatter_add_(0, idx.reshape(-1), vals.reshape(-1))\n",
    "\n",
    "    spectrum = spectrum / (spectrum.sum() + eps)\n",
    "    return spectrum.to(out_dtype)\n",
    "\n",
    "\n",
    "\n",
    "def wasserstein_1d(p, q, eps=1e-8):\n",
    "    \"\"\"\n",
    "    p, q : tensors [K]\n",
    "    \"\"\"\n",
    "    p32 = p.to(torch.float32)\n",
    "    q32 = q.to(torch.float32)\n",
    "\n",
    "    p32 = p32 / (p32.sum() + eps)\n",
    "    q32 = q32 / (q32.sum() + eps)\n",
    "\n",
    "    cdf_p = torch.cumsum(p32, dim=0)\n",
    "    cdf_q = torch.cumsum(q32, dim=0)\n",
    "\n",
    "    return torch.sum(torch.abs(cdf_p - cdf_q)).to(p.dtype)\n",
    "\n",
    "\n",
    "def kl_div_spectrum(q, p, eps=1e-8):\n",
    "    # q, p: [B,K], each row sums to 1\n",
    "    q = q.clamp_min(eps)\n",
    "    p = p.clamp_min(eps)\n",
    "    return (q * (q.log() - p.log())).sum(dim=-1).mean()\n",
    "\n",
    "\n",
    "def temp_kl(p, q, T=2.0, eps=1e-8):\n",
    "    \"\"\"\n",
    "    p, q: [B, K] distributions (sum=1)\n",
    "    Applies temperature to log-prob space:\n",
    "      softmax(log(p)/T)\n",
    "    \"\"\"\n",
    "    logp = torch.log(p.clamp_min(eps))\n",
    "    logq = torch.log(q.clamp_min(eps))\n",
    "\n",
    "    pT = F.softmax(logp / T, dim=-1)\n",
    "    qT = F.softmax(logq / T, dim=-1)\n",
    "\n",
    "    # KL(p||q) = sum p * (log p - log q)\n",
    "    kl = (pT * (torch.log(pT.clamp_min(eps)) - torch.log(qT.clamp_min(eps)))).sum(dim=-1).mean()\n",
    "\n",
    "    return (T * T) * kl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06a892a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'activation_cache' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mactivation_cache\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'activation_cache' is not defined"
     ]
    }
   ],
   "source": [
    "activation_cache.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dae658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spectrum = radial_spectrum_2d(activation_cache['layer4.1.conv2'].to(torch.float32))\n",
    "target_spectrum = radial_spectrum_2d(x.to(torch.float32)).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486adee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.4471)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wasserstein_1d(model_spectrum, target_spectrum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe0e76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.0\n",
      "features.1.0.block.0.0\n",
      "features.1.0.block.1.fc1\n",
      "features.1.0.block.1.fc2\n",
      "features.1.0.block.2.0\n",
      "features.2.0.block.0.0\n",
      "features.2.0.block.1.0\n",
      "features.2.0.block.2.fc1\n",
      "features.2.0.block.2.fc2\n",
      "features.2.0.block.3.0\n",
      "features.2.1.block.0.0\n",
      "features.2.1.block.1.0\n",
      "features.2.1.block.2.fc1\n",
      "features.2.1.block.2.fc2\n",
      "features.2.1.block.3.0\n",
      "features.3.0.block.0.0\n",
      "features.3.0.block.1.0\n",
      "features.3.0.block.2.fc1\n",
      "features.3.0.block.2.fc2\n",
      "features.3.0.block.3.0\n",
      "features.3.1.block.0.0\n",
      "features.3.1.block.1.0\n",
      "features.3.1.block.2.fc1\n",
      "features.3.1.block.2.fc2\n",
      "features.3.1.block.3.0\n",
      "features.4.0.block.0.0\n",
      "features.4.0.block.1.0\n",
      "features.4.0.block.2.fc1\n",
      "features.4.0.block.2.fc2\n",
      "features.4.0.block.3.0\n",
      "features.4.1.block.0.0\n",
      "features.4.1.block.1.0\n",
      "features.4.1.block.2.fc1\n",
      "features.4.1.block.2.fc2\n",
      "features.4.1.block.3.0\n",
      "features.4.2.block.0.0\n",
      "features.4.2.block.1.0\n",
      "features.4.2.block.2.fc1\n",
      "features.4.2.block.2.fc2\n",
      "features.4.2.block.3.0\n",
      "features.5.0.block.0.0\n",
      "features.5.0.block.1.0\n",
      "features.5.0.block.2.fc1\n",
      "features.5.0.block.2.fc2\n",
      "features.5.0.block.3.0\n",
      "features.5.1.block.0.0\n",
      "features.5.1.block.1.0\n",
      "features.5.1.block.2.fc1\n",
      "features.5.1.block.2.fc2\n",
      "features.5.1.block.3.0\n",
      "features.5.2.block.0.0\n",
      "features.5.2.block.1.0\n",
      "features.5.2.block.2.fc1\n",
      "features.5.2.block.2.fc2\n",
      "features.5.2.block.3.0\n",
      "features.6.0.block.0.0\n",
      "features.6.0.block.1.0\n",
      "features.6.0.block.2.fc1\n",
      "features.6.0.block.2.fc2\n",
      "features.6.0.block.3.0\n",
      "features.6.1.block.0.0\n",
      "features.6.1.block.1.0\n",
      "features.6.1.block.2.fc1\n",
      "features.6.1.block.2.fc2\n",
      "features.6.1.block.3.0\n",
      "features.6.2.block.0.0\n",
      "features.6.2.block.1.0\n",
      "features.6.2.block.2.fc1\n",
      "features.6.2.block.2.fc2\n",
      "features.6.2.block.3.0\n",
      "features.6.3.block.0.0\n",
      "features.6.3.block.1.0\n",
      "features.6.3.block.2.fc1\n",
      "features.6.3.block.2.fc2\n",
      "features.6.3.block.3.0\n",
      "features.7.0.block.0.0\n",
      "features.7.0.block.1.0\n",
      "features.7.0.block.2.fc1\n",
      "features.7.0.block.2.fc2\n",
      "features.7.0.block.3.0\n",
      "features.8.0\n",
      "classifier.1\n"
     ]
    }
   ],
   "source": [
    "for n, m in model.named_modules():\n",
    "    if isinstance(m, (nn.Linear, nn.Conv1d, nn.Conv2d)):\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590962e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function kaiming_normal_fan_in at 0x1551dff54670>\n",
      "min score: 9.649715423583984\n",
      "<function kaiming_normal_fan_out at 0x1553ec0a1ea0>\n",
      "min score: 3.960505485534668\n",
      "<function kaiming_uniform_fan_in at 0x1553ea4969e0>\n",
      "<function kaiming_uniform_fan_out at 0x1553ec0a1c60>\n",
      "<function kaiming_low_std at 0x1553ea496950>\n",
      "min score: 3.7391810417175293\n",
      "<function trunc_normal at 0x1551dff54820>\n",
      "<function trunc_normal_small at 0x1553ed6892d0>\n",
      "<function orthogonal at 0x1553ed689360>\n",
      "<function orthogonal2 at 0x1553ed689480>\n",
      "<function xavier_normal at 0x1553e937e200>\n",
      "<function xavier_uniform at 0x1551dff54700>\n",
      "<function lecun_normal at 0x1551dff545e0>\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "activation_cache = {}\n",
    "low_frequency_energy_ratio_grad = {}\n",
    "low_frequency_energy_ratio_act = {}\n",
    "baseline = copy.deepcopy(model.state_dict())\n",
    "\n",
    "hook_handles = {}\n",
    "cahced_init = None\n",
    "cached_rng_state = None\n",
    "\n",
    "def kaiming_normal_fan_in(w):\n",
    "    return nn.init.kaiming_normal_(w, mode='fan_in', nonlinearity='relu')\n",
    "def kaiming_normal_fan_out(w):\n",
    "    return nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')\n",
    "def kaiming_uniform_fan_in(w):\n",
    "    return nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')\n",
    "def kaiming_uniform_fan_out(w):\n",
    "    return nn.init.kaiming_uniform_(w, mode='fan_out', nonlinearity='relu')\n",
    "def trunc_normal(w):\n",
    "    return trunc_normal_(w, std=0.02)\n",
    "def trunc_normal_small(w):\n",
    "    return trunc_normal_(w, std=0.01)\n",
    "def orthogonal(w):\n",
    "    return nn.init.orthogonal_(w, gain=1.0)\n",
    "def orthogonal2(w):\n",
    "    return nn.init.orthogonal_(w, gain=math.sqrt(2))\n",
    "def xavier_normal(w):\n",
    "    return nn.init.xavier_normal_(w)\n",
    "def xavier_uniform(w):\n",
    "    return nn.init.xavier_uniform_(w)\n",
    "def lecun_normal(w):\n",
    "    return nn.init.normal_(w, std=1.0 / math.sqrt(w.shape[1]))\n",
    "def kaiming_low_std(w):\n",
    "    fan_in = nn.init._calculate_correct_fan(w, \"fan_in\")\n",
    "    std = math.sqrt(2.0 / fan_in) * 0.5\n",
    "    return nn.init.normal_(w, std=std)\n",
    "\n",
    "\n",
    "def forward_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        activation_cache[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "\n",
    "min_score = 100\n",
    "for init in [\n",
    "    kaiming_normal_fan_in,\n",
    "    kaiming_normal_fan_out,\n",
    "    kaiming_uniform_fan_in,\n",
    "    kaiming_uniform_fan_out,\n",
    "    kaiming_low_std,\n",
    "    trunc_normal,\n",
    "    trunc_normal_small,\n",
    "    orthogonal,\n",
    "    orthogonal2,\n",
    "    xavier_normal,\n",
    "    xavier_uniform,\n",
    "    lecun_normal\n",
    "    ]:\n",
    "\n",
    "    # 42, 49, 0, 1, 2, 3, 4\n",
    "    set_seed(2)\n",
    "    cpu_rng_state = torch.get_rng_state()\n",
    "    gpu_rng_state = torch.cuda.get_rng_state()\n",
    "\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n",
    "            init(m.weight)\n",
    "    print(init)\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n",
    "            hook_handles[name] = module.register_forward_hook(forward_hook(name))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "    act = activation_cache['features.18.0']\n",
    "\n",
    "    model_spectrum = radial_spectrum_2d(act.to(torch.float32)).cpu()\n",
    "    target_spectrum = radial_spectrum_2d(x.to(torch.float32)).cpu()\n",
    "\n",
    "    score = wasserstein_1d(model_spectrum, target_spectrum)\n",
    "    if score < min_score:\n",
    "        min_score = score\n",
    "        cahced_init = init\n",
    "        cached_rng_state = {\n",
    "            'CPU' : cpu_rng_state,\n",
    "            'GPU' : gpu_rng_state\n",
    "        }\n",
    "        print(f\"min score: {min_score}\")\n",
    "\n",
    "\n",
    "    for h in hook_handles.values():\n",
    "        h.remove()\n",
    "    activation_cache.clear()\n",
    "    torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cc6180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.orthogonal2(w)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cahced_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dd41f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada04915",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#torch vision default\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtensor\u001b[49m(\u001b[38;5;241m17.0655\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# timm's trunc_init\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tensor(\u001b[38;5;241m25.9191\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tensor' is not defined"
     ]
    }
   ],
   "source": [
    "#torch vision default\n",
    "tensor(17.0655, device='cuda:0')\n",
    "\n",
    "# timm's trunc_init\n",
    "tensor(25.9191, device='cuda:0')\n",
    "\n",
    "# fan out\n",
    "tensor(11.3065, device='cuda:0')\n",
    "\n",
    "# init_orthogonal_conv(m, gain=math.sqrt(2))\n",
    "tensor(22.3052, device='cuda:0')\n",
    "\n",
    "# init_orthogonal_conv(m, gain=1.0)\n",
    "tensor(7.9659, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e523df",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x, y = x.to('cuda'), y.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de857ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with profile(\n",
    "    activities=[\n",
    "        ProfilerActivity.CPU,\n",
    "        ProfilerActivity.CUDA\n",
    "    ],\n",
    "    record_shapes=True,\n",
    "    with_stack=True,\n",
    "    profile_memory=True,\n",
    ") as prof:\n",
    "\n",
    "    for step in range(10):\n",
    "        with record_function(\"train_step\"):\n",
    "            with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "                output = controller.traced_model(x)\n",
    "                loss = criterion(output, y)\n",
    "                loss.backward()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "print(prof.key_averages().table(\n",
    "    sort_by=\"cuda_time_total\",\n",
    "    row_limit=200\n",
    "))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
