{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f98e646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shapes: K=8192, CIN=768, CIN_PAD=768, COUT=768, G=3, BITS=2, VPW=16, NWORDS=16\n",
      "✅ correctness: fused ~ unfused (within tolerance)\n",
      "Unfused (dequant + mm): 0.045 ms/iter\n",
      "Fused   (unpack+mm):    0.694 ms/iter\n",
      "Speedup: 0.06x\n",
      "Dequant-only: 0.024 ms/iter\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "# -----------------------------\n",
    "# Your provided kernels (as-is)\n",
    "# -----------------------------\n",
    "@triton.jit\n",
    "def quant_pack_kernel(\n",
    "    X_ptr, P_ptr, S_ptr, M_ptr,\n",
    "    stride_x0: tl.constexpr, stride_x1: tl.constexpr,\n",
    "    stride_p0: tl.constexpr, stride_p1: tl.constexpr,\n",
    "    BITS: tl.constexpr,\n",
    "    VPW: tl.constexpr,\n",
    "    NWORDS: tl.constexpr,\n",
    "    QMAX: tl.constexpr,\n",
    "):\n",
    "    pid = tl.program_id(0)\n",
    "\n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        base=X_ptr + pid * stride_x0,\n",
    "        shape=(NWORDS, VPW),\n",
    "        strides=(stride_x1 * VPW, stride_x1),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(NWORDS, VPW),\n",
    "        order=(0, 1),\n",
    "    )\n",
    "\n",
    "    x = tl.load(x_block_ptr).to(tl.float32)   # [NWORDS, VPW]\n",
    "\n",
    "    xmin = tl.min(x, axis=1)\n",
    "    xmin = tl.min(xmin, axis=0)\n",
    "\n",
    "    xmax = tl.max(x, axis=1)\n",
    "    xmax = tl.max(xmax, axis=0)\n",
    "\n",
    "    rng = xmax - xmin\n",
    "    scale = (rng / tl.full([], QMAX, tl.float32))\n",
    "    scale = tl.where(rng > 0.0, scale, tl.full([], 1.0, tl.float32))\n",
    "\n",
    "    tl.store(S_ptr + pid, scale)\n",
    "    tl.store(M_ptr + pid, xmin)\n",
    "\n",
    "    inv_scale = tl.full([], 1.0, tl.float32) / scale\n",
    "\n",
    "    j = tl.arange(0, VPW)\n",
    "    shifts = (j * BITS).to(tl.int32)\n",
    "    eps  = tl.full([VPW], 1e-6, tl.float32)\n",
    "    half = tl.full([VPW], 0.5,  tl.float32)\n",
    "\n",
    "    qf = (x - xmin) * inv_scale + (half - eps)\n",
    "    qi = qf.to(tl.int32)\n",
    "    qi = tl.maximum(qi, 0)\n",
    "    qi = tl.minimum(qi, QMAX)\n",
    "\n",
    "    words = tl.sum(qi << shifts[None, :], axis=1)  # [NWORDS]\n",
    "\n",
    "    p_block_ptr = tl.make_block_ptr(\n",
    "        base = P_ptr + pid * stride_p0,\n",
    "        shape=(NWORDS,),\n",
    "        strides=(stride_p1,),\n",
    "        offsets=(0,),\n",
    "        block_shape=(NWORDS,),\n",
    "        order=(0,)\n",
    "    )\n",
    "    tl.store(p_block_ptr, words.to(tl.int32))\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def dequant_unpack_kernel(\n",
    "    P_ptr, S_ptr, M_ptr, Y_ptr,\n",
    "    stride_p0: tl.constexpr, stride_p1: tl.constexpr,\n",
    "    stride_y0: tl.constexpr, stride_y1: tl.constexpr,\n",
    "    BITS: tl.constexpr,\n",
    "    VPW: tl.constexpr,\n",
    "    NWORDS: tl.constexpr\n",
    "):\n",
    "    pid = tl.program_id(0)\n",
    "\n",
    "    p_block_ptr = tl.make_block_ptr(\n",
    "        base=P_ptr + pid * stride_p0,\n",
    "        shape=(NWORDS,),\n",
    "        strides=(stride_p1,),\n",
    "        offsets=(0,),\n",
    "        block_shape=(NWORDS,),\n",
    "        order=(0,)\n",
    "    )\n",
    "\n",
    "    word = tl.load(p_block_ptr)\n",
    "\n",
    "    scale = tl.load(S_ptr + pid)\n",
    "    scale_dtype = scale.dtype\n",
    "    scale = scale.to(tl.float32)\n",
    "    xmin  = tl.load(M_ptr + pid).to(tl.float32)\n",
    "\n",
    "    mask = (1 << BITS) - 1\n",
    "    j = tl.arange(0, VPW)\n",
    "    shifts = (j * BITS).to(tl.int32)\n",
    "\n",
    "    q = ((word[:, None] >> shifts[None, :]) & mask).to(tl.float32)\n",
    "    q = q * scale + xmin\n",
    "    q_flat = tl.reshape(q, (NWORDS * VPW,))\n",
    "\n",
    "    y_block_ptr = tl.make_block_ptr(\n",
    "        base=Y_ptr + pid * stride_y0,\n",
    "        shape=(NWORDS * VPW,),\n",
    "        strides=(stride_y1,),\n",
    "        offsets=(0,),\n",
    "        block_shape=(NWORDS * VPW,),\n",
    "        order=(0,)\n",
    "    )\n",
    "\n",
    "    tl.store(y_block_ptr, q_flat.to(scale_dtype))\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Fused kernel: bit-unpack + dequant + accumulate dw tile\n",
    "# dw[m, n:n+256] += sum_k dy[k,m] * x[k,n:n+256]\n",
    "# -------------------------------------------------------\n",
    "@triton.jit\n",
    "def fused_linear_dw_from_packed_kernel(\n",
    "    P_ptr, S_ptr, M_ptr, DY_ptr, DW_ptr,\n",
    "    K: tl.constexpr,\n",
    "    COUT: tl.constexpr,\n",
    "    CIN: tl.constexpr,\n",
    "    G: tl.constexpr,\n",
    "    stride_p0: tl.constexpr, stride_p1: tl.constexpr,\n",
    "    stride_s0: tl.constexpr,\n",
    "    stride_m0: tl.constexpr,\n",
    "    stride_dy0: tl.constexpr, stride_dy1: tl.constexpr,\n",
    "    stride_dw0: tl.constexpr, stride_dw1: tl.constexpr,\n",
    "    BITS: tl.constexpr,\n",
    "    VPW: tl.constexpr,\n",
    "    NWORDS: tl.constexpr,\n",
    "    GROUP: tl.constexpr,          # <-- add this\n",
    "    BLOCK_M: tl.constexpr,\n",
    "    BLOCK_K: tl.constexpr,\n",
    "):\n",
    "    pid_m = tl.program_id(0)\n",
    "    pid_g = tl.program_id(1)\n",
    "\n",
    "    tl.static_assert(GROUP == NWORDS * VPW)\n",
    "\n",
    "    m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    mask_m = m < COUT\n",
    "\n",
    "    n = pid_g * GROUP + tl.arange(0, GROUP)   # <-- now GROUP is constexpr\n",
    "    mask_n = n < CIN\n",
    "\n",
    "    g = pid_g\n",
    "    valid_g = g < G\n",
    "\n",
    "    acc = tl.zeros((BLOCK_M, GROUP), dtype=tl.float32)\n",
    "\n",
    "    bitmask = (1 << BITS) - 1\n",
    "    j = tl.arange(0, VPW)\n",
    "    shifts = (j * BITS).to(tl.int32)\n",
    "    w = tl.arange(0, NWORDS)\n",
    "\n",
    "    for k0 in tl.range(0, K, BLOCK_K):\n",
    "        k = k0 + tl.arange(0, BLOCK_K)\n",
    "        mask_k = k < K\n",
    "\n",
    "        dy_ptrs = DY_ptr + k[:, None] * stride_dy0 + m[None, :] * stride_dy1\n",
    "        dy = tl.load(dy_ptrs, mask=mask_k[:, None] & mask_m[None, :], other=0.0).to(tl.bfloat16)\n",
    "        a = tl.trans(dy)  # [BM, BK]\n",
    "\n",
    "        pids = k * G + g\n",
    "        p_ptrs = P_ptr + pids[:, None] * stride_p0 + w[None, :] * stride_p1\n",
    "        words = tl.load(p_ptrs, mask=mask_k[:, None] & valid_g, other=0).to(tl.int32)\n",
    "\n",
    "        scale = tl.load(S_ptr + pids * stride_s0, mask=mask_k & valid_g, other=1.0).to(tl.float32)\n",
    "        xmin  = tl.load(M_ptr + pids * stride_m0, mask=mask_k & valid_g, other=0.0).to(tl.float32)\n",
    "\n",
    "        q = ((words[:, :, None] >> shifts[None, None, :]) & bitmask).to(tl.float32)\n",
    "        x = q * scale[:, None, None] + xmin[:, None, None]\n",
    "        x = tl.reshape(x, (BLOCK_K, GROUP)).to(tl.bfloat16)\n",
    "\n",
    "        acc += tl.dot(a, x)\n",
    "\n",
    "    dw_ptrs = DW_ptr + m[:, None] * stride_dw0 + n[None, :] * stride_dw1\n",
    "    tl.store(dw_ptrs, acc.to(tl.bfloat16), mask=mask_m[:, None] & mask_n[None, :])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Timing helpers\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def time_ms(fn, iters=50, warmup=10):\n",
    "    torch.cuda.synchronize()\n",
    "    for _ in range(warmup):\n",
    "        fn()\n",
    "    torch.cuda.synchronize()\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end   = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    for _ in range(iters):\n",
    "        fn()\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return start.elapsed_time(end) / iters\n",
    "\n",
    "\n",
    "def cdiv(a, b): return (a + b - 1) // b\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main benchmark\n",
    "# -----------------------------\n",
    "def main(\n",
    "    K=8192,        # rows in x_flat / dy_flat\n",
    "    CIN=768,       # input features\n",
    "    COUT=768,      # output features\n",
    "    BITS=2,        # 1/2/4/8 supported by your bitpack\n",
    "    GROUP=256,     # your pack group size (must match kernels)\n",
    "    iters=50,\n",
    "):\n",
    "    assert GROUP == 256, \"This script assumes GROUP=256 like your kernels (NWORDS*VPW=256).\"\n",
    "    assert BITS in (1,2,4,8), \"Test bits in {1,2,4,8} first.\"\n",
    "\n",
    "    device = \"cuda\"\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    VPW = 32 // BITS\n",
    "    NWORDS = GROUP // VPW\n",
    "    QMAX = (1 << BITS) - 1\n",
    "\n",
    "    # pad CIN to multiple of GROUP (for packing)\n",
    "    CIN_PAD = cdiv(CIN, GROUP) * GROUP\n",
    "    G = CIN_PAD // GROUP\n",
    "\n",
    "    print(f\"\\nShapes: K={K}, CIN={CIN}, CIN_PAD={CIN_PAD}, COUT={COUT}, G={G}, BITS={BITS}, VPW={VPW}, NWORDS={NWORDS}\")\n",
    "\n",
    "    # inputs\n",
    "    x = torch.randn((K, CIN), device=device, dtype=torch.bfloat16)\n",
    "    if CIN_PAD != CIN:\n",
    "        x = torch.nn.functional.pad(x, (0, CIN_PAD - CIN))\n",
    "    x = x.contiguous()  # [K, CIN_PAD]\n",
    "\n",
    "    dy = torch.randn((K, COUT), device=device, dtype=torch.bfloat16).contiguous()\n",
    "    dy_T = dy.t().contiguous()  # [COUT, K]\n",
    "\n",
    "    # view as groups and flatten to [K*G, GROUP]\n",
    "    X_in = x.view(K, G, GROUP).reshape(K * G, GROUP).contiguous()\n",
    "\n",
    "    # packed buffers (constant during benchmark)\n",
    "    P = torch.empty((K * G, NWORDS), device=device, dtype=torch.int32)\n",
    "    S = torch.empty((K * G,), device=device, dtype=torch.bfloat16)\n",
    "    M = torch.empty((K * G,), device=device, dtype=torch.bfloat16)\n",
    "\n",
    "    grid_q = (K * G,)\n",
    "    quant_pack_kernel[grid_q](\n",
    "        X_in, P, S, M,\n",
    "        X_in.stride(0), X_in.stride(1),\n",
    "        P.stride(0), P.stride(1),\n",
    "        BITS=BITS, VPW=VPW, NWORDS=NWORDS, QMAX=QMAX,\n",
    "        num_warps=4,\n",
    "    )\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # unfused buffers (reused)\n",
    "    Y = torch.empty((K * G, GROUP), device=device, dtype=torch.bfloat16)\n",
    "    dw_unfused = torch.empty((COUT, CIN), device=device, dtype=torch.bfloat16)\n",
    "\n",
    "    # fused output buffer (reused)\n",
    "    dw_fused = torch.empty((COUT, CIN), device=device, dtype=torch.bfloat16)\n",
    "\n",
    "    # --- unfused path: dequant -> reshape -> mm\n",
    "    def run_unfused():\n",
    "        # dequant into Y\n",
    "        dequant_unpack_kernel[grid_q](\n",
    "            P, S, M, Y,\n",
    "            P.stride(0), P.stride(1),\n",
    "            Y.stride(0), Y.stride(1),\n",
    "            BITS=BITS, VPW=VPW, NWORDS=NWORDS,\n",
    "            num_warps=4,\n",
    "        )\n",
    "        # rebuild x_deq: [K, CIN_PAD] then slice [:, :CIN]\n",
    "        x_deq = Y.view(K, G, GROUP).reshape(K, CIN_PAD)[:, :CIN]\n",
    "        # dw = dy_T @ x_deq  => [COUT, CIN]\n",
    "        torch.mm(dy_T, x_deq, out=dw_unfused)\n",
    "\n",
    "    # --- fused path: unpack+dequant inside GEMM-like kernel\n",
    "    def run_fused():\n",
    "        # zero dw_fused (kernel writes full tile; no atomic)\n",
    "        dw_fused.zero_()\n",
    "        grid = (triton.cdiv(COUT, 16), triton.cdiv(CIN, GROUP))\n",
    "        fused_linear_dw_from_packed_kernel[grid](\n",
    "            P, S, M, dy, dw_fused,\n",
    "            K=K, COUT=COUT, CIN=CIN, G=G,\n",
    "            stride_p0=P.stride(0), stride_p1=P.stride(1),\n",
    "            stride_s0=S.stride(0),\n",
    "            stride_m0=M.stride(0),\n",
    "            stride_dy0=dy.stride(0), stride_dy1=dy.stride(1),\n",
    "            stride_dw0=dw_fused.stride(0), stride_dw1=dw_fused.stride(1),\n",
    "            BITS=BITS, VPW=VPW, NWORDS=NWORDS,\n",
    "            GROUP=GROUP,                 # <-- add this\n",
    "            BLOCK_M=16, BLOCK_K=16,\n",
    "            num_warps=8, num_stages=3\n",
    "        )\n",
    "\n",
    "\n",
    "    # correctness check (allow small tolerance)\n",
    "    run_unfused()\n",
    "    run_fused()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # These should be close (same quant format; different reduction order possible)\n",
    "    torch.testing.assert_close(dw_fused, dw_unfused, rtol=1e-2, atol=1e-2)\n",
    "    print(\"✅ correctness: fused ~ unfused (within tolerance)\")\n",
    "\n",
    "    # timing\n",
    "    t_unfused = time_ms(run_unfused, iters=iters, warmup=10)\n",
    "    t_fused   = time_ms(run_fused,   iters=iters, warmup=10)\n",
    "\n",
    "    print(f\"Unfused (dequant + mm): {t_unfused:.3f} ms/iter\")\n",
    "    print(f\"Fused   (unpack+mm):    {t_fused:.3f} ms/iter\")\n",
    "    print(f\"Speedup: {t_unfused / t_fused:.2f}x\")\n",
    "\n",
    "    # optional: isolate dequant cost alone\n",
    "    def run_dequant_only():\n",
    "        dequant_unpack_kernel[grid_q](\n",
    "            P, S, M, Y,\n",
    "            P.stride(0), P.stride(1),\n",
    "            Y.stride(0), Y.stride(1),\n",
    "            BITS=BITS, VPW=VPW, NWORDS=NWORDS,\n",
    "            num_warps=4,\n",
    "        )\n",
    "    t_deq = time_ms(run_dequant_only, iters=iters, warmup=10)\n",
    "    print(f\"Dequant-only: {t_deq:.3f} ms/iter\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    assert torch.cuda.is_available()\n",
    "    # adjust K/CIN/COUT to your layer; start moderate so compile+timing is quick\n",
    "    main(K=8192, CIN=768, COUT=768, BITS=2, iters=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b94cc4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def fused_dw_kernel_splitk(\n",
    "    P_ptr, S_ptr, M_ptr, DY_ptr, DW_ptr,   # DW is FP32 for atomic accumulation\n",
    "    K: tl.constexpr,\n",
    "    COUT: tl.constexpr,\n",
    "    CIN: tl.constexpr,\n",
    "    G: tl.constexpr,  # groups per row = CIN_PAD // GROUP\n",
    "    stride_p0: tl.constexpr, stride_p1: tl.constexpr,   # P: [K*G, NWORDS]\n",
    "    stride_s0: tl.constexpr,                            # S: [K*G]\n",
    "    stride_m0: tl.constexpr,                            # M: [K*G]\n",
    "    stride_dy0: tl.constexpr, stride_dy1: tl.constexpr, # dy: [K, COUT]\n",
    "    stride_dw0: tl.constexpr, stride_dw1: tl.constexpr, # dw_fp32: [COUT, CIN]\n",
    "    # quant format\n",
    "    BITS: tl.constexpr,\n",
    "    VPW: tl.constexpr,\n",
    "    NWORDS: tl.constexpr,\n",
    "    GROUP: tl.constexpr,          # 256\n",
    "    # tiling\n",
    "    BM: tl.constexpr,\n",
    "    BN: tl.constexpr,\n",
    "    BK: tl.constexpr,\n",
    "    # split-K\n",
    "    SPLIT_K: tl.constexpr,\n",
    "    K_SLICE: tl.constexpr,\n",
    "    # NEW: make these explicit constexpr so tl.arange accepts them\n",
    "    WORDS_N: tl.constexpr,        # = BN // VPW\n",
    "    TILES_PER_GROUP: tl.constexpr # = GROUP // BN\n",
    "):\n",
    "    pid_m  = tl.program_id(0)\n",
    "    pid_n  = tl.program_id(1)\n",
    "    pid_sk = tl.program_id(2)\n",
    "\n",
    "    tl.static_assert(GROUP == 256)\n",
    "    tl.static_assert((GROUP % BN) == 0)\n",
    "    tl.static_assert((BN % VPW) == 0)\n",
    "    tl.static_assert(WORDS_N * VPW == BN)\n",
    "    tl.static_assert(TILES_PER_GROUP * BN == GROUP)\n",
    "\n",
    "    # output indices\n",
    "    m = pid_m * BM + tl.arange(0, BM)\n",
    "    n = pid_n * BN + tl.arange(0, BN)\n",
    "    mask_m = m < COUT\n",
    "    mask_n = n < CIN\n",
    "\n",
    "    # map BN-tile -> group g, and tile offset within that group\n",
    "    g = pid_n // TILES_PER_GROUP\n",
    "    tile_in_group = pid_n - g * TILES_PER_GROUP\n",
    "    valid_g = g < G\n",
    "\n",
    "    # which packed words inside this group does this BN-tile need?\n",
    "    # word_base = (tile_in_group*BN) // VPW = tile_in_group * (BN//VPW) = tile_in_group * WORDS_N\n",
    "    word_base = tile_in_group * WORDS_N\n",
    "\n",
    "    acc = tl.zeros((BM, BN), dtype=tl.float32)\n",
    "\n",
    "    bitmask = (1 << BITS) - 1\n",
    "    shifts = (tl.arange(0, VPW) * BITS).to(tl.int32)  # [VPW]\n",
    "    w = tl.arange(0, WORDS_N)                         # ✅ now WORDS_N is constexpr\n",
    "\n",
    "    # split-K range\n",
    "    k_start = pid_sk * K_SLICE\n",
    "\n",
    "    for kk in tl.range(0, K_SLICE, BK):\n",
    "        k = k_start + kk + tl.arange(0, BK)\n",
    "        mask_k = k < K\n",
    "\n",
    "        # dy: [BK, BM] -> A = [BM, BK]\n",
    "        dy_ptrs = DY_ptr + k[:, None] * stride_dy0 + m[None, :] * stride_dy1\n",
    "        dy = tl.load(dy_ptrs, mask=mask_k[:, None] & mask_m[None, :], other=0.0).to(tl.bfloat16)\n",
    "        A = tl.trans(dy)\n",
    "\n",
    "        # packed words: [BK, WORDS_N]\n",
    "        pids = k * G + g\n",
    "        p_ptrs = P_ptr + pids[:, None] * stride_p0 + (word_base + w)[None, :] * stride_p1\n",
    "        words = tl.load(p_ptrs, mask=mask_k[:, None] & valid_g, other=0).to(tl.int32)\n",
    "\n",
    "        scale = tl.load(S_ptr + pids * stride_s0, mask=mask_k & valid_g, other=1.0).to(tl.float16)\n",
    "        xmin  = tl.load(M_ptr + pids * stride_m0, mask=mask_k & valid_g, other=0.0).to(tl.float16)\n",
    "\n",
    "        # unpack only BN cols: [BK, WORDS_N, VPW] -> [BK, BN]\n",
    "        q = ((words[:, :, None] >> shifts[None, None, :]) & bitmask).to(tl.float16)\n",
    "        X = q * scale[:, None, None] + xmin[:, None, None]\n",
    "        B = tl.reshape(X, (BK, BN)).to(tl.bfloat16)\n",
    "\n",
    "        acc += tl.dot(A, B)  # MMA path\n",
    "\n",
    "    # atomic add into dw_fp32\n",
    "    dw_ptrs = DW_ptr + m[:, None] * stride_dw0 + n[None, :] * stride_dw1\n",
    "    tl.atomic_add(dw_ptrs, acc, mask=mask_m[:, None] & mask_n[None, :] & valid_g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0af3b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shapes: K=8192, CIN=768, CIN_PAD=768, COUT=768, G=3, BITS=2, VPW=16, NWORDS=16\n",
      "Tiling: BM=64, BN=64, BK=32, SPLIT_K=8\n",
      "✅ correctness OK\n",
      "Unfused (dequant + cuBLAS mm): 0.064 ms/iter\n",
      "Fused   (unpack-in-GEMM):      0.422 ms/iter\n",
      "Speedup: 0.15x\n",
      "\n",
      "Shapes: K=100864, CIN=768, CIN_PAD=768, COUT=768, G=3, BITS=2, VPW=16, NWORDS=16\n",
      "Tiling: BM=64, BN=64, BK=32, SPLIT_K=16\n",
      "✅ correctness OK\n",
      "Unfused (dequant + cuBLAS mm): 0.764 ms/iter\n",
      "Fused   (unpack-in-GEMM):      4.597 ms/iter\n",
      "Speedup: 0.17x\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import triton\n",
    "\n",
    "# reuse your quant_pack_kernel and dequant_unpack_kernel here\n",
    "\n",
    "def time_ms(fn, iters=50, warmup=10):\n",
    "    torch.cuda.synchronize()\n",
    "    for _ in range(warmup):\n",
    "        fn()\n",
    "    torch.cuda.synchronize()\n",
    "    start = torch.cuda.Event(True); end = torch.cuda.Event(True)\n",
    "    start.record()\n",
    "    for _ in range(iters):\n",
    "        fn()\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return start.elapsed_time(end) / iters\n",
    "\n",
    "def cdiv(a,b): return (a+b-1)//b\n",
    "\n",
    "@torch.no_grad()\n",
    "def bench(K=100864, CIN=768, COUT=768, BITS=2, GROUP=256, iters=50,\n",
    "          BM=64, BN=64, BK=32, SPLIT_K=8):\n",
    "    assert GROUP == 256\n",
    "    assert BN % (32//BITS) == 0\n",
    "    assert GROUP % BN == 0\n",
    "    assert BK % 16 == 0\n",
    "\n",
    "    VPW = 32 // BITS\n",
    "    NWORDS = GROUP // VPW\n",
    "    QMAX = (1 << BITS) - 1\n",
    "\n",
    "    CIN_PAD = cdiv(CIN, GROUP) * GROUP\n",
    "    G = CIN_PAD // GROUP\n",
    "\n",
    "    print(f\"\\nShapes: K={K}, CIN={CIN}, CIN_PAD={CIN_PAD}, COUT={COUT}, G={G}, \"\n",
    "          f\"BITS={BITS}, VPW={VPW}, NWORDS={NWORDS}\")\n",
    "    print(f\"Tiling: BM={BM}, BN={BN}, BK={BK}, SPLIT_K={SPLIT_K}\")\n",
    "\n",
    "    # ---- inputs\n",
    "    x = torch.randn((K, CIN), device=\"cuda\", dtype=torch.bfloat16)\n",
    "    if CIN_PAD != CIN:\n",
    "        x = torch.nn.functional.pad(x, (0, CIN_PAD - CIN))\n",
    "    x = x.contiguous()\n",
    "\n",
    "    dy = torch.randn((K, COUT), device=\"cuda\", dtype=torch.bfloat16).contiguous()\n",
    "    dy_T = dy.t().contiguous()\n",
    "\n",
    "    # pack row-wise by groups along Cin\n",
    "    X_in = x.view(K, G, GROUP).reshape(K * G, GROUP).contiguous()\n",
    "\n",
    "    P = torch.empty((K * G, NWORDS), device=\"cuda\", dtype=torch.int32)\n",
    "    S = torch.empty((K * G,), device=\"cuda\", dtype=torch.bfloat16)\n",
    "    M = torch.empty((K * G,), device=\"cuda\", dtype=torch.bfloat16)\n",
    "\n",
    "    grid_q = (K * G,)\n",
    "    quant_pack_kernel[grid_q](\n",
    "        X_in, P, S, M,\n",
    "        X_in.stride(0), X_in.stride(1),\n",
    "        P.stride(0), P.stride(1),\n",
    "        BITS=BITS, VPW=VPW, NWORDS=NWORDS, QMAX=QMAX,\n",
    "        num_warps=4\n",
    "    )\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # unfused buffers\n",
    "    Y = torch.empty((K * G, GROUP), device=\"cuda\", dtype=torch.bfloat16)\n",
    "    dw_unfused = torch.empty((COUT, CIN), device=\"cuda\", dtype=torch.bfloat16)\n",
    "\n",
    "    # fused buffers (fp32 for split-k accumulation)\n",
    "    dw_fp32 = torch.empty((COUT, CIN), device=\"cuda\", dtype=torch.float32)\n",
    "    dw_fused = torch.empty((COUT, CIN), device=\"cuda\", dtype=torch.bfloat16)\n",
    "\n",
    "    # ---- unfused\n",
    "    def run_unfused():\n",
    "        dequant_unpack_kernel[grid_q](\n",
    "            P, S, M, Y,\n",
    "            P.stride(0), P.stride(1),\n",
    "            Y.stride(0), Y.stride(1),\n",
    "            BITS=BITS, VPW=VPW, NWORDS=NWORDS,\n",
    "            num_warps=4\n",
    "        )\n",
    "        x_deq = Y.view(K, G, GROUP).reshape(K, CIN_PAD)[:, :CIN]\n",
    "        torch.mm(dy_T, x_deq, out=dw_unfused)\n",
    "\n",
    "    # ---- fused\n",
    "    K_SLICE = cdiv(K, SPLIT_K)\n",
    "\n",
    "    def run_fused():\n",
    "        VPW = 32 // BITS\n",
    "        WORDS_N = BN // VPW\n",
    "        TILES_PER_GROUP = GROUP // BN\n",
    "        K_SLICE = (K + SPLIT_K - 1) // SPLIT_K\n",
    "        # atomic accumulation needs zero\n",
    "        dw_fp32.zero_()\n",
    "        grid = (triton.cdiv(COUT, BM), triton.cdiv(CIN, BN), SPLIT_K)\n",
    "        fused_dw_kernel_splitk[grid](\n",
    "            P, S, M, dy, dw_fp32,\n",
    "            K=K, COUT=COUT, CIN=CIN, G=G,\n",
    "            stride_p0=P.stride(0), stride_p1=P.stride(1),\n",
    "            stride_s0=S.stride(0),\n",
    "            stride_m0=M.stride(0),\n",
    "            stride_dy0=dy.stride(0), stride_dy1=dy.stride(1),\n",
    "            stride_dw0=dw_fp32.stride(0), stride_dw1=dw_fp32.stride(1),\n",
    "            BITS=BITS, VPW=VPW, NWORDS=NWORDS, GROUP=GROUP,\n",
    "            BM=BM, BN=BN, BK=BK,\n",
    "            SPLIT_K=SPLIT_K, K_SLICE=K_SLICE,\n",
    "            WORDS_N=WORDS_N,\n",
    "            TILES_PER_GROUP=TILES_PER_GROUP,\n",
    "            num_warps=8,\n",
    "            num_stages=3\n",
    "        )\n",
    "        # match baseline dtype\n",
    "        dw_fused.copy_(dw_fp32.to(torch.bfloat16))\n",
    "\n",
    "    # correctness (loose tol because quant + different reduction order)\n",
    "    run_unfused()\n",
    "    run_fused()\n",
    "    torch.cuda.synchronize()\n",
    "    torch.testing.assert_close(dw_fused, dw_unfused, rtol=2e-2, atol=2e-2)\n",
    "    print(\"✅ correctness OK\")\n",
    "\n",
    "    t_unf = time_ms(run_unfused, iters=iters)\n",
    "    t_fus = time_ms(run_fused, iters=iters)\n",
    "\n",
    "    print(f\"Unfused (dequant + cuBLAS mm): {t_unf:.3f} ms/iter\")\n",
    "    print(f\"Fused   (unpack-in-GEMM):      {t_fus:.3f} ms/iter\")\n",
    "    print(f\"Speedup: {t_unf/t_fus:.2f}x\")\n",
    "\n",
    "# Example runs:\n",
    "# - your old K=8192 will still likely favor cuBLAS\n",
    "# - ViT-like K (~100k) is where fusion starts to matter\n",
    "if __name__ == \"__main__\":\n",
    "    bench(K=8192,   CIN=768, COUT=768, BITS=2, iters=100, SPLIT_K=8)      # likely slower than cuBLAS\n",
    "    bench(K=100864, CIN=768, COUT=768, BITS=2, iters=50,  SPLIT_K=16)     # more realistic ViT case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2519701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "694099bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def pack(\n",
    "    X_ptr, P_ptr, \n",
    "    stride_x0: tl.constexpr, stride_x1: tl.constexpr,\n",
    "    stride_p0: tl.constexpr, stride_p1: tl.constexpr,\n",
    "    BITS: tl.constexpr,\n",
    "    VPW: tl.constexpr,\n",
    "    NWORDS: tl.constexpr\n",
    "):\n",
    "    pid = tl.program_id(0)\n",
    "\n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        base = X_ptr + pid * stride_x0,\n",
    "        shape=(NWORDS * VPW,),\n",
    "        strides=(stride_x1,),\n",
    "        offsets=(0,),\n",
    "        block_shape=(NWORDS * VPW,),\n",
    "        order=(0,)\n",
    "    )\n",
    "\n",
    "    x = tl.load(x_block_ptr)\n",
    "    x = tl.reshape(x, (NWORDS, VPW))\n",
    "\n",
    "    j = tl.arange(0, VPW)\n",
    "    shifts = (j * BITS).to(tl.int32)\n",
    "\n",
    "    q = tl.sum(x >> shifts[None, :], axis=1).to(tl.int8)\n",
    "\n",
    "    q_block_ptr = tl.make_block_ptr(\n",
    "        base=P_ptr + pid * stride_p0,\n",
    "        shape=(NWORDS,),\n",
    "        strides=(stride_p1,),\n",
    "        offsets=(0,),\n",
    "        block_shape=(NWORDS,),\n",
    "        order=(0,)\n",
    "    )\n",
    "\n",
    "    tl.store(q_block_ptr, q)\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def unpack(\n",
    "    P_ptr, Y_ptr,\n",
    "    stride_p0: tl.constexpr, stride_p1: tl.constexpr,\n",
    "    stride_y0: tl.constexpr, stride_y1: tl.constexpr,\n",
    "    BITS: tl.constexpr,\n",
    "    VPW: tl.constexpr,\n",
    "    NWORDS: tl.constexpr\n",
    "):\n",
    "    pid = tl.program_id(0)\n",
    "\n",
    "    p_block_ptr = tl.make_block_ptr(\n",
    "        base=P_ptr + pid * stride_p0,\n",
    "        shape=(NWORDS,),\n",
    "        strides=(stride_p1,),\n",
    "        offsets=(0,),\n",
    "        block_shape=(NWORDS,),\n",
    "        order=(0,)\n",
    "    )\n",
    "\n",
    "    p = tl.load(p_block_ptr)\n",
    "\n",
    "    mask = (1 << BITS) - 1\n",
    "    j = tl.arange(0, VPW)\n",
    "    shifts = (j * BITS).to(tl.int32)\n",
    "\n",
    "    y = ((p[:, None] >> shifts[None, :]) & mask).to(tl.int8)\n",
    "\n",
    "    y_block_ptr = tl.make_block_ptr(\n",
    "        base=Y_ptr + pid * stride_y0,\n",
    "        shape=(NWORDS, VPW),\n",
    "        strides=(VPW, stride_y1),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(NWORDS, VPW),\n",
    "        order=(0, 1)\n",
    "    )\n",
    "\n",
    "    tl.store(y_block_ptr, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0be8c208",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def relu_fwd_fused_pack(\n",
    "    X_ptr, P_ptr, Y_ptr,\n",
    "    stride_x0: tl.constexpr, stride_x1: tl.constexpr,\n",
    "    stride_p0: tl.constexpr, stride_p1: tl.constexpr,\n",
    "    stride_y0: tl.constexpr, stride_y1: tl.constexpr,\n",
    "    BITS: tl.constexpr,\n",
    "    VPW: tl.constexpr,\n",
    "    NWORDS: tl.constexpr\n",
    "):\n",
    "    pid = tl.program_id(0)\n",
    "\n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        base=X_ptr + pid * stride_x0,\n",
    "        shape=(NWORDS * VPW,),\n",
    "        strides=(stride_x1, ),\n",
    "        offsets=(0,),\n",
    "        block_shape=(NWORDS * VPW,),\n",
    "        order=(0,)\n",
    "    )\n",
    "\n",
    "    x = tl.load(x_block_ptr)\n",
    "\n",
    "    relu_mask = x > 0\n",
    "    x = tl.where(relu_mask, x, 0)\n",
    "\n",
    "    relu_mask = tl.reshape(relu_mask, (NWORDS, VPW))\n",
    "    j = tl.arange(0, VPW)\n",
    "    shifts = (j * BITS).to(tl.int32)\n",
    "    p_relu_mask = tl.sum(relu_mask >> shifts[None, :], axis=1).to(tl.int8)\n",
    "\n",
    "    p_block_ptr = tl.make_block_ptr(\n",
    "        base=P_ptr + pid * stride_p1,\n",
    "        shape=(NWORDS,),\n",
    "        strides=(stride_p1,),\n",
    "        offsets=(0,),\n",
    "        block_shape=(NWORDS,),\n",
    "        order=(0,)\n",
    "    )\n",
    "\n",
    "    y_block_ptr = tl.make_block_ptr(\n",
    "        base=Y_ptr + pid * stride_y0,\n",
    "        shape=(NWORDS * VPW,),\n",
    "        strides=(stride_y1,),\n",
    "        offsets=(0,),\n",
    "        block_shape=(NWORDS * VPW,),\n",
    "        order=(0,)\n",
    "    )\n",
    "\n",
    "    tl.store(p_block_ptr, p_relu_mask)\n",
    "    tl.store(y_block_ptr, x)\n",
    "\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def relu_bwd_fused_unpack(\n",
    "    P_ptr, DY_ptr, DX_ptr,\n",
    "    stride_p0: tl.constexpr, stride_p1: tl.constexpr,\n",
    "    stride_dy0: tl.constexpr, stride_dy1: tl.constexpr,\n",
    "    stride_dx0: tl.constexpr, stride_dx1: tl.constexpr,\n",
    "    BITS: tl.constexpr,\n",
    "    VPW: tl.constexpr,\n",
    "    NWORDS: tl.constexpr\n",
    "):\n",
    "    pid = tl.program_id(0)\n",
    "\n",
    "    p_block_ptr = tl.make_block_ptr(\n",
    "        base=P_ptr + pid * stride_p0,\n",
    "        shape=(NWORDS,),\n",
    "        strides=(stride_dy1,),\n",
    "        offsets=(0,),\n",
    "        block_shape=(NWORDS,),\n",
    "        order=(0,)\n",
    "    )\n",
    "\n",
    "    dy_block_ptr = tl.make_block_ptr(\n",
    "        base=DY_ptr + pid * stride_dy0,\n",
    "        shape=(NWORDS, VPW),\n",
    "        strides=(VPW, stride_dy1),\n",
    "        offsets=(0,0),\n",
    "        block_shape=(NWORDS, VPW),\n",
    "        order=(0,1)\n",
    "    )\n",
    "\n",
    "    packed = tl.load(p_block_ptr)\n",
    "\n",
    "    mask = (1 << BITS) - 1\n",
    "    j = tl.arange(0, VPW)\n",
    "    shifts = (j * BITS).to(tl.int32)\n",
    "\n",
    "    relu_mask = ((packed[:, None] >> shifts[None, :]) & mask).to(tl.float32) # [NWORDS, VPW]\n",
    "    dy = tl.load(dy_block_ptr).to(tl.float32)\n",
    "\n",
    "    dx = dy * relu_mask\n",
    "    dx = tl.reshape(dx, (NWORDS * VPW,))\n",
    "\n",
    "    dx_block_ptr = tl.make_block_ptr(\n",
    "        base=DX_ptr + pid * stride_dx0,\n",
    "        shape=(NWORDS * VPW,),\n",
    "        strides=(stride_dx1,),\n",
    "        offsets=(0,),\n",
    "        block_shape=(NWORDS * VPW,),\n",
    "        order=(0,)\n",
    "    )\n",
    "\n",
    "    tl.store(dx_block_ptr, dx.to(tl.bfloat16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa66d7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda as cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd8492e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, G = 32, 100\n",
    "\n",
    "x = torch.randint(-100, 100, (N*G, 256), dtype=torch.int8, device=\"cuda\")\n",
    "y = torch.empty_like(x)\n",
    "dy = torch.randn(*x.shape, dtype=torch.bfloat16, device='cuda')\n",
    "x2 = torch.empty_like(dy, dtype=torch.bfloat16, device='cuda')\n",
    "\n",
    "p = torch.empty((N * G, 16), dtype=torch.int8, device='cuda')\n",
    "\n",
    "\n",
    "start_timer = cuda.Event(enable_timing=True)\n",
    "end_timer = cuda.Event(enable_timing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "badad32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BITS = 2\n",
    "VPW = 32 // 2\n",
    "NWORDS = 256 // VPW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f9ae27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.00025513601303100584 s\n"
     ]
    }
   ],
   "source": [
    "cuda.synchronize()\n",
    "\n",
    "start_timer.record()\n",
    "\n",
    "grid = (N * G,)\n",
    "# pack[grid](\n",
    "#     x, p, \n",
    "#     x.stride(0), x.stride(1),\n",
    "#     p.stride(0), p.stride(1),\n",
    "#     BITS=BITS,\n",
    "#     VPW=VPW,\n",
    "#     NWORDS=NWORDS\n",
    "# )\n",
    "# y2 = F.relu(x)\n",
    "# relu_fwd_fused_pack[grid](\n",
    "#     x, p, y,\n",
    "#     x.stride(0), x.stride(1),\n",
    "#     p.stride(0), p.stride(1),\n",
    "#     y.stride(0), y.stride(1),\n",
    "#     BITS=BITS,\n",
    "#     VPW=VPW,\n",
    "#     NWORDS=NWORDS\n",
    "# )\n",
    "relu_bwd_fused_unpack[grid](\n",
    "    p, dy, x2,\n",
    "    p.stride(0), p.stride(1),\n",
    "    dy.stride(0), dy.stride(1),\n",
    "    x2.stride(0), x2.stride(1),\n",
    "    BITS=BITS,\n",
    "    VPW=VPW,\n",
    "    NWORDS=NWORDS\n",
    ")\n",
    "\n",
    "end_timer.record()\n",
    "cuda.synchronize()\n",
    "time = start_timer.elapsed_time(end_timer)\n",
    "\n",
    "print(f\"Time: {time / 1000} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "be7a657d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3298410441.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[102], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Time: 0.014269120216369629 s\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Time: 0.014269120216369629 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3ad583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
