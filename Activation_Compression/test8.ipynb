{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e96b8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/yyu496/.conda/envs/lib/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "# os.environ[\"TORCH_LOGS\"] = \"output_code\"\n",
    "# os.environ[\"TORCH_LOGS\"] = \"inductor\"\n",
    "# os.environ[\"TORCHINDUCTOR_TRACE\"] = \"1\"\n",
    "# os.environ[\"TORCHINDUCTOR_VERBOSE\"] = \"1\"\n",
    "# os.environ[\"TORCHINDUCTOR_DEBUG\"] = \"1\"\n",
    "# os.environ[\"TORCHINDUCTOR_DUMP\"] = \"1\"\n",
    "# os.environ[\"TORCHINDUCTOR_COMPILE_THREADS\"] = \"1\"\n",
    "# os.environ[\"TORCH_COMPILE_DEBUG\"] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = (\n",
    "    # \"backend:cudaMallocAsync,\"\n",
    "    \"expandable_segments:True,\"\n",
    "    # \"garbage_collection_threshold:0.6\"\n",
    ")\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/hice1/yyu496/kaggle/CW')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch._functorch.aot_autograd import aot_module, make_boxed_func\n",
    "from torch.autograd import grad\n",
    "from torch.optim.lr_scheduler import (\n",
    "    LinearLR,\n",
    "    CosineAnnealingLR,\n",
    "    SequentialLR\n",
    ")\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "import torch\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import torch._dynamo as dynamo\n",
    "\n",
    "import ACT6.cpp_extension as cpp_extension\n",
    "\n",
    "from ACT6.controller import Controller\n",
    "import ACT6.cuda_graph_utils as cuda_utils\n",
    "from ACT6.layers import RMSNorm\n",
    "\n",
    "import timm\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "\n",
    "import actnn\n",
    "# available choices are [\"L0\", \"L1\", \"L2\", \"L3\", \"L4\", \"L5\"]\n",
    "actnn.set_optimization_level(\"L3\")\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchmetrics import Accuracy \n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aad128bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/yyu496/.conda/envs/lib/lib/python3.10/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 45 worker processes in total. Our suggested max number of worker in current system is 15, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "warmup_epochs = 20\n",
    "num_epochs = 512\n",
    "# ============= Data ==================\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "transform_train = v2.Compose([\n",
    "    # --- Convert to tensor FIRST (kills PIL early) ---\n",
    "    v2.ToImage(),                          # handles PIL → Tensor safely\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "\n",
    "    # --- Geometry ---\n",
    "    v2.RandomResizedCrop(224, scale=(0.5, 1.0), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "    # --- Color ---\n",
    "    v2.RandomApply([\n",
    "        v2.ColorJitter(\n",
    "            brightness=0.2,\n",
    "            contrast=0.2,\n",
    "            saturation=0.2,\n",
    "            hue=0.1\n",
    "        )\n",
    "    ], p=0.5),\n",
    "\n",
    "    # --- RandAugment (vectorized, NO PIL) ---\n",
    "    v2.RandAugment(num_ops=4, magnitude=9),\n",
    "\n",
    "    # --- Normalize ---\n",
    "    v2.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "\n",
    "    # --- Random Erasing (tensor-based, fast) ---\n",
    "    v2.RandomErasing(p=0.5, scale=(0.02, 0.33),\n",
    "                     ratio=(0.3, 3.3), value=0),\n",
    "])\n",
    "\n",
    "# download and prepare\n",
    "# train_dataset = datasets.CIFAR100(root=\"./data\", train=True,\n",
    "#                                  download=True, transform=transform_train)\n",
    "\n",
    "train_dataset = ImageFolder(\n",
    "    root=\"/home/hice1/yyu496/scratch/data/cifar10_resized/train\",\n",
    "    transform=transform_train\n",
    ")\n",
    "\n",
    "# DataLoader with GPU-friendly settings\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                          num_workers=45, pin_memory=True, drop_last=True,\n",
    "                          persistent_workers=True, prefetch_factor=15)\n",
    "\n",
    "\n",
    "\n",
    "val_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "\n",
    "    v2.Resize(224, antialias=True),\n",
    "\n",
    "    v2.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# === CIFAR-10 validation dataset (train=False) ===\n",
    "# val_dataset = datasets.CIFAR100(\n",
    "#     root=\"./data\",\n",
    "#     train=False,\n",
    "#     download=True,\n",
    "#     transform=val_transform\n",
    "# )\n",
    "val_dataset = ImageFolder(\n",
    "    root=\"/home/hice1/yyu496/scratch/data/cifar10_resized/test\",\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "# === Validation DataLoader ===\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,   \n",
    "    num_workers=45,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True, \n",
    "    prefetch_factor=15\n",
    ")\n",
    "\n",
    "def replace_activation_func(m):\n",
    "    for name, child in m.named_children():\n",
    "        if isinstance(child, nn.ReLU):\n",
    "            setattr(m, name, nn.SiLU(inplace=True))\n",
    "            # setattr(m, name, nn.ReLU6(inplace=True))\n",
    "            # setattr(m, name, nn.LeakyReLU(inplace=True))\n",
    "            # setattr(m, name, nn.GELU())\n",
    "        else:\n",
    "            replace_activation_func(child)\n",
    "\n",
    "\n",
    "def replace_norm(m):\n",
    "    for name, child in m.named_children():\n",
    "        if isinstance(child, nn.LayerNorm):\n",
    "            setattr(m, name, RMSNorm(dims=child.normalized_shape[-1]))\n",
    "        else:\n",
    "            replace_activation_func(child)\n",
    "\n",
    "\n",
    "def disable_act_inplace(m):\n",
    "    for module in m.modules():\n",
    "        if isinstance(module, (nn.ReLU, nn.SiLU, nn.GELU)) and hasattr(module, 'inplace'):\n",
    "            module.inplace = False\n",
    "\n",
    "\n",
    "def init_fan_out(m):\n",
    "    if isinstance(m, (nn.Conv2d,)):\n",
    "        nn.init.kaiming_normal_(\n",
    "            m.weight,\n",
    "            mode='fan_out',\n",
    "            nonlinearity='relu'\n",
    "        )\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(\n",
    "            m.weight,\n",
    "            mode='fan_in',\n",
    "            nonlinearity='relu'\n",
    "        )\n",
    "\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a19dbf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIVISION = {\n",
    "    'pool_kernel_size' : 3\n",
    "}\n",
    "# DIVISION = None\n",
    "\n",
    "config = {\n",
    "    \"default_bits\": 2,\n",
    "    'auto_precision': None,\n",
    "    'DIVISION' : DIVISION,\n",
    "    \"group_size\": 256,\n",
    "    'batch_size' : batch_size,\n",
    "    'fp8' : False,\n",
    "    'depth_point_conv' : False,\n",
    "    'rms_norm' : False\n",
    "}\n",
    "\n",
    "# vit_base_patch16_224\n",
    "# efficientnet_b0\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=False, num_classes=10)\n",
    "# replace_activation_func(model)\n",
    "# replace_norm(model)\n",
    "\n",
    "\n",
    "num_classes = 100\n",
    "# model = models.resnet50(weights=None)\n",
    "# model = models.resnet18(weights=None)\n",
    "# model = models.efficientnet_b0(weights=None)\n",
    "# model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "# model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "# replace_activation_func(model)\n",
    "disable_act_inplace(model)\n",
    "# model.apply(init_fan_out)\n",
    "# model = actnn.QModule(model)\n",
    "# model.cuda()\n",
    "# model.compile(fullgraph=True)\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "compute_stream = torch.cuda.Stream()\n",
    "controller = Controller(model, config, train_loader, criterion, test=False)\n",
    "controller.iterate(criterion)\n",
    "controller.warp_model(graph_mode=True, quantizer=True)\n",
    "# controller.warmup(train_loader, criterion, compute_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a490600",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N = len(controller.quantizer.layer_hessian_eigenvalue_spectral_density)\n",
    "cols = 3\n",
    "rows = math.ceil(N / cols)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 3*rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (ax, name) in enumerate(zip(axes, controller.quantizer.layer_hessian_eigenvalue_spectral_density.keys())):\n",
    "    if i < N:\n",
    "        ax.hist(controller.quantizer.layer_hessian_eigenvalue_spectral_density[name], bins=50)\n",
    "        ax.set_title(f\"Layer {name}\")\n",
    "    else:\n",
    "        ax.axis(\"off\")  # hide unused subplots\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "962045d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD_LARS(Optimizer):\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 lr=0.1,\n",
    "                 momentum=0.9,\n",
    "                 weight_decay=1e-4,\n",
    "                 eta=0.001,\n",
    "                 eps=1e-9,\n",
    "                 nesterov=False,\n",
    "                 trust_coef=True):\n",
    "        defaults = dict(lr=lr,\n",
    "                        momentum=momentum,\n",
    "                        weight_decay=weight_decay,\n",
    "                        eta=eta,\n",
    "                        eps=eps,\n",
    "                        nesterov=nesterov,\n",
    "                        trust_coef=trust_coef)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            momentum = group['momentum']\n",
    "            weight_decay = group['weight_decay']\n",
    "            eta = group['eta']\n",
    "            eps = group['eps']\n",
    "            nesterov = group['nesterov']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                grad = p.grad\n",
    "\n",
    "                # ---- decoupled weight decay ----\n",
    "                if weight_decay != 0 and p.ndim > 1:\n",
    "                    grad = grad.add(p, alpha=weight_decay)\n",
    "\n",
    "                # ---- LARS trust ratio (only apply to weight tensors) ----\n",
    "                if p.ndim > 1:      # don't LARS biases / norms\n",
    "                    w_norm = p.norm()\n",
    "                    g_norm = grad.norm()\n",
    "\n",
    "                    if w_norm > 0 and g_norm > 0:\n",
    "                        trust_ratio = eta * (w_norm / (g_norm + eps))\n",
    "                    else:\n",
    "                        trust_ratio = 1.0\n",
    "                else:\n",
    "                    trust_ratio = 0.001\n",
    "\n",
    "                scaled_lr = lr * trust_ratio\n",
    "\n",
    "                # ---- Momentum buffer ----\n",
    "                param_state = self.state[p]\n",
    "                if 'momentum_buffer' not in param_state:\n",
    "                    buf = param_state['momentum_buffer'] = torch.zeros_like(p)\n",
    "                else:\n",
    "                    buf = param_state['momentum_buffer']\n",
    "\n",
    "                buf.mul_(momentum).add_(grad)  # standard momentum update\n",
    "\n",
    "                if nesterov:\n",
    "                    update = grad.add(buf, alpha=momentum)\n",
    "                else:\n",
    "                    update = buf\n",
    "\n",
    "                # ---- Update weights ----\n",
    "                p.add_(update, alpha=-scaled_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb276b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = Accuracy(task='multiclass', num_classes=10, average='macro')\n",
    "\n",
    "opt = torch.optim.AdamW(controller.traced_model.parameters(), lr=(1e-3 * 3), fused=True, capturable=True)\n",
    "# opt = SGD_LARS(\n",
    "#     controller.traced_model.parameters(),\n",
    "#     lr=0.5,                     # LARS needs high LR\n",
    "#     momentum=0.9,\n",
    "#     weight_decay=1e-7,          # decoupled weight decay\n",
    "#     eta=0.001,                  # LARS coefficient (0.001–0.02)\n",
    "#     nesterov=True\n",
    "# )\n",
    "# opt = torch.optim.AdamW(model.parameters(), lr=(1e-3 * 3), fused=True, capturable=True)\n",
    "# opt = SGD_LARS(\n",
    "#     model.parameters(),\n",
    "#     lr=1.0,                     # LARS needs high LR\n",
    "#     momentum=0.9,\n",
    "#     weight_decay=1e-7,          # decoupled weight decay\n",
    "#     eta=0.001,                  # LARS coefficient (0.001–0.02)\n",
    "#     nesterov=True\n",
    "# )\n",
    "\n",
    "\n",
    "total_timer_start = torch.cuda.Event(enable_timing=True)\n",
    "total_timer_end = torch.cuda.Event(enable_timing=True)\n",
    "total_time = 0.0\n",
    "\n",
    "e_timer_start = torch.cuda.Event(enable_timing=True)\n",
    "e_timer_end = torch.cuda.Event(enable_timing=True)\n",
    "partile_time = 0.0\n",
    "\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "#     opt,\n",
    "#     1e-3,\n",
    "#     total_steps= num_epochs * len(train_loader),\n",
    "#     pct_start=0.1,\n",
    "#     div_factor=10,\n",
    "#     final_div_factor=1000\n",
    "# )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "#     opt,\n",
    "#     T_0=10,\n",
    "#     T_mult=2,\n",
    "#     eta_min=1e-5\n",
    "# )\n",
    "\n",
    "warmup_sched = LinearLR(\n",
    "    opt,\n",
    "    start_factor=0.1,   # lr = lr * start_factor\n",
    "    end_factor=1.0,    # lr = lr * end_factor = lr\n",
    "    total_iters=warmup_epochs\n",
    ")\n",
    "\n",
    "# 2. Cosine decay\n",
    "cosine_sched = CosineAnnealingLR(\n",
    "    opt,\n",
    "    T_max=num_epochs - warmup_epochs,\n",
    "    eta_min=1e-4\n",
    ")\n",
    "\n",
    "# 3. Combine them\n",
    "scheduler = SequentialLR(\n",
    "    opt,\n",
    "    schedulers=[warmup_sched, cosine_sched],\n",
    "    milestones=[warmup_epochs]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4f1d2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_graph_generator = cuda_utils.Graph(controller.traced_model,\n",
    "                                        criterion,\n",
    "                                        opt,\n",
    "                                        train_loader,\n",
    "                                        compute_stream,\n",
    "                                        mode='qdrop',\n",
    "                                        num_of_graph=1,\n",
    "                                        device='cuda')\n",
    "\n",
    "\n",
    "# cuda_graph_generator = cuda_utils.Graph(model,\n",
    "#                                         criterion,\n",
    "#                                         opt,\n",
    "#                                         train_loader,\n",
    "#                                         compute_stream,\n",
    "#                                         mode='qdrop',\n",
    "#                                         num_of_graph=1,\n",
    "#                                         device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "530033ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs, compute_stream, static_x, static_y, logits, loss = cuda_graph_generator.capture_cuda_graph_qdrop()\n",
    "\n",
    "total_graphs = cuda_graph_generator.num_of_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "833f0627",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def lowpass_param_2d(t: torch.Tensor, k: int = 3):\n",
    "    \"\"\"\n",
    "    Low-pass filter for parameter tensors that have 2D structure in the last two dims\n",
    "    (e.g. conv weights [out_c, in_c, kH, kW] or activation-like grads [B, C, H, W]).\n",
    "\n",
    "    For non-4D tensors or very small spatial dims, returns t unchanged.\n",
    "    \"\"\"\n",
    "    if t.ndim != 4:\n",
    "        return t\n",
    "\n",
    "    *prefix, H, W = t.shape\n",
    "    if H < k or W < k:\n",
    "        return t  # nothing to do, kernel larger than spatial size\n",
    "\n",
    "    # Merge prefix dims into batch*channels, so shape -> [N, 1, H, W]\n",
    "    t_flat = t.view(-1, 1, H, W)\n",
    "    t_l = F.avg_pool2d(t_flat, kernel_size=k, stride=1, padding=k // 2)\n",
    "    t_l = F.interpolate(t_l, size=(H, W), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    return t_l.view_as(t)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def inject_grad_noise_large_batch(\n",
    "    model,\n",
    "    step,\n",
    "    batch_size,\n",
    "    total_samples,\n",
    "    base_std=1,\n",
    "    gamma=0.55,\n",
    "    lp_kernel: int = 3\n",
    "):\n",
    "    # Large-batch SGD noise recovery term\n",
    "    gns_scale = (1.0 / batch_size - 1.0 / total_samples) ** 0.5\n",
    "    decay = 1.0 / ((1 + step) ** gamma)\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.grad is None:\n",
    "            continue\n",
    "\n",
    "        # Skip bias / norm (critical for ViT & BN stability)\n",
    "        if p.ndim == 1:\n",
    "            continue\n",
    "\n",
    "\n",
    "        grad_std = p.grad.std().clamp(min=1e-6)\n",
    "\n",
    "        noise = torch.randn_like(p.grad) * base_std * gns_scale * grad_std * decay\n",
    "\n",
    "        if lp_kernel is not None and lp_kernel > 1:\n",
    "            noise = lowpass_param_2d(noise, k=lp_kernel)\n",
    "\n",
    "        p.grad.add_(noise)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "441888b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train Loss: 2.0698013305664062\n",
      "Learning Rate: 0.00030000000000000003\n",
      "Train Accuracy: 0.15378844738006592\n",
      "Peak Mem Reserved: 33395048448\n",
      "Peak Mem Allocated: 32231807488\n",
      "Current train time: 26.066883728027342 s\n",
      "Throughout: 1918.1425950904734 samples per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/yyu496/.conda/envs/lib/lib/python3.10/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 45 worker processes in total. Our suggested max number of worker in current system is 15, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 2.296875\n",
      "Val Accuracy: 0.23899999260902405\n",
      "\n",
      "\n",
      "Epoch: 1\n",
      "Train Loss: 2.0035934448242188\n",
      "Learning Rate: 0.000435\n",
      "Train Accuracy: 0.221540629863739\n",
      "Peak Mem Reserved: 33416019968\n",
      "Peak Mem Allocated: 32241442816\n",
      "Current train time: 25.03799557495117 s\n",
      "Throughout: 1996.9649667172891 samples per second\n",
      "Val Loss: 2.109375\n",
      "Val Accuracy: 0.31049999594688416\n",
      "\n",
      "\n",
      "Epoch: 2\n",
      "Train Loss: 1.8642311096191406\n",
      "Learning Rate: 0.00057\n",
      "Train Accuracy: 0.2683746814727783\n",
      "Peak Mem Reserved: 33416019968\n",
      "Peak Mem Allocated: 32241442816\n",
      "Current train time: 24.93927995300293 s\n",
      "Throughout: 2004.869430642063 samples per second\n",
      "Val Loss: 1.8046875\n",
      "Val Accuracy: 0.35919997096061707\n",
      "\n",
      "\n",
      "Epoch: 3\n",
      "Train Loss: 1.8989295959472656\n",
      "Learning Rate: 0.000705\n",
      "Train Accuracy: 0.2811632454395294\n",
      "Peak Mem Reserved: 33416019968\n",
      "Peak Mem Allocated: 32241442816\n",
      "Current train time: 24.937581253051757 s\n",
      "Throughout: 2005.0059984819582 samples per second\n",
      "Val Loss: 1.71875\n",
      "Val Accuracy: 0.39480000734329224\n",
      "\n",
      "\n",
      "Epoch: 4\n",
      "Train Loss: 1.8609466552734375\n",
      "Learning Rate: 0.0008399999999999999\n",
      "Train Accuracy: 0.30308738350868225\n",
      "Peak Mem Reserved: 33416019968\n",
      "Peak Mem Allocated: 32241442816\n",
      "Current train time: 24.94341275024414 s\n",
      "Throughout: 2004.537250000428 samples per second\n",
      "Val Loss: 1.609375\n",
      "Val Accuracy: 0.3947000503540039\n",
      "\n",
      "\n",
      "Epoch: 5\n",
      "Train Loss: 1.9885616302490234\n",
      "Learning Rate: 0.000975\n",
      "Train Accuracy: 0.2941741645336151\n",
      "Peak Mem Reserved: 33416019968\n",
      "Peak Mem Allocated: 32241442816\n",
      "Current train time: 24.926957290649415 s\n",
      "Throughout: 2005.860539535484 samples per second\n",
      "Val Loss: 1.5625\n",
      "Val Accuracy: 0.3562999963760376\n",
      "\n",
      "\n",
      "Epoch: 6\n",
      "Train Loss: 1.9522781372070312\n",
      "Learning Rate: 0.0011099999999999999\n",
      "Train Accuracy: 0.28091415762901306\n",
      "Peak Mem Reserved: 33416019968\n",
      "Peak Mem Allocated: 32241442816\n",
      "Current train time: 24.954410400390625 s\n",
      "Throughout: 2003.6538310365097 samples per second\n",
      "Val Loss: 1.4375\n",
      "Val Accuracy: 0.3440000116825104\n",
      "\n",
      "\n",
      "Epoch: 7\n",
      "Train Loss: 2.0984268188476562\n",
      "Learning Rate: 0.001245\n",
      "Train Accuracy: 0.24066627025604248\n",
      "Peak Mem Reserved: 33416019968\n",
      "Peak Mem Allocated: 32241442816\n",
      "Current train time: 24.928163604736326 s\n",
      "Throughout: 2005.7634727052277 samples per second\n",
      "Val Loss: 1.8203125\n",
      "Val Accuracy: 0.31939998269081116\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 80\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# scheduler.step()      \u001b[39;00m\n\u001b[1;32m     73\u001b[0m \n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# if step % 20 == 0:\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m#     temp = torch.nn.utils.parameters_to_vector(p.reshape(-1) for p in controller.traced_model.parameters() if p.grad is not None)\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m#     # temp = torch.nn.utils.parameters_to_vector(p.reshape(-1) for p in model.parameters() if p.grad is not None)\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m#     G.append(temp)     \u001b[39;00m\n\u001b[1;32m     79\u001b[0m e_timer_end\u001b[38;5;241m.\u001b[39mrecord()\n\u001b[0;32m---> 80\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m partile_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m e_timer_start\u001b[38;5;241m.\u001b[39melapsed_time(e_timer_end)\n\u001b[1;32m     83\u001b[0m train_logits\u001b[38;5;241m.\u001b[39mappend(logits\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu())\n",
      "File \u001b[0;32m~/.conda/envs/lib/lib/python3.10/site-packages/torch/cuda/__init__.py:1085\u001b[0m, in \u001b[0;36msynchronize\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m   1083\u001b[0m _lazy_init()\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[0;32m-> 1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_synchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "G_GNS = []\n",
    "G_SNR = []\n",
    "\n",
    "\n",
    "def gradient_noise_scale(grad):\n",
    "    tr_sigma = grad.var(0, unbiased=False).sum()\n",
    "    return tr_sigma / (grad.mean(0).pow(2).sum() + 1e-12)\n",
    "\n",
    "def signal_to_noise_ratio(grad):\n",
    "    mu = grad.mean(0)\n",
    "    tr_sigma = grad.var(0, unbiased=False).sum()\n",
    "    return mu.norm() / (tr_sigma.sqrt() + 1e-12)\n",
    "\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "total_timer_start.record()\n",
    "for i in range(num_epochs):\n",
    "    train_logits = []\n",
    "    train_y = []\n",
    "    \n",
    "\n",
    "    val_logits = []\n",
    "    val_y = []\n",
    "    partile_time = 0\n",
    "    controller.traced_model.train()\n",
    "    # model.train()\n",
    "    # aot_model.train()\n",
    "    G = []\n",
    "    torch.cuda.synchronize()\n",
    "    # k = i % total_graphs\n",
    "    for step, (x, y) in enumerate(train_loader):\n",
    "        \n",
    "        # torch.cuda.current_stream().wait_stream(warmup)  \n",
    "        x, y = x.to('cuda', non_blocking=True), y.to('cuda', non_blocking=True)\n",
    "        # x = transform_train(x)\n",
    "        \n",
    "        e_timer_start.record()\n",
    "        # compute_stream.wait_stream(torch.cuda.current_stream())\n",
    "        # with torch.cuda.stream(compute_stream):\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16, enabled=True):\n",
    "                # static_x.copy_(x.to('cuda'))\n",
    "                # static_y.copy_(y.to('cuda'))\n",
    "                # opt.zero_grad(set_to_none=False)\n",
    "\n",
    "                # total_steps = i * len(train_loader) + step\n",
    "                # k = step % total_graphs\n",
    "                # add_smoothout_noise(controller.traced_model)\n",
    "                # assign_theta(controller.traced_model)\n",
    "                # graphs[0].replay()\n",
    "                # remove_smoothout_noise(controller.traced_model)\n",
    "                # opt.step()\n",
    "                # scheduler.step()   \n",
    "                \n",
    "                # train_logits.append(logits.detach().cpu())\n",
    "                # train_y.append(static_y.detach().cpu())  \n",
    "\n",
    "                # for m in controller.traced_model.modules():\n",
    "                #     if hasattr(m, \"qdrop\"):\n",
    "                #         temp = torch.rand(1).item()\n",
    "                #         m.qdrop.copy_(temp)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            logits = controller.traced_model(x)\n",
    "            # logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            # inject_grad_noise_large_batch(controller.traced_model, step, batch_size, len(train_loader))\n",
    "            # inject_grad_noise_large_batch(model, step, batch_size, len(train_loader))\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            # scheduler.step()      \n",
    "            \n",
    "            # if step % 20 == 0:\n",
    "            #     temp = torch.nn.utils.parameters_to_vector(p.reshape(-1) for p in controller.traced_model.parameters() if p.grad is not None)\n",
    "            #     # temp = torch.nn.utils.parameters_to_vector(p.reshape(-1) for p in model.parameters() if p.grad is not None)\n",
    "            #     G.append(temp)     \n",
    "                \n",
    "            e_timer_end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            partile_time += e_timer_start.elapsed_time(e_timer_end)\n",
    "\n",
    "            train_logits.append(logits.detach().cpu())\n",
    "            train_y.append(y.detach().cpu())\n",
    "\n",
    "                # print(\"train logits mean:\", logits.mean().item())\n",
    "                # print(\"train logits std:\", logits.std().item())\n",
    "\n",
    "    # partile_time = e_timer_start.elapsed_time(e_timer_end)\n",
    "    # total_time += e_timer_start.elapsed_time(e_timer_end)\n",
    "\n",
    "    train_logits = torch.cat(train_logits)\n",
    "    train_y = torch.cat(train_y)\n",
    "    computed_acc = acc(train_logits, train_y)\n",
    "    throughtout = len(train_dataset) / (partile_time / 1000)\n",
    "\n",
    "    # temp_G = torch.cat(G)\n",
    "    # GNS = gradient_noise_scale(temp_G)\n",
    "    # SNR = signal_to_noise_ratio(temp_G)\n",
    "\n",
    "    print(f'Epoch: {i}')\n",
    "    print(f\"Train Loss: {loss}\")\n",
    "    print(f\"Learning Rate: {scheduler.get_last_lr()[0]}\")\n",
    "    print(f\"Train Accuracy: {computed_acc}\")\n",
    "    print(f'Peak Mem Reserved: {torch.cuda.max_memory_reserved()}')\n",
    "    print(f'Peak Mem Allocated: {torch.cuda.max_memory_allocated()}')\n",
    "    print(f'Current train time: {partile_time / 1000} s')\n",
    "    print(f\"Throughout: {throughtout} samples per second\")\n",
    "    # print(f'GNS: {GNS}')\n",
    "    # print(f'SNR: {SNR}')\n",
    "    # G_GNS.append(GNS)\n",
    "    # G_SNR.append(SNR)\n",
    "    scheduler.step()\n",
    "\n",
    "    train_logits = []\n",
    "    train_y = []\n",
    "\n",
    "    controller.traced_model.eval()\n",
    "    # model.eval()\n",
    "    # aot_model.eval()\n",
    "    with torch.compiler.set_stance(\"force_eager\"):\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in val_loader:\n",
    "                x_val, y_val = x_val.to('cuda'), y_val.to('cuda')\n",
    "\n",
    "                with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16, enabled=True):\n",
    "                    y_preds = controller.traced_model(x_val)\n",
    "                    # y_preds = model(x_val)\n",
    "                val_loss = F.cross_entropy(y_preds, y_val)\n",
    "\n",
    "                val_logits.append(y_preds.detach().cpu())\n",
    "                val_y.append(y_val.detach().cpu())\n",
    "\n",
    "    val_logits = torch.cat(val_logits)\n",
    "    val_y = torch.cat(val_y)\n",
    "\n",
    "    computed_acc = acc(val_logits, val_y)\n",
    "    print(f\"Val Loss: {val_loss}\")\n",
    "    print(f\"Val Accuracy: {computed_acc}\\n\\n\")\n",
    "\n",
    "\n",
    "total_timer_end.record()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "full_time = total_timer_start.elapsed_time(total_timer_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30bbafda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'group_size': 256,\n",
       " 'fp8': False,\n",
       " 'patch_embed.proj': {'bits': 2,\n",
       "  'group_size': 512,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': {'pool_kernel_size': 3},\n",
       "  'pack_only': False},\n",
       " 'batch_size': 512,\n",
       " 'blocks.0.attn.qkv': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.0.attn.proj': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.0.mlp.fc1': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.0.mlp.fc2': {'bits': 2,\n",
       "  'group_size': 512,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': {'pool_kernel_size': 3},\n",
       "  'pack_only': False},\n",
       " 'blocks.1.attn.qkv': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.1.attn.proj': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.1.mlp.fc1': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.1.mlp.fc2': {'bits': 2,\n",
       "  'group_size': 512,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': {'pool_kernel_size': 3},\n",
       "  'pack_only': False},\n",
       " 'blocks.2.attn.qkv': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.2.attn.proj': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.2.mlp.fc1': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.2.mlp.fc2': {'bits': 2,\n",
       "  'group_size': 512,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': {'pool_kernel_size': 3},\n",
       "  'pack_only': False},\n",
       " 'blocks.3.attn.qkv': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.3.attn.proj': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': {'pool_kernel_size': 3},\n",
       "  'pack_only': False},\n",
       " 'blocks.3.mlp.fc1': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.3.mlp.fc2': {'bits': 2,\n",
       "  'group_size': 512,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': {'pool_kernel_size': 3},\n",
       "  'pack_only': False},\n",
       " 'blocks.4.attn.qkv': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.4.attn.proj': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.4.mlp.fc1': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.4.mlp.fc2': {'bits': 2,\n",
       "  'group_size': 512,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': {'pool_kernel_size': 3},\n",
       "  'pack_only': False},\n",
       " 'blocks.5.attn.qkv': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.5.attn.proj': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': {'pool_kernel_size': 3},\n",
       "  'pack_only': False},\n",
       " 'blocks.5.mlp.fc1': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.5.mlp.fc2': {'bits': 2,\n",
       "  'group_size': 512,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': {'pool_kernel_size': 3},\n",
       "  'pack_only': False},\n",
       " 'blocks.6.attn.qkv': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.6.attn.proj': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.6.mlp.fc1': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.6.mlp.fc2': {'bits': 2,\n",
       "  'group_size': 512,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': {'pool_kernel_size': 3},\n",
       "  'pack_only': False},\n",
       " 'blocks.7.attn.qkv': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.7.attn.proj': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': {'pool_kernel_size': 3},\n",
       "  'pack_only': False},\n",
       " 'blocks.7.mlp.fc1': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.7.mlp.fc2': {'bits': 2,\n",
       "  'group_size': 512,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': {'pool_kernel_size': 3},\n",
       "  'pack_only': False},\n",
       " 'blocks.8.attn.qkv': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.8.attn.proj': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': {'pool_kernel_size': 3},\n",
       "  'pack_only': False},\n",
       " 'blocks.8.mlp.fc1': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.8.mlp.fc2': {'bits': 2,\n",
       "  'group_size': 512,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': {'pool_kernel_size': 3},\n",
       "  'pack_only': False},\n",
       " 'blocks.9.attn.qkv': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.9.attn.proj': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.9.mlp.fc1': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.9.mlp.fc2': {'bits': 2,\n",
       "  'group_size': 512,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': {'pool_kernel_size': 3},\n",
       "  'pack_only': False},\n",
       " 'blocks.10.attn.qkv': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.10.attn.proj': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': {'pool_kernel_size': 3},\n",
       "  'pack_only': False},\n",
       " 'blocks.10.mlp.fc1': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.10.mlp.fc2': {'bits': 2,\n",
       "  'group_size': 512,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': {'pool_kernel_size': 3},\n",
       "  'pack_only': False},\n",
       " 'blocks.11.attn.qkv': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.11.attn.proj': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.11.mlp.fc1': {'bits': 2,\n",
       "  'group_size': 256,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': None,\n",
       "  'pack_only': False},\n",
       " 'blocks.11.mlp.fc2': {'bits': 2,\n",
       "  'group_size': 512,\n",
       "  'act_padding': False,\n",
       "  'N': 512,\n",
       "  'DIVISION': {'pool_kernel_size': 3},\n",
       "  'pack_only': False}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "controller.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e523df",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x, y = x.to('cuda'), y.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de857ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with profile(\n",
    "    activities=[\n",
    "        ProfilerActivity.CPU,\n",
    "        ProfilerActivity.CUDA\n",
    "    ],\n",
    "    record_shapes=True,\n",
    "    with_stack=True,\n",
    "    profile_memory=True,\n",
    ") as prof:\n",
    "\n",
    "    for step in range(10):\n",
    "        with record_function(\"train_step\"):\n",
    "            with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "                output = controller.traced_model(x)\n",
    "                loss = criterion(output, y)\n",
    "                loss.backward()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "print(prof.key_averages().table(\n",
    "    sort_by=\"cuda_time_total\",\n",
    "    row_limit=200\n",
    "))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
